{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving the Way Neural Networks Learn\n",
    "\n",
    "This chapter discusses an improved cost function, the cross-entropy cost function, regularization methods, initializing the weights better, and choosing better hyper-parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Entropy Cost Function\n",
    "\n",
    "When we make large mistakes, we learn very quickly; however, when our errors are less well-defined we learn more slowly.\n",
    "This is not the case with neural networks, when they are grossly wrong they learn slower than if they are only marignally wrong.\n",
    "\n",
    "### The Equation and Its Properties\n",
    "\n",
    "The cross-entropy cost function is: $ C = -\\frac{1}{n} \\sum_x \\left[ y \\ln a + (1 - y)\\ln(1 - a) \\right] $, where $ n $ is the total number of training data, and the sum $ x $ is over all inputs, and $ y $ is the desired output.\n",
    "This is a suitable cost function because:\n",
    "\n",
    "1. It is non-negative, $ C \\gt 0 $.\n",
    "2. If the neuron's output is close to the desired output for all inputs, $ x $, then this function will be close to zero.\n",
    "\n",
    "These properties, expecially #2 contribute to the cross-entropy function being less susceptible to learning slowly when compared to the quadratic cost function.\n",
    "\n",
    "The partial derivative of the cross-entropy function with respect to the weight is: $ \\frac{\\partial C}{\\partial w_j} = \\frac{1}{n} \\sum_x \\frac{\\sigma'(z)x_j}{\\sigma(z)(1-\\sigma(z))} (\\sigma(z) - y) $, which when simplified is: $ \\frac{\\partial C}{\\partial w_j} = \\frac{1}{n} \\sum_x x_j (\\sigma(z) - y) $ because $ \\sigma'(z) = \\sigma(z)(1 - \\sigma(z)) $.\n",
    "It follows that $ \\frac{\\partial C}{\\partial b} = \\frac{1}{n} \\sum_x (\\sigma(z) - y) $.\n",
    "\n",
    "The equation for a multi-neuron multi-layer neural network is as follows: $ C = -\\frac{1}{n} \\sum_x \\sum_j \\left[y_j ln a^L_j + (1 - y_j)ln(1 - a^L_j) \\right] $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json, random, sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QuadraticCost(object):\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        return 0.5 * np.linalg.norm(a - y) ** 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a - y) * sigmoid_prime(z)\n",
    "    \n",
    "class CrossEntropyCost(object):\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        return np.sum(np.nan_to_num(-y * np.log(a) - (1 - y) * np.log( 1- a)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Loading a Network\n",
    "def load(filename):\n",
    "    \"\"\"Load a neural network from the file ``filename``.  Returns an\n",
    "    instance of Network.\n",
    "    \"\"\"\n",
    "    f = open(filename, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
    "    net = Network(data[\"sizes\"], cost=cost)\n",
    "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
    "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
    "    return net\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the j'th position\n",
    "    and zeroes elsewhere.  This is used to convert a digit (0...9)\n",
    "    into a corresponding desired output from the neural network.\n",
    "    \"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, './code')\n",
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "import network2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Accuracy on evaluation data: 9113 / 10000\n",
      "\n",
      "Epoch 1 training complete\n",
      "Accuracy on evaluation data: 9235 / 10000\n",
      "\n",
      "Epoch 2 training complete\n",
      "Accuracy on evaluation data: 9291 / 10000\n",
      "\n",
      "Epoch 3 training complete\n",
      "Accuracy on evaluation data: 9345 / 10000\n",
      "\n",
      "Epoch 4 training complete\n",
      "Accuracy on evaluation data: 9373 / 10000\n",
      "\n",
      "Epoch 5 training complete\n",
      "Accuracy on evaluation data: 9380 / 10000\n",
      "\n",
      "Epoch 6 training complete\n",
      "Accuracy on evaluation data: 9396 / 10000\n",
      "\n",
      "Epoch 7 training complete\n",
      "Accuracy on evaluation data: 9387 / 10000\n",
      "\n",
      "Epoch 8 training complete\n",
      "Accuracy on evaluation data: 9406 / 10000\n",
      "\n",
      "Epoch 9 training complete\n",
      "Accuracy on evaluation data: 9424 / 10000\n",
      "\n",
      "Epoch 10 training complete\n",
      "Accuracy on evaluation data: 9453 / 10000\n",
      "\n",
      "Epoch 11 training complete\n",
      "Accuracy on evaluation data: 9452 / 10000\n",
      "\n",
      "Epoch 12 training complete\n",
      "Accuracy on evaluation data: 9467 / 10000\n",
      "\n",
      "Epoch 13 training complete\n",
      "Accuracy on evaluation data: 9419 / 10000\n",
      "\n",
      "Epoch 14 training complete\n",
      "Accuracy on evaluation data: 9473 / 10000\n",
      "\n",
      "Epoch 15 training complete\n",
      "Accuracy on evaluation data: 9449 / 10000\n",
      "\n",
      "Epoch 16 training complete\n",
      "Accuracy on evaluation data: 9478 / 10000\n",
      "\n",
      "Epoch 17 training complete\n",
      "Accuracy on evaluation data: 9473 / 10000\n",
      "\n",
      "Epoch 18 training complete\n",
      "Accuracy on evaluation data: 9472 / 10000\n",
      "\n",
      "Epoch 19 training complete\n",
      "Accuracy on evaluation data: 9483 / 10000\n",
      "\n",
      "Epoch 20 training complete\n",
      "Accuracy on evaluation data: 9467 / 10000\n",
      "\n",
      "Epoch 21 training complete\n",
      "Accuracy on evaluation data: 9441 / 10000\n",
      "\n",
      "Epoch 22 training complete\n",
      "Accuracy on evaluation data: 9430 / 10000\n",
      "\n",
      "Epoch 23 training complete\n",
      "Accuracy on evaluation data: 9465 / 10000\n",
      "\n",
      "Epoch 24 training complete\n",
      "Accuracy on evaluation data: 9481 / 10000\n",
      "\n",
      "Epoch 25 training complete\n",
      "Accuracy on evaluation data: 9487 / 10000\n",
      "\n",
      "Epoch 26 training complete\n",
      "Accuracy on evaluation data: 9468 / 10000\n",
      "\n",
      "Epoch 27 training complete\n",
      "Accuracy on evaluation data: 9500 / 10000\n",
      "\n",
      "Epoch 28 training complete\n",
      "Accuracy on evaluation data: 9480 / 10000\n",
      "\n",
      "Epoch 29 training complete\n",
      "Accuracy on evaluation data: 9486 / 10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [9113,\n",
       "  9235,\n",
       "  9291,\n",
       "  9345,\n",
       "  9373,\n",
       "  9380,\n",
       "  9396,\n",
       "  9387,\n",
       "  9406,\n",
       "  9424,\n",
       "  9453,\n",
       "  9452,\n",
       "  9467,\n",
       "  9419,\n",
       "  9473,\n",
       "  9449,\n",
       "  9478,\n",
       "  9473,\n",
       "  9472,\n",
       "  9483,\n",
       "  9467,\n",
       "  9441,\n",
       "  9430,\n",
       "  9465,\n",
       "  9481,\n",
       "  9487,\n",
       "  9468,\n",
       "  9500,\n",
       "  9480,\n",
       "  9486],\n",
       " [],\n",
       " [])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = network2.Network([784, 30, 10], cost=CrossEntropyCost)\n",
    "net.large_weight_initializer()\n",
    "net.SGD(training_data, 30, 10, 0.5, evaluation_data=test_data, \n",
    "       monitor_evaluation_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "Softmax is a method that acheives the same goal as cross-entropy, to minimize learning slowdown.\n",
    "Softmax creates a new output layer for the network.\n",
    "Similar to a sigmoid layer it forms the weighted inputs $ z^L_j = \\sum_k w^L_{jk} a^{L-1}_k + b^L_j $, next the *softmax function* is applied to $ z^L_j $.\n",
    "The activation $ a^L_j $ of the $ j^{th} $ output is $ a^L_j = \\frac{e^{z^L_j}}{\\sum_k e^{z^L_k}} $.\n",
    "\n",
    "Note that $ \\sum_j a^L_j = \\frac{e^{z^L_j}}{\\sum_k e^{z^L_k}} = 1 $, which means that when one activation increases, the others will decrease and vice versa.\n",
    "The output of this function can be thought of as a probability distribution.\n",
    "\n",
    "### Log Likelihood Cost Function\n",
    "Let the log-likelihood cost function be $ C \\equiv - \\ln a^L_y $ where $ C $ is the function, $ a $ is the activation, $ y $ is the desired output, and $ L $ is the layer. \n",
    "This means that when the network is confident that the input is close to $ y $ the output of the function is close to $ 1 $.\n",
    "When the network is not doing a good job, the cost $ - \\ln a^L_y $ will be larger.\n",
    "\n",
    "In conclusion, softmax and log-likelihood cost is used when the output activations are to be used as probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Overfitting and Regularization\n",
    "\n",
    "The more parameters a model has, the more susceptible the model is to overfitting.\n",
    "\n",
    "### How to Recognize When a Model is Overfitted\n",
    "\n",
    "There are multiple metrics that can help one realize when a model is overfitted.\n",
    "The first step is to investigate the accuracy on the test data, if this isn't good, it may mean that the model is overfitted.\n",
    "The learning rate is suspicious if it plateaus prematurely (the model stops learning with only a small number of epochs).\n",
    "I don't think that this is a definitive metric for overfitting, but even if the model isn't overfitted, something is wrong because the model isn't acomplishing it's end goal.\n",
    "\n",
    "Another way that overfitting can be found is by looking at the cost on the test data.\n",
    "Recall that the purpose of the neural network is to minimize the cost.\n",
    "If you see that the cost drops, and then increases as the epochs progress, then the model is being overfitted.\n",
    "This can be confirmed if the cost on the training data decreases as the cost on the test data increases.\n",
    "\n",
    "Yet another sign of overfitting is in looking into the accuracy of the training data.\n",
    "If the accuracy approaches 100% too quickly (or even at all), then the model is probably overfitted.\n",
    "\n",
    "### Methods to Reduce Overfitting\n",
    "\n",
    "One way to detect overfitting is to keep track of how the accuracy on the test data, and if the accuracy of the test data stops increasing, then stop training.\n",
    "We use the `validation_data` set to measure overfitting.\n",
    "The `validation_data` is not found in the `test_data` or the `training_data`.\n",
    "When the classification accuracy on the `validation_data` has saturated, we stop training the model.\n",
    "This process is called *early stopping*. \n",
    "\n",
    "### `validation_data`\n",
    "\n",
    "`validation_data` is used to set the hyper-parameters.\n",
    "Why don't we use the `test_data` to set the hyper-parameters?\n",
    "This is to make sure that the model doesn't overfit the `test_data`.\n",
    "After a good set of hyper-parameters are discovered, then the `test_data` is evaluated.\n",
    "The approach of using the `validation_data` as a type of training data that helps to learn good hyper-parameters is called the *hold out* method because the `validation_data` is held out from the `training_data`.\n",
    "\n",
    "One of the best ways of reducing overfitting is to increase the size of `training_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Cost on training data: 1.8937980110453954\n",
      "Accuracy on evaluation data: 5278 / 10000\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 1.3844676075671265\n",
      "Accuracy on evaluation data: 6636 / 10000\n",
      "\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 1.1532897293201265\n",
      "Accuracy on evaluation data: 7114 / 10000\n",
      "\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 0.9592227754186763\n",
      "Accuracy on evaluation data: 7387 / 10000\n",
      "\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 0.8759523682530747\n",
      "Accuracy on evaluation data: 7423 / 10000\n",
      "\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 0.7387848786497457\n",
      "Accuracy on evaluation data: 7635 / 10000\n",
      "\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 0.6459295164151101\n",
      "Accuracy on evaluation data: 7865 / 10000\n",
      "\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 0.5986429580352461\n",
      "Accuracy on evaluation data: 7871 / 10000\n",
      "\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 0.5378963522395405\n",
      "Accuracy on evaluation data: 7967 / 10000\n",
      "\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 0.4864421979836552\n",
      "Accuracy on evaluation data: 7971 / 10000\n",
      "\n",
      "Epoch 10 training complete\n",
      "Cost on training data: 0.4561196691841285\n",
      "Accuracy on evaluation data: 7993 / 10000\n",
      "\n",
      "Epoch 11 training complete\n",
      "Cost on training data: 0.4160375177551692\n",
      "Accuracy on evaluation data: 8053 / 10000\n",
      "\n",
      "Epoch 12 training complete\n",
      "Cost on training data: 0.37817303413324976\n",
      "Accuracy on evaluation data: 8036 / 10000\n",
      "\n",
      "Epoch 13 training complete\n",
      "Cost on training data: 0.3486465325089059\n",
      "Accuracy on evaluation data: 8090 / 10000\n",
      "\n",
      "Epoch 14 training complete\n",
      "Cost on training data: 0.31602232367451966\n",
      "Accuracy on evaluation data: 8092 / 10000\n",
      "\n",
      "Epoch 15 training complete\n",
      "Cost on training data: 0.33545049983640857\n",
      "Accuracy on evaluation data: 8034 / 10000\n",
      "\n",
      "Epoch 16 training complete\n",
      "Cost on training data: 0.283917599154713\n",
      "Accuracy on evaluation data: 8094 / 10000\n",
      "\n",
      "Epoch 17 training complete\n",
      "Cost on training data: 0.27398740993166726\n",
      "Accuracy on evaluation data: 8108 / 10000\n",
      "\n",
      "Epoch 18 training complete\n",
      "Cost on training data: 0.2556419679895353\n",
      "Accuracy on evaluation data: 8131 / 10000\n",
      "\n",
      "Epoch 19 training complete\n",
      "Cost on training data: 0.23736620423949142\n",
      "Accuracy on evaluation data: 8141 / 10000\n",
      "\n",
      "Epoch 20 training complete\n",
      "Cost on training data: 0.23016297580538064\n",
      "Accuracy on evaluation data: 8139 / 10000\n",
      "\n",
      "Epoch 21 training complete\n",
      "Cost on training data: 0.2160255790803321\n",
      "Accuracy on evaluation data: 8118 / 10000\n",
      "\n",
      "Epoch 22 training complete\n",
      "Cost on training data: 0.20732322140761308\n",
      "Accuracy on evaluation data: 8118 / 10000\n",
      "\n",
      "Epoch 23 training complete\n",
      "Cost on training data: 0.19871446666109482\n",
      "Accuracy on evaluation data: 8150 / 10000\n",
      "\n",
      "Epoch 24 training complete\n",
      "Cost on training data: 0.1915482507364734\n",
      "Accuracy on evaluation data: 8149 / 10000\n",
      "\n",
      "Epoch 25 training complete\n",
      "Cost on training data: 0.18038908830382522\n",
      "Accuracy on evaluation data: 8152 / 10000\n",
      "\n",
      "Epoch 26 training complete\n",
      "Cost on training data: 0.17573245847761196\n",
      "Accuracy on evaluation data: 8155 / 10000\n",
      "\n",
      "Epoch 27 training complete\n",
      "Cost on training data: 0.1685077173381047\n",
      "Accuracy on evaluation data: 8159 / 10000\n",
      "\n",
      "Epoch 28 training complete\n",
      "Cost on training data: 0.1612964690516659\n",
      "Accuracy on evaluation data: 8164 / 10000\n",
      "\n",
      "Epoch 29 training complete\n",
      "Cost on training data: 0.16021407103796856\n",
      "Accuracy on evaluation data: 8149 / 10000\n",
      "\n",
      "Epoch 30 training complete\n",
      "Cost on training data: 0.15352097332952505\n",
      "Accuracy on evaluation data: 8182 / 10000\n",
      "\n",
      "Epoch 31 training complete\n",
      "Cost on training data: 0.14779254116010826\n",
      "Accuracy on evaluation data: 8164 / 10000\n",
      "\n",
      "Epoch 32 training complete\n",
      "Cost on training data: 0.14348165629746717\n",
      "Accuracy on evaluation data: 8181 / 10000\n",
      "\n",
      "Epoch 33 training complete\n",
      "Cost on training data: 0.13541136236961307\n",
      "Accuracy on evaluation data: 8179 / 10000\n",
      "\n",
      "Epoch 34 training complete\n",
      "Cost on training data: 0.13666120517143182\n",
      "Accuracy on evaluation data: 8175 / 10000\n",
      "\n",
      "Epoch 35 training complete\n",
      "Cost on training data: 0.12264801525785922\n",
      "Accuracy on evaluation data: 8183 / 10000\n",
      "\n",
      "Epoch 36 training complete\n",
      "Cost on training data: 0.12937460879831567\n",
      "Accuracy on evaluation data: 8177 / 10000\n",
      "\n",
      "Epoch 37 training complete\n",
      "Cost on training data: 0.1159025786001395\n",
      "Accuracy on evaluation data: 8183 / 10000\n",
      "\n",
      "Epoch 38 training complete\n",
      "Cost on training data: 0.11050240641857254\n",
      "Accuracy on evaluation data: 8215 / 10000\n",
      "\n",
      "Epoch 39 training complete\n",
      "Cost on training data: 0.1094927818046913\n",
      "Accuracy on evaluation data: 8189 / 10000\n",
      "\n",
      "Epoch 40 training complete\n",
      "Cost on training data: 0.10385178734627594\n",
      "Accuracy on evaluation data: 8194 / 10000\n",
      "\n",
      "Epoch 41 training complete\n",
      "Cost on training data: 0.09932721596732173\n",
      "Accuracy on evaluation data: 8211 / 10000\n",
      "\n",
      "Epoch 42 training complete\n",
      "Cost on training data: 0.09649447425965242\n",
      "Accuracy on evaluation data: 8213 / 10000\n",
      "\n",
      "Epoch 43 training complete\n",
      "Cost on training data: 0.0917886372363235\n",
      "Accuracy on evaluation data: 8229 / 10000\n",
      "\n",
      "Epoch 44 training complete\n",
      "Cost on training data: 0.08878884440589378\n",
      "Accuracy on evaluation data: 8222 / 10000\n",
      "\n",
      "Epoch 45 training complete\n",
      "Cost on training data: 0.08384863328162226\n",
      "Accuracy on evaluation data: 8221 / 10000\n",
      "\n",
      "Epoch 46 training complete\n",
      "Cost on training data: 0.08276837741971411\n",
      "Accuracy on evaluation data: 8207 / 10000\n",
      "\n",
      "Epoch 47 training complete\n",
      "Cost on training data: 0.08110218876928121\n",
      "Accuracy on evaluation data: 8203 / 10000\n",
      "\n",
      "Epoch 48 training complete\n",
      "Cost on training data: 0.07611432392730401\n",
      "Accuracy on evaluation data: 8234 / 10000\n",
      "\n",
      "Epoch 49 training complete\n",
      "Cost on training data: 0.07381516426656053\n",
      "Accuracy on evaluation data: 8236 / 10000\n",
      "\n",
      "Epoch 50 training complete\n",
      "Cost on training data: 0.07113924701352017\n",
      "Accuracy on evaluation data: 8240 / 10000\n",
      "\n",
      "Epoch 51 training complete\n",
      "Cost on training data: 0.06911668998995402\n",
      "Accuracy on evaluation data: 8231 / 10000\n",
      "\n",
      "Epoch 52 training complete\n",
      "Cost on training data: 0.06694232332961705\n",
      "Accuracy on evaluation data: 8226 / 10000\n",
      "\n",
      "Epoch 53 training complete\n",
      "Cost on training data: 0.06508774928749039\n",
      "Accuracy on evaluation data: 8230 / 10000\n",
      "\n",
      "Epoch 54 training complete\n",
      "Cost on training data: 0.06371527235637948\n",
      "Accuracy on evaluation data: 8223 / 10000\n",
      "\n",
      "Epoch 55 training complete\n",
      "Cost on training data: 0.06205633454940486\n",
      "Accuracy on evaluation data: 8228 / 10000\n",
      "\n",
      "Epoch 56 training complete\n",
      "Cost on training data: 0.060369575754091256\n",
      "Accuracy on evaluation data: 8230 / 10000\n",
      "\n",
      "Epoch 57 training complete\n",
      "Cost on training data: 0.05813223146077689\n",
      "Accuracy on evaluation data: 8234 / 10000\n",
      "\n",
      "Epoch 58 training complete\n",
      "Cost on training data: 0.057065528928577296\n",
      "Accuracy on evaluation data: 8215 / 10000\n",
      "\n",
      "Epoch 59 training complete\n",
      "Cost on training data: 0.05516438158635242\n",
      "Accuracy on evaluation data: 8227 / 10000\n",
      "\n",
      "Epoch 60 training complete\n",
      "Cost on training data: 0.054049357792255004\n",
      "Accuracy on evaluation data: 8230 / 10000\n",
      "\n",
      "Epoch 61 training complete\n",
      "Cost on training data: 0.05310692343512827\n",
      "Accuracy on evaluation data: 8219 / 10000\n",
      "\n",
      "Epoch 62 training complete\n",
      "Cost on training data: 0.051906049358039566\n",
      "Accuracy on evaluation data: 8225 / 10000\n",
      "\n",
      "Epoch 63 training complete\n",
      "Cost on training data: 0.050405422926562\n",
      "Accuracy on evaluation data: 8238 / 10000\n",
      "\n",
      "Epoch 64 training complete\n",
      "Cost on training data: 0.04939217305069287\n",
      "Accuracy on evaluation data: 8228 / 10000\n",
      "\n",
      "Epoch 65 training complete\n",
      "Cost on training data: 0.04824471432841786\n",
      "Accuracy on evaluation data: 8225 / 10000\n",
      "\n",
      "Epoch 66 training complete\n",
      "Cost on training data: 0.047464474248840045\n",
      "Accuracy on evaluation data: 8235 / 10000\n",
      "\n",
      "Epoch 67 training complete\n",
      "Cost on training data: 0.0463517631638704\n",
      "Accuracy on evaluation data: 8232 / 10000\n",
      "\n",
      "Epoch 68 training complete\n",
      "Cost on training data: 0.04516137761782436\n",
      "Accuracy on evaluation data: 8227 / 10000\n",
      "\n",
      "Epoch 69 training complete\n",
      "Cost on training data: 0.04410928966736897\n",
      "Accuracy on evaluation data: 8239 / 10000\n",
      "\n",
      "Epoch 70 training complete\n",
      "Cost on training data: 0.04294283944674896\n",
      "Accuracy on evaluation data: 8231 / 10000\n",
      "\n",
      "Epoch 71 training complete\n",
      "Cost on training data: 0.04226758160076539\n",
      "Accuracy on evaluation data: 8231 / 10000\n",
      "\n",
      "Epoch 72 training complete\n",
      "Cost on training data: 0.0413322358910447\n",
      "Accuracy on evaluation data: 8230 / 10000\n",
      "\n",
      "Epoch 73 training complete\n",
      "Cost on training data: 0.04048088237898116\n",
      "Accuracy on evaluation data: 8245 / 10000\n",
      "\n",
      "Epoch 74 training complete\n",
      "Cost on training data: 0.0395850107519111\n",
      "Accuracy on evaluation data: 8240 / 10000\n",
      "\n",
      "Epoch 75 training complete\n",
      "Cost on training data: 0.039038102352925624\n",
      "Accuracy on evaluation data: 8246 / 10000\n",
      "\n",
      "Epoch 76 training complete\n",
      "Cost on training data: 0.03821843202314996\n",
      "Accuracy on evaluation data: 8232 / 10000\n",
      "\n",
      "Epoch 77 training complete\n",
      "Cost on training data: 0.03755859856363082\n",
      "Accuracy on evaluation data: 8249 / 10000\n",
      "\n",
      "Epoch 78 training complete\n",
      "Cost on training data: 0.036769292098142475\n",
      "Accuracy on evaluation data: 8236 / 10000\n",
      "\n",
      "Epoch 79 training complete\n",
      "Cost on training data: 0.036039584943953255\n",
      "Accuracy on evaluation data: 8243 / 10000\n",
      "\n",
      "Epoch 80 training complete\n",
      "Cost on training data: 0.03541715916846108\n",
      "Accuracy on evaluation data: 8239 / 10000\n",
      "\n",
      "Epoch 81 training complete\n",
      "Cost on training data: 0.03465808164700103\n",
      "Accuracy on evaluation data: 8256 / 10000\n",
      "\n",
      "Epoch 82 training complete\n",
      "Cost on training data: 0.03411927092140432\n",
      "Accuracy on evaluation data: 8247 / 10000\n",
      "\n",
      "Epoch 83 training complete\n",
      "Cost on training data: 0.033299544337982157\n",
      "Accuracy on evaluation data: 8255 / 10000\n",
      "\n",
      "Epoch 84 training complete\n",
      "Cost on training data: 0.03279197523688091\n",
      "Accuracy on evaluation data: 8252 / 10000\n",
      "\n",
      "Epoch 85 training complete\n",
      "Cost on training data: 0.03241143918860559\n",
      "Accuracy on evaluation data: 8255 / 10000\n",
      "\n",
      "Epoch 86 training complete\n",
      "Cost on training data: 0.031766174427852505\n",
      "Accuracy on evaluation data: 8252 / 10000\n",
      "\n",
      "Epoch 87 training complete\n",
      "Cost on training data: 0.03124963999646294\n",
      "Accuracy on evaluation data: 8253 / 10000\n",
      "\n",
      "Epoch 88 training complete\n",
      "Cost on training data: 0.03072914178064024\n",
      "Accuracy on evaluation data: 8254 / 10000\n",
      "\n",
      "Epoch 89 training complete\n",
      "Cost on training data: 0.030295824048034065\n",
      "Accuracy on evaluation data: 8253 / 10000\n",
      "\n",
      "Epoch 90 training complete\n",
      "Cost on training data: 0.029812861970101464\n",
      "Accuracy on evaluation data: 8252 / 10000\n",
      "\n",
      "Epoch 91 training complete\n",
      "Cost on training data: 0.02947700235160686\n",
      "Accuracy on evaluation data: 8259 / 10000\n",
      "\n",
      "Epoch 92 training complete\n",
      "Cost on training data: 0.028918276386841968\n",
      "Accuracy on evaluation data: 8250 / 10000\n",
      "\n",
      "Epoch 93 training complete\n",
      "Cost on training data: 0.028597790660603774\n",
      "Accuracy on evaluation data: 8260 / 10000\n",
      "\n",
      "Epoch 94 training complete\n",
      "Cost on training data: 0.02822901924739127\n",
      "Accuracy on evaluation data: 8254 / 10000\n",
      "\n",
      "Epoch 95 training complete\n",
      "Cost on training data: 0.027760765162782912\n",
      "Accuracy on evaluation data: 8258 / 10000\n",
      "\n",
      "Epoch 96 training complete\n",
      "Cost on training data: 0.02736973233092448\n",
      "Accuracy on evaluation data: 8256 / 10000\n",
      "\n",
      "Epoch 97 training complete\n",
      "Cost on training data: 0.026961822487068507\n",
      "Accuracy on evaluation data: 8251 / 10000\n",
      "\n",
      "Epoch 98 training complete\n",
      "Cost on training data: 0.026624399512978892\n",
      "Accuracy on evaluation data: 8255 / 10000\n",
      "\n",
      "Epoch 99 training complete\n",
      "Cost on training data: 0.02624425380211355\n",
      "Accuracy on evaluation data: 8255 / 10000\n",
      "\n",
      "Epoch 100 training complete\n",
      "Cost on training data: 0.025903500937351658\n",
      "Accuracy on evaluation data: 8253 / 10000\n",
      "\n",
      "Epoch 101 training complete\n",
      "Cost on training data: 0.025611366488493933\n",
      "Accuracy on evaluation data: 8247 / 10000\n",
      "\n",
      "Epoch 102 training complete\n",
      "Cost on training data: 0.025241771775940065\n",
      "Accuracy on evaluation data: 8257 / 10000\n",
      "\n",
      "Epoch 103 training complete\n",
      "Cost on training data: 0.024885107479477947\n",
      "Accuracy on evaluation data: 8252 / 10000\n",
      "\n",
      "Epoch 104 training complete\n",
      "Cost on training data: 0.02456524312688965\n",
      "Accuracy on evaluation data: 8253 / 10000\n",
      "\n",
      "Epoch 105 training complete\n",
      "Cost on training data: 0.024245194591268615\n",
      "Accuracy on evaluation data: 8259 / 10000\n",
      "\n",
      "Epoch 106 training complete\n",
      "Cost on training data: 0.02394688087763801\n",
      "Accuracy on evaluation data: 8258 / 10000\n",
      "\n",
      "Epoch 107 training complete\n",
      "Cost on training data: 0.023641655662521383\n",
      "Accuracy on evaluation data: 8250 / 10000\n",
      "\n",
      "Epoch 108 training complete\n",
      "Cost on training data: 0.023317065847187356\n",
      "Accuracy on evaluation data: 8253 / 10000\n",
      "\n",
      "Epoch 109 training complete\n",
      "Cost on training data: 0.023036452537305543\n",
      "Accuracy on evaluation data: 8256 / 10000\n",
      "\n",
      "Epoch 110 training complete\n",
      "Cost on training data: 0.02271728388611234\n",
      "Accuracy on evaluation data: 8258 / 10000\n",
      "\n",
      "Epoch 111 training complete\n",
      "Cost on training data: 0.022433508337322126\n",
      "Accuracy on evaluation data: 8254 / 10000\n",
      "\n",
      "Epoch 112 training complete\n",
      "Cost on training data: 0.022138927493056716\n",
      "Accuracy on evaluation data: 8252 / 10000\n",
      "\n",
      "Epoch 113 training complete\n",
      "Cost on training data: 0.021847477091533157\n",
      "Accuracy on evaluation data: 8252 / 10000\n",
      "\n",
      "Epoch 114 training complete\n",
      "Cost on training data: 0.021596556370190315\n",
      "Accuracy on evaluation data: 8252 / 10000\n",
      "\n",
      "Epoch 115 training complete\n",
      "Cost on training data: 0.02126554170534393\n",
      "Accuracy on evaluation data: 8253 / 10000\n",
      "\n",
      "Epoch 116 training complete\n",
      "Cost on training data: 0.021022195300076887\n",
      "Accuracy on evaluation data: 8246 / 10000\n",
      "\n",
      "Epoch 117 training complete\n",
      "Cost on training data: 0.02071263206229747\n",
      "Accuracy on evaluation data: 8251 / 10000\n",
      "\n",
      "Epoch 118 training complete\n",
      "Cost on training data: 0.02044431020533689\n",
      "Accuracy on evaluation data: 8250 / 10000\n",
      "\n",
      "Epoch 119 training complete\n",
      "Cost on training data: 0.02015162736276598\n",
      "Accuracy on evaluation data: 8253 / 10000\n",
      "\n",
      "Epoch 120 training complete\n",
      "Cost on training data: 0.019905072739548706\n",
      "Accuracy on evaluation data: 8251 / 10000\n",
      "\n",
      "Epoch 121 training complete\n",
      "Cost on training data: 0.01968205789306128\n",
      "Accuracy on evaluation data: 8260 / 10000\n",
      "\n",
      "Epoch 122 training complete\n",
      "Cost on training data: 0.01942054453115829\n",
      "Accuracy on evaluation data: 8253 / 10000\n",
      "\n",
      "Epoch 123 training complete\n",
      "Cost on training data: 0.019181300533654553\n",
      "Accuracy on evaluation data: 8254 / 10000\n",
      "\n",
      "Epoch 124 training complete\n",
      "Cost on training data: 0.018960153498224774\n",
      "Accuracy on evaluation data: 8255 / 10000\n",
      "\n",
      "Epoch 125 training complete\n",
      "Cost on training data: 0.018732601985158963\n",
      "Accuracy on evaluation data: 8255 / 10000\n",
      "\n",
      "Epoch 126 training complete\n",
      "Cost on training data: 0.018529797962629876\n",
      "Accuracy on evaluation data: 8259 / 10000\n",
      "\n",
      "Epoch 127 training complete\n",
      "Cost on training data: 0.018309985630572984\n",
      "Accuracy on evaluation data: 8256 / 10000\n",
      "\n",
      "Epoch 128 training complete\n",
      "Cost on training data: 0.018099696318601432\n",
      "Accuracy on evaluation data: 8256 / 10000\n",
      "\n",
      "Epoch 129 training complete\n",
      "Cost on training data: 0.01790124607072648\n",
      "Accuracy on evaluation data: 8257 / 10000\n",
      "\n",
      "Epoch 130 training complete\n",
      "Cost on training data: 0.017716673597768236\n",
      "Accuracy on evaluation data: 8257 / 10000\n",
      "\n",
      "Epoch 131 training complete\n",
      "Cost on training data: 0.017528981278823588\n",
      "Accuracy on evaluation data: 8259 / 10000\n",
      "\n",
      "Epoch 132 training complete\n",
      "Cost on training data: 0.017351674466078482\n",
      "Accuracy on evaluation data: 8261 / 10000\n",
      "\n",
      "Epoch 133 training complete\n",
      "Cost on training data: 0.017168421987334977\n",
      "Accuracy on evaluation data: 8259 / 10000\n",
      "\n",
      "Epoch 134 training complete\n",
      "Cost on training data: 0.016987473337896027\n",
      "Accuracy on evaluation data: 8261 / 10000\n",
      "\n",
      "Epoch 135 training complete\n",
      "Cost on training data: 0.01680477436952445\n",
      "Accuracy on evaluation data: 8264 / 10000\n",
      "\n",
      "Epoch 136 training complete\n",
      "Cost on training data: 0.0166464080134259\n",
      "Accuracy on evaluation data: 8265 / 10000\n",
      "\n",
      "Epoch 137 training complete\n",
      "Cost on training data: 0.01646445486159879\n",
      "Accuracy on evaluation data: 8266 / 10000\n",
      "\n",
      "Epoch 138 training complete\n",
      "Cost on training data: 0.016334590215237935\n",
      "Accuracy on evaluation data: 8256 / 10000\n",
      "\n",
      "Epoch 139 training complete\n",
      "Cost on training data: 0.01613323809470416\n",
      "Accuracy on evaluation data: 8260 / 10000\n",
      "\n",
      "Epoch 140 training complete\n",
      "Cost on training data: 0.015976008124056914\n",
      "Accuracy on evaluation data: 8257 / 10000\n",
      "\n",
      "Epoch 141 training complete\n",
      "Cost on training data: 0.015806263774614207\n",
      "Accuracy on evaluation data: 8260 / 10000\n",
      "\n",
      "Epoch 142 training complete\n",
      "Cost on training data: 0.015625162153222506\n",
      "Accuracy on evaluation data: 8258 / 10000\n",
      "\n",
      "Epoch 143 training complete\n",
      "Cost on training data: 0.015462727480563716\n",
      "Accuracy on evaluation data: 8260 / 10000\n",
      "\n",
      "Epoch 144 training complete\n",
      "Cost on training data: 0.01529877424857819\n",
      "Accuracy on evaluation data: 8265 / 10000\n",
      "\n",
      "Epoch 145 training complete\n",
      "Cost on training data: 0.01513845920881712\n",
      "Accuracy on evaluation data: 8261 / 10000\n",
      "\n",
      "Epoch 146 training complete\n",
      "Cost on training data: 0.014995663041250143\n",
      "Accuracy on evaluation data: 8266 / 10000\n",
      "\n",
      "Epoch 147 training complete\n",
      "Cost on training data: 0.014834922238819147\n",
      "Accuracy on evaluation data: 8266 / 10000\n",
      "\n",
      "Epoch 148 training complete\n",
      "Cost on training data: 0.014699590554560854\n",
      "Accuracy on evaluation data: 8266 / 10000\n",
      "\n",
      "Epoch 149 training complete\n",
      "Cost on training data: 0.014555570705052312\n",
      "Accuracy on evaluation data: 8264 / 10000\n",
      "\n",
      "Epoch 150 training complete\n",
      "Cost on training data: 0.014419251025301743\n",
      "Accuracy on evaluation data: 8267 / 10000\n",
      "\n",
      "Epoch 151 training complete\n",
      "Cost on training data: 0.014291816663632858\n",
      "Accuracy on evaluation data: 8270 / 10000\n",
      "\n",
      "Epoch 152 training complete\n",
      "Cost on training data: 0.01416883145080157\n",
      "Accuracy on evaluation data: 8262 / 10000\n",
      "\n",
      "Epoch 153 training complete\n",
      "Cost on training data: 0.0140368244866699\n",
      "Accuracy on evaluation data: 8269 / 10000\n",
      "\n",
      "Epoch 154 training complete\n",
      "Cost on training data: 0.013930103717222884\n",
      "Accuracy on evaluation data: 8268 / 10000\n",
      "\n",
      "Epoch 155 training complete\n",
      "Cost on training data: 0.013793617924768105\n",
      "Accuracy on evaluation data: 8269 / 10000\n",
      "\n",
      "Epoch 156 training complete\n",
      "Cost on training data: 0.013681326440411583\n",
      "Accuracy on evaluation data: 8267 / 10000\n",
      "\n",
      "Epoch 157 training complete\n",
      "Cost on training data: 0.013565996816637043\n",
      "Accuracy on evaluation data: 8268 / 10000\n",
      "\n",
      "Epoch 158 training complete\n",
      "Cost on training data: 0.013451952397087791\n",
      "Accuracy on evaluation data: 8271 / 10000\n",
      "\n",
      "Epoch 159 training complete\n",
      "Cost on training data: 0.013347605619578883\n",
      "Accuracy on evaluation data: 8268 / 10000\n",
      "\n",
      "Epoch 160 training complete\n",
      "Cost on training data: 0.013231999426233175\n",
      "Accuracy on evaluation data: 8269 / 10000\n",
      "\n",
      "Epoch 161 training complete\n",
      "Cost on training data: 0.013132857418653531\n",
      "Accuracy on evaluation data: 8275 / 10000\n",
      "\n",
      "Epoch 162 training complete\n",
      "Cost on training data: 0.013028509579888003\n",
      "Accuracy on evaluation data: 8269 / 10000\n",
      "\n",
      "Epoch 163 training complete\n",
      "Cost on training data: 0.012928239213846778\n",
      "Accuracy on evaluation data: 8270 / 10000\n",
      "\n",
      "Epoch 164 training complete\n",
      "Cost on training data: 0.01282169035891357\n",
      "Accuracy on evaluation data: 8273 / 10000\n",
      "\n",
      "Epoch 165 training complete\n",
      "Cost on training data: 0.01271867386866192\n",
      "Accuracy on evaluation data: 8273 / 10000\n",
      "\n",
      "Epoch 166 training complete\n",
      "Cost on training data: 0.0126293168602396\n",
      "Accuracy on evaluation data: 8286 / 10000\n",
      "\n",
      "Epoch 167 training complete\n",
      "Cost on training data: 0.012528847958586583\n",
      "Accuracy on evaluation data: 8277 / 10000\n",
      "\n",
      "Epoch 168 training complete\n",
      "Cost on training data: 0.01243505692755039\n",
      "Accuracy on evaluation data: 8272 / 10000\n",
      "\n",
      "Epoch 169 training complete\n",
      "Cost on training data: 0.012343935901510337\n",
      "Accuracy on evaluation data: 8275 / 10000\n",
      "\n",
      "Epoch 170 training complete\n",
      "Cost on training data: 0.012253377206393084\n",
      "Accuracy on evaluation data: 8275 / 10000\n",
      "\n",
      "Epoch 171 training complete\n",
      "Cost on training data: 0.012158970109384973\n",
      "Accuracy on evaluation data: 8274 / 10000\n",
      "\n",
      "Epoch 172 training complete\n",
      "Cost on training data: 0.012071721567343539\n",
      "Accuracy on evaluation data: 8272 / 10000\n",
      "\n",
      "Epoch 173 training complete\n",
      "Cost on training data: 0.01198122799558047\n",
      "Accuracy on evaluation data: 8279 / 10000\n",
      "\n",
      "Epoch 174 training complete\n",
      "Cost on training data: 0.011898168471260889\n",
      "Accuracy on evaluation data: 8277 / 10000\n",
      "\n",
      "Epoch 175 training complete\n",
      "Cost on training data: 0.01181502715443735\n",
      "Accuracy on evaluation data: 8275 / 10000\n",
      "\n",
      "Epoch 176 training complete\n",
      "Cost on training data: 0.011731511799923282\n",
      "Accuracy on evaluation data: 8274 / 10000\n",
      "\n",
      "Epoch 177 training complete\n",
      "Cost on training data: 0.011650838927478003\n",
      "Accuracy on evaluation data: 8272 / 10000\n",
      "\n",
      "Epoch 178 training complete\n",
      "Cost on training data: 0.011563596238702395\n",
      "Accuracy on evaluation data: 8281 / 10000\n",
      "\n",
      "Epoch 179 training complete\n",
      "Cost on training data: 0.01148493218319778\n",
      "Accuracy on evaluation data: 8276 / 10000\n",
      "\n",
      "Epoch 180 training complete\n",
      "Cost on training data: 0.011404095554794755\n",
      "Accuracy on evaluation data: 8280 / 10000\n",
      "\n",
      "Epoch 181 training complete\n",
      "Cost on training data: 0.01132743512939463\n",
      "Accuracy on evaluation data: 8278 / 10000\n",
      "\n",
      "Epoch 182 training complete\n",
      "Cost on training data: 0.01125348808737052\n",
      "Accuracy on evaluation data: 8276 / 10000\n",
      "\n",
      "Epoch 183 training complete\n",
      "Cost on training data: 0.011178709192642836\n",
      "Accuracy on evaluation data: 8282 / 10000\n",
      "\n",
      "Epoch 184 training complete\n",
      "Cost on training data: 0.011102234658851738\n",
      "Accuracy on evaluation data: 8276 / 10000\n",
      "\n",
      "Epoch 185 training complete\n",
      "Cost on training data: 0.011028218791294762\n",
      "Accuracy on evaluation data: 8273 / 10000\n",
      "\n",
      "Epoch 186 training complete\n",
      "Cost on training data: 0.010958064878876564\n",
      "Accuracy on evaluation data: 8274 / 10000\n",
      "\n",
      "Epoch 187 training complete\n",
      "Cost on training data: 0.01088600715823334\n",
      "Accuracy on evaluation data: 8279 / 10000\n",
      "\n",
      "Epoch 188 training complete\n",
      "Cost on training data: 0.01081740915401427\n",
      "Accuracy on evaluation data: 8278 / 10000\n",
      "\n",
      "Epoch 189 training complete\n",
      "Cost on training data: 0.010746628707759392\n",
      "Accuracy on evaluation data: 8276 / 10000\n",
      "\n",
      "Epoch 190 training complete\n",
      "Cost on training data: 0.010678440736796465\n",
      "Accuracy on evaluation data: 8277 / 10000\n",
      "\n",
      "Epoch 191 training complete\n",
      "Cost on training data: 0.010612007665652902\n",
      "Accuracy on evaluation data: 8282 / 10000\n",
      "\n",
      "Epoch 192 training complete\n",
      "Cost on training data: 0.010544133424962295\n",
      "Accuracy on evaluation data: 8277 / 10000\n",
      "\n",
      "Epoch 193 training complete\n",
      "Cost on training data: 0.010478942986256483\n",
      "Accuracy on evaluation data: 8274 / 10000\n",
      "\n",
      "Epoch 194 training complete\n",
      "Cost on training data: 0.010415399793566033\n",
      "Accuracy on evaluation data: 8274 / 10000\n",
      "\n",
      "Epoch 195 training complete\n",
      "Cost on training data: 0.010354488078336159\n",
      "Accuracy on evaluation data: 8275 / 10000\n",
      "\n",
      "Epoch 196 training complete\n",
      "Cost on training data: 0.01029047811200066\n",
      "Accuracy on evaluation data: 8278 / 10000\n",
      "\n",
      "Epoch 197 training complete\n",
      "Cost on training data: 0.010225506424346446\n",
      "Accuracy on evaluation data: 8276 / 10000\n",
      "\n",
      "Epoch 198 training complete\n",
      "Cost on training data: 0.010170670957606258\n",
      "Accuracy on evaluation data: 8283 / 10000\n",
      "\n",
      "Epoch 199 training complete\n",
      "Cost on training data: 0.010104107251668513\n",
      "Accuracy on evaluation data: 8275 / 10000\n",
      "\n",
      "Epoch 200 training complete\n",
      "Cost on training data: 0.010045431289979338\n",
      "Accuracy on evaluation data: 8276 / 10000\n",
      "\n",
      "Epoch 201 training complete\n",
      "Cost on training data: 0.009984331423677157\n",
      "Accuracy on evaluation data: 8275 / 10000\n",
      "\n",
      "Epoch 202 training complete\n",
      "Cost on training data: 0.009926990878254825\n",
      "Accuracy on evaluation data: 8274 / 10000\n",
      "\n",
      "Epoch 203 training complete\n",
      "Cost on training data: 0.009873812737463166\n",
      "Accuracy on evaluation data: 8283 / 10000\n",
      "\n",
      "Epoch 204 training complete\n",
      "Cost on training data: 0.00981199323575123\n",
      "Accuracy on evaluation data: 8280 / 10000\n",
      "\n",
      "Epoch 205 training complete\n",
      "Cost on training data: 0.009755805584456878\n",
      "Accuracy on evaluation data: 8276 / 10000\n",
      "\n",
      "Epoch 206 training complete\n",
      "Cost on training data: 0.009698716975861166\n",
      "Accuracy on evaluation data: 8278 / 10000\n",
      "\n",
      "Epoch 207 training complete\n",
      "Cost on training data: 0.009645888861369589\n",
      "Accuracy on evaluation data: 8277 / 10000\n",
      "\n",
      "Epoch 208 training complete\n",
      "Cost on training data: 0.00958783814826825\n",
      "Accuracy on evaluation data: 8278 / 10000\n",
      "\n",
      "Epoch 209 training complete\n",
      "Cost on training data: 0.009536546501978094\n",
      "Accuracy on evaluation data: 8275 / 10000\n",
      "\n",
      "Epoch 210 training complete\n",
      "Cost on training data: 0.009480926223824944\n",
      "Accuracy on evaluation data: 8277 / 10000\n",
      "\n",
      "Epoch 211 training complete\n",
      "Cost on training data: 0.009429853865725282\n",
      "Accuracy on evaluation data: 8279 / 10000\n",
      "\n",
      "Epoch 212 training complete\n",
      "Cost on training data: 0.009375370378997933\n",
      "Accuracy on evaluation data: 8279 / 10000\n",
      "\n",
      "Epoch 213 training complete\n",
      "Cost on training data: 0.00932469213624107\n",
      "Accuracy on evaluation data: 8278 / 10000\n",
      "\n",
      "Epoch 214 training complete\n",
      "Cost on training data: 0.009275993577815791\n",
      "Accuracy on evaluation data: 8283 / 10000\n",
      "\n",
      "Epoch 215 training complete\n",
      "Cost on training data: 0.009222996477917278\n",
      "Accuracy on evaluation data: 8280 / 10000\n",
      "\n",
      "Epoch 216 training complete\n",
      "Cost on training data: 0.009172274489450185\n",
      "Accuracy on evaluation data: 8282 / 10000\n",
      "\n",
      "Epoch 217 training complete\n",
      "Cost on training data: 0.009124669202437723\n",
      "Accuracy on evaluation data: 8283 / 10000\n",
      "\n",
      "Epoch 218 training complete\n",
      "Cost on training data: 0.009076559111409073\n",
      "Accuracy on evaluation data: 8279 / 10000\n",
      "\n",
      "Epoch 219 training complete\n",
      "Cost on training data: 0.009024484992787352\n",
      "Accuracy on evaluation data: 8283 / 10000\n",
      "\n",
      "Epoch 220 training complete\n",
      "Cost on training data: 0.008978961060647977\n",
      "Accuracy on evaluation data: 8281 / 10000\n",
      "\n",
      "Epoch 221 training complete\n",
      "Cost on training data: 0.008928042600553764\n",
      "Accuracy on evaluation data: 8283 / 10000\n",
      "\n",
      "Epoch 222 training complete\n",
      "Cost on training data: 0.008881339288547521\n",
      "Accuracy on evaluation data: 8281 / 10000\n",
      "\n",
      "Epoch 223 training complete\n",
      "Cost on training data: 0.008834415838994258\n",
      "Accuracy on evaluation data: 8282 / 10000\n",
      "\n",
      "Epoch 224 training complete\n",
      "Cost on training data: 0.008787418659981543\n",
      "Accuracy on evaluation data: 8281 / 10000\n",
      "\n",
      "Epoch 225 training complete\n",
      "Cost on training data: 0.008741851747165482\n",
      "Accuracy on evaluation data: 8284 / 10000\n",
      "\n",
      "Epoch 226 training complete\n",
      "Cost on training data: 0.008696451496227017\n",
      "Accuracy on evaluation data: 8286 / 10000\n",
      "\n",
      "Epoch 227 training complete\n",
      "Cost on training data: 0.008650598299876982\n",
      "Accuracy on evaluation data: 8285 / 10000\n",
      "\n",
      "Epoch 228 training complete\n",
      "Cost on training data: 0.008605526480675467\n",
      "Accuracy on evaluation data: 8284 / 10000\n",
      "\n",
      "Epoch 229 training complete\n",
      "Cost on training data: 0.00856114489926893\n",
      "Accuracy on evaluation data: 8287 / 10000\n",
      "\n",
      "Epoch 230 training complete\n",
      "Cost on training data: 0.008517689675563898\n",
      "Accuracy on evaluation data: 8285 / 10000\n",
      "\n",
      "Epoch 231 training complete\n",
      "Cost on training data: 0.00847509916869195\n",
      "Accuracy on evaluation data: 8284 / 10000\n",
      "\n",
      "Epoch 232 training complete\n",
      "Cost on training data: 0.008432707507483565\n",
      "Accuracy on evaluation data: 8280 / 10000\n",
      "\n",
      "Epoch 233 training complete\n",
      "Cost on training data: 0.00839047126152384\n",
      "Accuracy on evaluation data: 8281 / 10000\n",
      "\n",
      "Epoch 234 training complete\n",
      "Cost on training data: 0.008345691051038103\n",
      "Accuracy on evaluation data: 8284 / 10000\n",
      "\n",
      "Epoch 235 training complete\n",
      "Cost on training data: 0.008304647421959268\n",
      "Accuracy on evaluation data: 8283 / 10000\n",
      "\n",
      "Epoch 236 training complete\n",
      "Cost on training data: 0.008264029114080028\n",
      "Accuracy on evaluation data: 8280 / 10000\n",
      "\n",
      "Epoch 237 training complete\n",
      "Cost on training data: 0.008221622646842416\n",
      "Accuracy on evaluation data: 8281 / 10000\n",
      "\n",
      "Epoch 238 training complete\n",
      "Cost on training data: 0.008181299675209246\n",
      "Accuracy on evaluation data: 8284 / 10000\n",
      "\n",
      "Epoch 239 training complete\n",
      "Cost on training data: 0.00814226304885849\n",
      "Accuracy on evaluation data: 8285 / 10000\n",
      "\n",
      "Epoch 240 training complete\n",
      "Cost on training data: 0.00809992877822682\n",
      "Accuracy on evaluation data: 8286 / 10000\n",
      "\n",
      "Epoch 241 training complete\n",
      "Cost on training data: 0.008059903769865397\n",
      "Accuracy on evaluation data: 8284 / 10000\n",
      "\n",
      "Epoch 242 training complete\n",
      "Cost on training data: 0.008020561229324054\n",
      "Accuracy on evaluation data: 8284 / 10000\n",
      "\n",
      "Epoch 243 training complete\n",
      "Cost on training data: 0.007982643467772646\n",
      "Accuracy on evaluation data: 8287 / 10000\n",
      "\n",
      "Epoch 244 training complete\n",
      "Cost on training data: 0.007942996470981576\n",
      "Accuracy on evaluation data: 8286 / 10000\n",
      "\n",
      "Epoch 245 training complete\n",
      "Cost on training data: 0.007904655580893455\n",
      "Accuracy on evaluation data: 8285 / 10000\n",
      "\n",
      "Epoch 246 training complete\n",
      "Cost on training data: 0.007866308819047848\n",
      "Accuracy on evaluation data: 8288 / 10000\n",
      "\n",
      "Epoch 247 training complete\n",
      "Cost on training data: 0.007829030865218375\n",
      "Accuracy on evaluation data: 8288 / 10000\n",
      "\n",
      "Epoch 248 training complete\n",
      "Cost on training data: 0.007791595395126328\n",
      "Accuracy on evaluation data: 8284 / 10000\n",
      "\n",
      "Epoch 249 training complete\n",
      "Cost on training data: 0.007754166638607026\n",
      "Accuracy on evaluation data: 8284 / 10000\n",
      "\n",
      "Epoch 250 training complete\n",
      "Cost on training data: 0.0077172654491075915\n",
      "Accuracy on evaluation data: 8285 / 10000\n",
      "\n",
      "Epoch 251 training complete\n",
      "Cost on training data: 0.00768138980821825\n",
      "Accuracy on evaluation data: 8289 / 10000\n",
      "\n",
      "Epoch 252 training complete\n",
      "Cost on training data: 0.0076455067306262356\n",
      "Accuracy on evaluation data: 8287 / 10000\n",
      "\n",
      "Epoch 253 training complete\n",
      "Cost on training data: 0.007609261741163228\n",
      "Accuracy on evaluation data: 8288 / 10000\n",
      "\n",
      "Epoch 254 training complete\n",
      "Cost on training data: 0.007576225837822074\n",
      "Accuracy on evaluation data: 8287 / 10000\n",
      "\n",
      "Epoch 255 training complete\n",
      "Cost on training data: 0.007539297937738278\n",
      "Accuracy on evaluation data: 8290 / 10000\n",
      "\n",
      "Epoch 256 training complete\n",
      "Cost on training data: 0.007505473060924103\n",
      "Accuracy on evaluation data: 8289 / 10000\n",
      "\n",
      "Epoch 257 training complete\n",
      "Cost on training data: 0.0074710618325655506\n",
      "Accuracy on evaluation data: 8289 / 10000\n",
      "\n",
      "Epoch 258 training complete\n",
      "Cost on training data: 0.00743735954045924\n",
      "Accuracy on evaluation data: 8287 / 10000\n",
      "\n",
      "Epoch 259 training complete\n",
      "Cost on training data: 0.007403792407303054\n",
      "Accuracy on evaluation data: 8290 / 10000\n",
      "\n",
      "Epoch 260 training complete\n",
      "Cost on training data: 0.0073703744290301715\n",
      "Accuracy on evaluation data: 8293 / 10000\n",
      "\n",
      "Epoch 261 training complete\n",
      "Cost on training data: 0.007337389974054715\n",
      "Accuracy on evaluation data: 8289 / 10000\n",
      "\n",
      "Epoch 262 training complete\n",
      "Cost on training data: 0.007305142618129047\n",
      "Accuracy on evaluation data: 8289 / 10000\n",
      "\n",
      "Epoch 263 training complete\n",
      "Cost on training data: 0.007273000851456612\n",
      "Accuracy on evaluation data: 8294 / 10000\n",
      "\n",
      "Epoch 264 training complete\n",
      "Cost on training data: 0.007241405390320589\n",
      "Accuracy on evaluation data: 8292 / 10000\n",
      "\n",
      "Epoch 265 training complete\n",
      "Cost on training data: 0.007209059221199927\n",
      "Accuracy on evaluation data: 8291 / 10000\n",
      "\n",
      "Epoch 266 training complete\n",
      "Cost on training data: 0.007178180959525674\n",
      "Accuracy on evaluation data: 8288 / 10000\n",
      "\n",
      "Epoch 267 training complete\n",
      "Cost on training data: 0.00714642705021936\n",
      "Accuracy on evaluation data: 8289 / 10000\n",
      "\n",
      "Epoch 268 training complete\n",
      "Cost on training data: 0.007115751996993352\n",
      "Accuracy on evaluation data: 8291 / 10000\n",
      "\n",
      "Epoch 269 training complete\n",
      "Cost on training data: 0.007085939264929981\n",
      "Accuracy on evaluation data: 8290 / 10000\n",
      "\n",
      "Epoch 270 training complete\n",
      "Cost on training data: 0.007054741235216446\n",
      "Accuracy on evaluation data: 8292 / 10000\n",
      "\n",
      "Epoch 271 training complete\n",
      "Cost on training data: 0.007024659502408967\n",
      "Accuracy on evaluation data: 8288 / 10000\n",
      "\n",
      "Epoch 272 training complete\n",
      "Cost on training data: 0.00699560855234288\n",
      "Accuracy on evaluation data: 8291 / 10000\n",
      "\n",
      "Epoch 273 training complete\n",
      "Cost on training data: 0.006966239219254633\n",
      "Accuracy on evaluation data: 8289 / 10000\n",
      "\n",
      "Epoch 274 training complete\n",
      "Cost on training data: 0.006937303035615813\n",
      "Accuracy on evaluation data: 8290 / 10000\n",
      "\n",
      "Epoch 275 training complete\n",
      "Cost on training data: 0.0069078998586838015\n",
      "Accuracy on evaluation data: 8290 / 10000\n",
      "\n",
      "Epoch 276 training complete\n",
      "Cost on training data: 0.006878968972911489\n",
      "Accuracy on evaluation data: 8292 / 10000\n",
      "\n",
      "Epoch 277 training complete\n",
      "Cost on training data: 0.006851134728407274\n",
      "Accuracy on evaluation data: 8290 / 10000\n",
      "\n",
      "Epoch 278 training complete\n",
      "Cost on training data: 0.006824524007466956\n",
      "Accuracy on evaluation data: 8288 / 10000\n",
      "\n",
      "Epoch 279 training complete\n",
      "Cost on training data: 0.006795275821932444\n",
      "Accuracy on evaluation data: 8288 / 10000\n",
      "\n",
      "Epoch 280 training complete\n",
      "Cost on training data: 0.006767326768873487\n",
      "Accuracy on evaluation data: 8289 / 10000\n",
      "\n",
      "Epoch 281 training complete\n",
      "Cost on training data: 0.006740101057282492\n",
      "Accuracy on evaluation data: 8293 / 10000\n",
      "\n",
      "Epoch 282 training complete\n",
      "Cost on training data: 0.006712859405393036\n",
      "Accuracy on evaluation data: 8290 / 10000\n",
      "\n",
      "Epoch 283 training complete\n",
      "Cost on training data: 0.006686606931601144\n",
      "Accuracy on evaluation data: 8289 / 10000\n",
      "\n",
      "Epoch 284 training complete\n",
      "Cost on training data: 0.006658588882674065\n",
      "Accuracy on evaluation data: 8291 / 10000\n",
      "\n",
      "Epoch 285 training complete\n",
      "Cost on training data: 0.006632359147167696\n",
      "Accuracy on evaluation data: 8294 / 10000\n",
      "\n",
      "Epoch 286 training complete\n",
      "Cost on training data: 0.006606000112920466\n",
      "Accuracy on evaluation data: 8290 / 10000\n",
      "\n",
      "Epoch 287 training complete\n",
      "Cost on training data: 0.006579729719764213\n",
      "Accuracy on evaluation data: 8292 / 10000\n",
      "\n",
      "Epoch 288 training complete\n",
      "Cost on training data: 0.006553716453881894\n",
      "Accuracy on evaluation data: 8292 / 10000\n",
      "\n",
      "Epoch 289 training complete\n",
      "Cost on training data: 0.006528195406524783\n",
      "Accuracy on evaluation data: 8294 / 10000\n",
      "\n",
      "Epoch 290 training complete\n",
      "Cost on training data: 0.006503121399879275\n",
      "Accuracy on evaluation data: 8289 / 10000\n",
      "\n",
      "Epoch 291 training complete\n",
      "Cost on training data: 0.006477385125046751\n",
      "Accuracy on evaluation data: 8291 / 10000\n",
      "\n",
      "Epoch 292 training complete\n",
      "Cost on training data: 0.006452693732827709\n",
      "Accuracy on evaluation data: 8294 / 10000\n",
      "\n",
      "Epoch 293 training complete\n",
      "Cost on training data: 0.006427270924614734\n",
      "Accuracy on evaluation data: 8294 / 10000\n",
      "\n",
      "Epoch 294 training complete\n",
      "Cost on training data: 0.006402462250104543\n",
      "Accuracy on evaluation data: 8293 / 10000\n",
      "\n",
      "Epoch 295 training complete\n",
      "Cost on training data: 0.006378538670285407\n",
      "Accuracy on evaluation data: 8292 / 10000\n",
      "\n",
      "Epoch 296 training complete\n",
      "Cost on training data: 0.00635381342947841\n",
      "Accuracy on evaluation data: 8292 / 10000\n",
      "\n",
      "Epoch 297 training complete\n",
      "Cost on training data: 0.0063295086138492815\n",
      "Accuracy on evaluation data: 8294 / 10000\n",
      "\n",
      "Epoch 298 training complete\n",
      "Cost on training data: 0.006306265957721258\n",
      "Accuracy on evaluation data: 8293 / 10000\n",
      "\n",
      "Epoch 299 training complete\n",
      "Cost on training data: 0.006282875895998422\n",
      "Accuracy on evaluation data: 8294 / 10000\n",
      "\n",
      "Epoch 300 training complete\n",
      "Cost on training data: 0.006259666071973251\n",
      "Accuracy on evaluation data: 8293 / 10000\n",
      "\n",
      "Epoch 301 training complete\n",
      "Cost on training data: 0.0062351599599945505\n",
      "Accuracy on evaluation data: 8296 / 10000\n",
      "\n",
      "Epoch 302 training complete\n",
      "Cost on training data: 0.006211760219188602\n",
      "Accuracy on evaluation data: 8297 / 10000\n",
      "\n",
      "Epoch 303 training complete\n",
      "Cost on training data: 0.006188760097901691\n",
      "Accuracy on evaluation data: 8297 / 10000\n",
      "\n",
      "Epoch 304 training complete\n",
      "Cost on training data: 0.006165744662315175\n",
      "Accuracy on evaluation data: 8296 / 10000\n",
      "\n",
      "Epoch 305 training complete\n",
      "Cost on training data: 0.006143109963970335\n",
      "Accuracy on evaluation data: 8295 / 10000\n",
      "\n",
      "Epoch 306 training complete\n",
      "Cost on training data: 0.0061208387916707185\n",
      "Accuracy on evaluation data: 8295 / 10000\n",
      "\n",
      "Epoch 307 training complete\n",
      "Cost on training data: 0.006098470186107615\n",
      "Accuracy on evaluation data: 8294 / 10000\n",
      "\n",
      "Epoch 308 training complete\n",
      "Cost on training data: 0.006076073059186687\n",
      "Accuracy on evaluation data: 8294 / 10000\n",
      "\n",
      "Epoch 309 training complete\n",
      "Cost on training data: 0.00605384771049891\n",
      "Accuracy on evaluation data: 8295 / 10000\n",
      "\n",
      "Epoch 310 training complete\n",
      "Cost on training data: 0.006032384639240124\n",
      "Accuracy on evaluation data: 8295 / 10000\n",
      "\n",
      "Epoch 311 training complete\n",
      "Cost on training data: 0.006010308701056035\n",
      "Accuracy on evaluation data: 8296 / 10000\n",
      "\n",
      "Epoch 312 training complete\n",
      "Cost on training data: 0.005989195659861959\n",
      "Accuracy on evaluation data: 8297 / 10000\n",
      "\n",
      "Epoch 313 training complete\n",
      "Cost on training data: 0.005967732903878569\n",
      "Accuracy on evaluation data: 8295 / 10000\n",
      "\n",
      "Epoch 314 training complete\n",
      "Cost on training data: 0.00594599735087798\n",
      "Accuracy on evaluation data: 8293 / 10000\n",
      "\n",
      "Epoch 315 training complete\n",
      "Cost on training data: 0.005925334737525032\n",
      "Accuracy on evaluation data: 8292 / 10000\n",
      "\n",
      "Epoch 316 training complete\n",
      "Cost on training data: 0.005904296776278317\n",
      "Accuracy on evaluation data: 8294 / 10000\n",
      "\n",
      "Epoch 317 training complete\n",
      "Cost on training data: 0.005883311781328661\n",
      "Accuracy on evaluation data: 8293 / 10000\n",
      "\n",
      "Epoch 318 training complete\n",
      "Cost on training data: 0.005862388768622765\n",
      "Accuracy on evaluation data: 8292 / 10000\n",
      "\n",
      "Epoch 319 training complete\n",
      "Cost on training data: 0.005841883893677132\n",
      "Accuracy on evaluation data: 8293 / 10000\n",
      "\n",
      "Epoch 320 training complete\n",
      "Cost on training data: 0.005821816133287035\n",
      "Accuracy on evaluation data: 8293 / 10000\n",
      "\n",
      "Epoch 321 training complete\n",
      "Cost on training data: 0.0058017275889092\n",
      "Accuracy on evaluation data: 8294 / 10000\n",
      "\n",
      "Epoch 322 training complete\n",
      "Cost on training data: 0.005781465253992844\n",
      "Accuracy on evaluation data: 8296 / 10000\n",
      "\n",
      "Epoch 323 training complete\n",
      "Cost on training data: 0.005761879565570262\n",
      "Accuracy on evaluation data: 8297 / 10000\n",
      "\n",
      "Epoch 324 training complete\n",
      "Cost on training data: 0.005742220114040024\n",
      "Accuracy on evaluation data: 8295 / 10000\n",
      "\n",
      "Epoch 325 training complete\n",
      "Cost on training data: 0.005722057044985432\n",
      "Accuracy on evaluation data: 8297 / 10000\n",
      "\n",
      "Epoch 326 training complete\n",
      "Cost on training data: 0.005702492767969574\n",
      "Accuracy on evaluation data: 8297 / 10000\n",
      "\n",
      "Epoch 327 training complete\n",
      "Cost on training data: 0.005682837914331534\n",
      "Accuracy on evaluation data: 8296 / 10000\n",
      "\n",
      "Epoch 328 training complete\n",
      "Cost on training data: 0.005663757358860105\n",
      "Accuracy on evaluation data: 8296 / 10000\n",
      "\n",
      "Epoch 329 training complete\n",
      "Cost on training data: 0.005645130418711626\n",
      "Accuracy on evaluation data: 8299 / 10000\n",
      "\n",
      "Epoch 330 training complete\n",
      "Cost on training data: 0.005625376286921752\n",
      "Accuracy on evaluation data: 8297 / 10000\n",
      "\n",
      "Epoch 331 training complete\n",
      "Cost on training data: 0.0056063298213567874\n",
      "Accuracy on evaluation data: 8293 / 10000\n",
      "\n",
      "Epoch 332 training complete\n",
      "Cost on training data: 0.005587546964889977\n",
      "Accuracy on evaluation data: 8296 / 10000\n",
      "\n",
      "Epoch 333 training complete\n",
      "Cost on training data: 0.0055691478047687855\n",
      "Accuracy on evaluation data: 8295 / 10000\n",
      "\n",
      "Epoch 334 training complete\n",
      "Cost on training data: 0.005550436725109379\n",
      "Accuracy on evaluation data: 8296 / 10000\n",
      "\n",
      "Epoch 335 training complete\n",
      "Cost on training data: 0.005532123302777824\n",
      "Accuracy on evaluation data: 8296 / 10000\n",
      "\n",
      "Epoch 336 training complete\n",
      "Cost on training data: 0.005513867757132915\n",
      "Accuracy on evaluation data: 8296 / 10000\n",
      "\n",
      "Epoch 337 training complete\n",
      "Cost on training data: 0.0054957612012706495\n",
      "Accuracy on evaluation data: 8298 / 10000\n",
      "\n",
      "Epoch 338 training complete\n",
      "Cost on training data: 0.0054776296745452525\n",
      "Accuracy on evaluation data: 8296 / 10000\n",
      "\n",
      "Epoch 339 training complete\n",
      "Cost on training data: 0.005459663342369199\n",
      "Accuracy on evaluation data: 8298 / 10000\n",
      "\n",
      "Epoch 340 training complete\n",
      "Cost on training data: 0.005441820015345867\n",
      "Accuracy on evaluation data: 8298 / 10000\n",
      "\n",
      "Epoch 341 training complete\n",
      "Cost on training data: 0.005423965482465211\n",
      "Accuracy on evaluation data: 8298 / 10000\n",
      "\n",
      "Epoch 342 training complete\n",
      "Cost on training data: 0.005406576949041802\n",
      "Accuracy on evaluation data: 8297 / 10000\n",
      "\n",
      "Epoch 343 training complete\n",
      "Cost on training data: 0.0053891692863244495\n",
      "Accuracy on evaluation data: 8299 / 10000\n",
      "\n",
      "Epoch 344 training complete\n",
      "Cost on training data: 0.005371872424942558\n",
      "Accuracy on evaluation data: 8300 / 10000\n",
      "\n",
      "Epoch 345 training complete\n",
      "Cost on training data: 0.005354265073981473\n",
      "Accuracy on evaluation data: 8298 / 10000\n",
      "\n",
      "Epoch 346 training complete\n",
      "Cost on training data: 0.005337134298913535\n",
      "Accuracy on evaluation data: 8298 / 10000\n",
      "\n",
      "Epoch 347 training complete\n",
      "Cost on training data: 0.005319877767829581\n",
      "Accuracy on evaluation data: 8297 / 10000\n",
      "\n",
      "Epoch 348 training complete\n",
      "Cost on training data: 0.005302868767466762\n",
      "Accuracy on evaluation data: 8299 / 10000\n",
      "\n",
      "Epoch 349 training complete\n",
      "Cost on training data: 0.005286111971510666\n",
      "Accuracy on evaluation data: 8299 / 10000\n",
      "\n",
      "Epoch 350 training complete\n",
      "Cost on training data: 0.0052695662130890835\n",
      "Accuracy on evaluation data: 8301 / 10000\n",
      "\n",
      "Epoch 351 training complete\n",
      "Cost on training data: 0.005252927084988284\n",
      "Accuracy on evaluation data: 8296 / 10000\n",
      "\n",
      "Epoch 352 training complete\n",
      "Cost on training data: 0.005236162998019647\n",
      "Accuracy on evaluation data: 8300 / 10000\n",
      "\n",
      "Epoch 353 training complete\n",
      "Cost on training data: 0.0052195362986898\n",
      "Accuracy on evaluation data: 8299 / 10000\n",
      "\n",
      "Epoch 354 training complete\n",
      "Cost on training data: 0.005203213498142\n",
      "Accuracy on evaluation data: 8300 / 10000\n",
      "\n",
      "Epoch 355 training complete\n",
      "Cost on training data: 0.005186989656519883\n",
      "Accuracy on evaluation data: 8301 / 10000\n",
      "\n",
      "Epoch 356 training complete\n",
      "Cost on training data: 0.005170600656431008\n",
      "Accuracy on evaluation data: 8299 / 10000\n",
      "\n",
      "Epoch 357 training complete\n",
      "Cost on training data: 0.005154636102651157\n",
      "Accuracy on evaluation data: 8299 / 10000\n",
      "\n",
      "Epoch 358 training complete\n",
      "Cost on training data: 0.005138503331682069\n",
      "Accuracy on evaluation data: 8301 / 10000\n",
      "\n",
      "Epoch 359 training complete\n",
      "Cost on training data: 0.005122636146680291\n",
      "Accuracy on evaluation data: 8301 / 10000\n",
      "\n",
      "Epoch 360 training complete\n",
      "Cost on training data: 0.005107179100350274\n",
      "Accuracy on evaluation data: 8301 / 10000\n",
      "\n",
      "Epoch 361 training complete\n",
      "Cost on training data: 0.005091125248613963\n",
      "Accuracy on evaluation data: 8300 / 10000\n",
      "\n",
      "Epoch 362 training complete\n",
      "Cost on training data: 0.0050752932278846025\n",
      "Accuracy on evaluation data: 8302 / 10000\n",
      "\n",
      "Epoch 363 training complete\n",
      "Cost on training data: 0.005059967936449249\n",
      "Accuracy on evaluation data: 8298 / 10000\n",
      "\n",
      "Epoch 364 training complete\n",
      "Cost on training data: 0.0050441906462594865\n",
      "Accuracy on evaluation data: 8302 / 10000\n",
      "\n",
      "Epoch 365 training complete\n",
      "Cost on training data: 0.005028794774735418\n",
      "Accuracy on evaluation data: 8302 / 10000\n",
      "\n",
      "Epoch 366 training complete\n",
      "Cost on training data: 0.005013490980126247\n",
      "Accuracy on evaluation data: 8300 / 10000\n",
      "\n",
      "Epoch 367 training complete\n",
      "Cost on training data: 0.0049983250945747565\n",
      "Accuracy on evaluation data: 8302 / 10000\n",
      "\n",
      "Epoch 368 training complete\n",
      "Cost on training data: 0.004983033380300337\n",
      "Accuracy on evaluation data: 8302 / 10000\n",
      "\n",
      "Epoch 369 training complete\n",
      "Cost on training data: 0.0049679865232092465\n",
      "Accuracy on evaluation data: 8301 / 10000\n",
      "\n",
      "Epoch 370 training complete\n",
      "Cost on training data: 0.004952825258325817\n",
      "Accuracy on evaluation data: 8300 / 10000\n",
      "\n",
      "Epoch 371 training complete\n",
      "Cost on training data: 0.0049379611507670024\n",
      "Accuracy on evaluation data: 8299 / 10000\n",
      "\n",
      "Epoch 372 training complete\n",
      "Cost on training data: 0.0049232286140285745\n",
      "Accuracy on evaluation data: 8298 / 10000\n",
      "\n",
      "Epoch 373 training complete\n",
      "Cost on training data: 0.004908383587618016\n",
      "Accuracy on evaluation data: 8299 / 10000\n",
      "\n",
      "Epoch 374 training complete\n",
      "Cost on training data: 0.004893667597355958\n",
      "Accuracy on evaluation data: 8299 / 10000\n",
      "\n",
      "Epoch 375 training complete\n",
      "Cost on training data: 0.0048788150975697525\n",
      "Accuracy on evaluation data: 8298 / 10000\n",
      "\n",
      "Epoch 376 training complete\n",
      "Cost on training data: 0.004864468031649096\n",
      "Accuracy on evaluation data: 8303 / 10000\n",
      "\n",
      "Epoch 377 training complete\n",
      "Cost on training data: 0.004849783301688879\n",
      "Accuracy on evaluation data: 8301 / 10000\n",
      "\n",
      "Epoch 378 training complete\n",
      "Cost on training data: 0.004835706917662206\n",
      "Accuracy on evaluation data: 8302 / 10000\n",
      "\n",
      "Epoch 379 training complete\n",
      "Cost on training data: 0.004821077987207894\n",
      "Accuracy on evaluation data: 8300 / 10000\n",
      "\n",
      "Epoch 380 training complete\n",
      "Cost on training data: 0.004806721090995966\n",
      "Accuracy on evaluation data: 8301 / 10000\n",
      "\n",
      "Epoch 381 training complete\n",
      "Cost on training data: 0.004792534707439629\n",
      "Accuracy on evaluation data: 8300 / 10000\n",
      "\n",
      "Epoch 382 training complete\n",
      "Cost on training data: 0.004778374973825806\n",
      "Accuracy on evaluation data: 8302 / 10000\n",
      "\n",
      "Epoch 383 training complete\n",
      "Cost on training data: 0.004764287221635387\n",
      "Accuracy on evaluation data: 8302 / 10000\n",
      "\n",
      "Epoch 384 training complete\n",
      "Cost on training data: 0.004750489611046982\n",
      "Accuracy on evaluation data: 8300 / 10000\n",
      "\n",
      "Epoch 385 training complete\n",
      "Cost on training data: 0.004736451818458986\n",
      "Accuracy on evaluation data: 8300 / 10000\n",
      "\n",
      "Epoch 386 training complete\n",
      "Cost on training data: 0.004722660076487001\n",
      "Accuracy on evaluation data: 8303 / 10000\n",
      "\n",
      "Epoch 387 training complete\n",
      "Cost on training data: 0.004709326042015866\n",
      "Accuracy on evaluation data: 8304 / 10000\n",
      "\n",
      "Epoch 388 training complete\n",
      "Cost on training data: 0.004695266938328684\n",
      "Accuracy on evaluation data: 8303 / 10000\n",
      "\n",
      "Epoch 389 training complete\n",
      "Cost on training data: 0.004681608224044033\n",
      "Accuracy on evaluation data: 8305 / 10000\n",
      "\n",
      "Epoch 390 training complete\n",
      "Cost on training data: 0.004667977817009257\n",
      "Accuracy on evaluation data: 8302 / 10000\n",
      "\n",
      "Epoch 391 training complete\n",
      "Cost on training data: 0.004654565485379877\n",
      "Accuracy on evaluation data: 8302 / 10000\n",
      "\n",
      "Epoch 392 training complete\n",
      "Cost on training data: 0.004641114376086823\n",
      "Accuracy on evaluation data: 8301 / 10000\n",
      "\n",
      "Epoch 393 training complete\n",
      "Cost on training data: 0.0046278714181218535\n",
      "Accuracy on evaluation data: 8303 / 10000\n",
      "\n",
      "Epoch 394 training complete\n",
      "Cost on training data: 0.004614647230259249\n",
      "Accuracy on evaluation data: 8301 / 10000\n",
      "\n",
      "Epoch 395 training complete\n",
      "Cost on training data: 0.004601464804781484\n",
      "Accuracy on evaluation data: 8300 / 10000\n",
      "\n",
      "Epoch 396 training complete\n",
      "Cost on training data: 0.004588525634705919\n",
      "Accuracy on evaluation data: 8302 / 10000\n",
      "\n",
      "Epoch 397 training complete\n",
      "Cost on training data: 0.004575372051866537\n",
      "Accuracy on evaluation data: 8302 / 10000\n",
      "\n",
      "Epoch 398 training complete\n",
      "Cost on training data: 0.00456246639208315\n",
      "Accuracy on evaluation data: 8302 / 10000\n",
      "\n",
      "Epoch 399 training complete\n",
      "Cost on training data: 0.004549652706350429\n",
      "Accuracy on evaluation data: 8301 / 10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [5278,\n",
       "  6636,\n",
       "  7114,\n",
       "  7387,\n",
       "  7423,\n",
       "  7635,\n",
       "  7865,\n",
       "  7871,\n",
       "  7967,\n",
       "  7971,\n",
       "  7993,\n",
       "  8053,\n",
       "  8036,\n",
       "  8090,\n",
       "  8092,\n",
       "  8034,\n",
       "  8094,\n",
       "  8108,\n",
       "  8131,\n",
       "  8141,\n",
       "  8139,\n",
       "  8118,\n",
       "  8118,\n",
       "  8150,\n",
       "  8149,\n",
       "  8152,\n",
       "  8155,\n",
       "  8159,\n",
       "  8164,\n",
       "  8149,\n",
       "  8182,\n",
       "  8164,\n",
       "  8181,\n",
       "  8179,\n",
       "  8175,\n",
       "  8183,\n",
       "  8177,\n",
       "  8183,\n",
       "  8215,\n",
       "  8189,\n",
       "  8194,\n",
       "  8211,\n",
       "  8213,\n",
       "  8229,\n",
       "  8222,\n",
       "  8221,\n",
       "  8207,\n",
       "  8203,\n",
       "  8234,\n",
       "  8236,\n",
       "  8240,\n",
       "  8231,\n",
       "  8226,\n",
       "  8230,\n",
       "  8223,\n",
       "  8228,\n",
       "  8230,\n",
       "  8234,\n",
       "  8215,\n",
       "  8227,\n",
       "  8230,\n",
       "  8219,\n",
       "  8225,\n",
       "  8238,\n",
       "  8228,\n",
       "  8225,\n",
       "  8235,\n",
       "  8232,\n",
       "  8227,\n",
       "  8239,\n",
       "  8231,\n",
       "  8231,\n",
       "  8230,\n",
       "  8245,\n",
       "  8240,\n",
       "  8246,\n",
       "  8232,\n",
       "  8249,\n",
       "  8236,\n",
       "  8243,\n",
       "  8239,\n",
       "  8256,\n",
       "  8247,\n",
       "  8255,\n",
       "  8252,\n",
       "  8255,\n",
       "  8252,\n",
       "  8253,\n",
       "  8254,\n",
       "  8253,\n",
       "  8252,\n",
       "  8259,\n",
       "  8250,\n",
       "  8260,\n",
       "  8254,\n",
       "  8258,\n",
       "  8256,\n",
       "  8251,\n",
       "  8255,\n",
       "  8255,\n",
       "  8253,\n",
       "  8247,\n",
       "  8257,\n",
       "  8252,\n",
       "  8253,\n",
       "  8259,\n",
       "  8258,\n",
       "  8250,\n",
       "  8253,\n",
       "  8256,\n",
       "  8258,\n",
       "  8254,\n",
       "  8252,\n",
       "  8252,\n",
       "  8252,\n",
       "  8253,\n",
       "  8246,\n",
       "  8251,\n",
       "  8250,\n",
       "  8253,\n",
       "  8251,\n",
       "  8260,\n",
       "  8253,\n",
       "  8254,\n",
       "  8255,\n",
       "  8255,\n",
       "  8259,\n",
       "  8256,\n",
       "  8256,\n",
       "  8257,\n",
       "  8257,\n",
       "  8259,\n",
       "  8261,\n",
       "  8259,\n",
       "  8261,\n",
       "  8264,\n",
       "  8265,\n",
       "  8266,\n",
       "  8256,\n",
       "  8260,\n",
       "  8257,\n",
       "  8260,\n",
       "  8258,\n",
       "  8260,\n",
       "  8265,\n",
       "  8261,\n",
       "  8266,\n",
       "  8266,\n",
       "  8266,\n",
       "  8264,\n",
       "  8267,\n",
       "  8270,\n",
       "  8262,\n",
       "  8269,\n",
       "  8268,\n",
       "  8269,\n",
       "  8267,\n",
       "  8268,\n",
       "  8271,\n",
       "  8268,\n",
       "  8269,\n",
       "  8275,\n",
       "  8269,\n",
       "  8270,\n",
       "  8273,\n",
       "  8273,\n",
       "  8286,\n",
       "  8277,\n",
       "  8272,\n",
       "  8275,\n",
       "  8275,\n",
       "  8274,\n",
       "  8272,\n",
       "  8279,\n",
       "  8277,\n",
       "  8275,\n",
       "  8274,\n",
       "  8272,\n",
       "  8281,\n",
       "  8276,\n",
       "  8280,\n",
       "  8278,\n",
       "  8276,\n",
       "  8282,\n",
       "  8276,\n",
       "  8273,\n",
       "  8274,\n",
       "  8279,\n",
       "  8278,\n",
       "  8276,\n",
       "  8277,\n",
       "  8282,\n",
       "  8277,\n",
       "  8274,\n",
       "  8274,\n",
       "  8275,\n",
       "  8278,\n",
       "  8276,\n",
       "  8283,\n",
       "  8275,\n",
       "  8276,\n",
       "  8275,\n",
       "  8274,\n",
       "  8283,\n",
       "  8280,\n",
       "  8276,\n",
       "  8278,\n",
       "  8277,\n",
       "  8278,\n",
       "  8275,\n",
       "  8277,\n",
       "  8279,\n",
       "  8279,\n",
       "  8278,\n",
       "  8283,\n",
       "  8280,\n",
       "  8282,\n",
       "  8283,\n",
       "  8279,\n",
       "  8283,\n",
       "  8281,\n",
       "  8283,\n",
       "  8281,\n",
       "  8282,\n",
       "  8281,\n",
       "  8284,\n",
       "  8286,\n",
       "  8285,\n",
       "  8284,\n",
       "  8287,\n",
       "  8285,\n",
       "  8284,\n",
       "  8280,\n",
       "  8281,\n",
       "  8284,\n",
       "  8283,\n",
       "  8280,\n",
       "  8281,\n",
       "  8284,\n",
       "  8285,\n",
       "  8286,\n",
       "  8284,\n",
       "  8284,\n",
       "  8287,\n",
       "  8286,\n",
       "  8285,\n",
       "  8288,\n",
       "  8288,\n",
       "  8284,\n",
       "  8284,\n",
       "  8285,\n",
       "  8289,\n",
       "  8287,\n",
       "  8288,\n",
       "  8287,\n",
       "  8290,\n",
       "  8289,\n",
       "  8289,\n",
       "  8287,\n",
       "  8290,\n",
       "  8293,\n",
       "  8289,\n",
       "  8289,\n",
       "  8294,\n",
       "  8292,\n",
       "  8291,\n",
       "  8288,\n",
       "  8289,\n",
       "  8291,\n",
       "  8290,\n",
       "  8292,\n",
       "  8288,\n",
       "  8291,\n",
       "  8289,\n",
       "  8290,\n",
       "  8290,\n",
       "  8292,\n",
       "  8290,\n",
       "  8288,\n",
       "  8288,\n",
       "  8289,\n",
       "  8293,\n",
       "  8290,\n",
       "  8289,\n",
       "  8291,\n",
       "  8294,\n",
       "  8290,\n",
       "  8292,\n",
       "  8292,\n",
       "  8294,\n",
       "  8289,\n",
       "  8291,\n",
       "  8294,\n",
       "  8294,\n",
       "  8293,\n",
       "  8292,\n",
       "  8292,\n",
       "  8294,\n",
       "  8293,\n",
       "  8294,\n",
       "  8293,\n",
       "  8296,\n",
       "  8297,\n",
       "  8297,\n",
       "  8296,\n",
       "  8295,\n",
       "  8295,\n",
       "  8294,\n",
       "  8294,\n",
       "  8295,\n",
       "  8295,\n",
       "  8296,\n",
       "  8297,\n",
       "  8295,\n",
       "  8293,\n",
       "  8292,\n",
       "  8294,\n",
       "  8293,\n",
       "  8292,\n",
       "  8293,\n",
       "  8293,\n",
       "  8294,\n",
       "  8296,\n",
       "  8297,\n",
       "  8295,\n",
       "  8297,\n",
       "  8297,\n",
       "  8296,\n",
       "  8296,\n",
       "  8299,\n",
       "  8297,\n",
       "  8293,\n",
       "  8296,\n",
       "  8295,\n",
       "  8296,\n",
       "  8296,\n",
       "  8296,\n",
       "  8298,\n",
       "  8296,\n",
       "  8298,\n",
       "  8298,\n",
       "  8298,\n",
       "  8297,\n",
       "  8299,\n",
       "  8300,\n",
       "  8298,\n",
       "  8298,\n",
       "  8297,\n",
       "  8299,\n",
       "  8299,\n",
       "  8301,\n",
       "  8296,\n",
       "  8300,\n",
       "  8299,\n",
       "  8300,\n",
       "  8301,\n",
       "  8299,\n",
       "  8299,\n",
       "  8301,\n",
       "  8301,\n",
       "  8301,\n",
       "  8300,\n",
       "  8302,\n",
       "  8298,\n",
       "  8302,\n",
       "  8302,\n",
       "  8300,\n",
       "  8302,\n",
       "  8302,\n",
       "  8301,\n",
       "  8300,\n",
       "  8299,\n",
       "  8298,\n",
       "  8299,\n",
       "  8299,\n",
       "  8298,\n",
       "  8303,\n",
       "  8301,\n",
       "  8302,\n",
       "  8300,\n",
       "  8301,\n",
       "  8300,\n",
       "  8302,\n",
       "  8302,\n",
       "  8300,\n",
       "  8300,\n",
       "  8303,\n",
       "  8304,\n",
       "  8303,\n",
       "  8305,\n",
       "  8302,\n",
       "  8302,\n",
       "  8301,\n",
       "  8303,\n",
       "  8301,\n",
       "  8300,\n",
       "  8302,\n",
       "  8302,\n",
       "  8302,\n",
       "  8301],\n",
       " [1.8937980110453954,\n",
       "  1.3844676075671265,\n",
       "  1.1532897293201265,\n",
       "  0.95922277541867629,\n",
       "  0.87595236825307465,\n",
       "  0.73878487864974574,\n",
       "  0.64592951641511009,\n",
       "  0.59864295803524614,\n",
       "  0.53789635223954047,\n",
       "  0.48644219798365518,\n",
       "  0.4561196691841285,\n",
       "  0.41603751775516917,\n",
       "  0.37817303413324976,\n",
       "  0.34864653250890593,\n",
       "  0.31602232367451966,\n",
       "  0.33545049983640857,\n",
       "  0.28391759915471299,\n",
       "  0.27398740993166726,\n",
       "  0.25564196798953531,\n",
       "  0.23736620423949142,\n",
       "  0.23016297580538064,\n",
       "  0.2160255790803321,\n",
       "  0.20732322140761308,\n",
       "  0.19871446666109482,\n",
       "  0.19154825073647341,\n",
       "  0.18038908830382522,\n",
       "  0.17573245847761196,\n",
       "  0.16850771733810471,\n",
       "  0.16129646905166589,\n",
       "  0.16021407103796856,\n",
       "  0.15352097332952505,\n",
       "  0.14779254116010826,\n",
       "  0.14348165629746717,\n",
       "  0.13541136236961307,\n",
       "  0.13666120517143182,\n",
       "  0.12264801525785922,\n",
       "  0.12937460879831567,\n",
       "  0.1159025786001395,\n",
       "  0.11050240641857254,\n",
       "  0.1094927818046913,\n",
       "  0.10385178734627594,\n",
       "  0.099327215967321733,\n",
       "  0.096494474259652421,\n",
       "  0.091788637236323495,\n",
       "  0.088788844405893783,\n",
       "  0.083848633281622259,\n",
       "  0.082768377419714115,\n",
       "  0.081102188769281214,\n",
       "  0.076114323927304009,\n",
       "  0.073815164266560526,\n",
       "  0.071139247013520174,\n",
       "  0.069116689989954017,\n",
       "  0.066942323329617054,\n",
       "  0.065087749287490393,\n",
       "  0.063715272356379485,\n",
       "  0.06205633454940486,\n",
       "  0.060369575754091256,\n",
       "  0.058132231460776888,\n",
       "  0.057065528928577296,\n",
       "  0.055164381586352423,\n",
       "  0.054049357792255004,\n",
       "  0.053106923435128267,\n",
       "  0.051906049358039566,\n",
       "  0.050405422926561999,\n",
       "  0.049392173050692871,\n",
       "  0.048244714328417863,\n",
       "  0.047464474248840045,\n",
       "  0.046351763163870401,\n",
       "  0.045161377617824357,\n",
       "  0.044109289667368969,\n",
       "  0.042942839446748961,\n",
       "  0.042267581600765387,\n",
       "  0.041332235891044698,\n",
       "  0.040480882378981162,\n",
       "  0.0395850107519111,\n",
       "  0.039038102352925624,\n",
       "  0.038218432023149958,\n",
       "  0.037558598563630821,\n",
       "  0.036769292098142475,\n",
       "  0.036039584943953255,\n",
       "  0.035417159168461082,\n",
       "  0.034658081647001029,\n",
       "  0.034119270921404321,\n",
       "  0.033299544337982157,\n",
       "  0.032791975236880912,\n",
       "  0.03241143918860559,\n",
       "  0.031766174427852505,\n",
       "  0.031249639996462939,\n",
       "  0.030729141780640241,\n",
       "  0.030295824048034065,\n",
       "  0.029812861970101464,\n",
       "  0.02947700235160686,\n",
       "  0.028918276386841968,\n",
       "  0.028597790660603774,\n",
       "  0.028229019247391272,\n",
       "  0.027760765162782912,\n",
       "  0.02736973233092448,\n",
       "  0.026961822487068507,\n",
       "  0.026624399512978892,\n",
       "  0.026244253802113551,\n",
       "  0.025903500937351658,\n",
       "  0.025611366488493933,\n",
       "  0.025241771775940065,\n",
       "  0.024885107479477947,\n",
       "  0.024565243126889649,\n",
       "  0.024245194591268615,\n",
       "  0.023946880877638011,\n",
       "  0.023641655662521383,\n",
       "  0.023317065847187356,\n",
       "  0.023036452537305543,\n",
       "  0.022717283886112341,\n",
       "  0.022433508337322126,\n",
       "  0.022138927493056716,\n",
       "  0.021847477091533157,\n",
       "  0.021596556370190315,\n",
       "  0.021265541705343931,\n",
       "  0.021022195300076887,\n",
       "  0.020712632062297471,\n",
       "  0.02044431020533689,\n",
       "  0.020151627362765979,\n",
       "  0.019905072739548706,\n",
       "  0.019682057893061281,\n",
       "  0.019420544531158289,\n",
       "  0.019181300533654553,\n",
       "  0.018960153498224774,\n",
       "  0.018732601985158963,\n",
       "  0.018529797962629876,\n",
       "  0.018309985630572984,\n",
       "  0.018099696318601432,\n",
       "  0.017901246070726479,\n",
       "  0.017716673597768236,\n",
       "  0.017528981278823588,\n",
       "  0.017351674466078482,\n",
       "  0.017168421987334977,\n",
       "  0.016987473337896027,\n",
       "  0.016804774369524451,\n",
       "  0.016646408013425901,\n",
       "  0.016464454861598789,\n",
       "  0.016334590215237935,\n",
       "  0.016133238094704159,\n",
       "  0.015976008124056914,\n",
       "  0.015806263774614207,\n",
       "  0.015625162153222506,\n",
       "  0.015462727480563716,\n",
       "  0.01529877424857819,\n",
       "  0.01513845920881712,\n",
       "  0.014995663041250143,\n",
       "  0.014834922238819147,\n",
       "  0.014699590554560854,\n",
       "  0.014555570705052312,\n",
       "  0.014419251025301743,\n",
       "  0.014291816663632858,\n",
       "  0.014168831450801569,\n",
       "  0.0140368244866699,\n",
       "  0.013930103717222884,\n",
       "  0.013793617924768105,\n",
       "  0.013681326440411583,\n",
       "  0.013565996816637043,\n",
       "  0.013451952397087791,\n",
       "  0.013347605619578883,\n",
       "  0.013231999426233175,\n",
       "  0.013132857418653531,\n",
       "  0.013028509579888003,\n",
       "  0.012928239213846778,\n",
       "  0.012821690358913569,\n",
       "  0.01271867386866192,\n",
       "  0.0126293168602396,\n",
       "  0.012528847958586583,\n",
       "  0.012435056927550391,\n",
       "  0.012343935901510337,\n",
       "  0.012253377206393084,\n",
       "  0.012158970109384973,\n",
       "  0.012071721567343539,\n",
       "  0.011981227995580469,\n",
       "  0.011898168471260889,\n",
       "  0.011815027154437351,\n",
       "  0.011731511799923282,\n",
       "  0.011650838927478003,\n",
       "  0.011563596238702395,\n",
       "  0.01148493218319778,\n",
       "  0.011404095554794755,\n",
       "  0.011327435129394631,\n",
       "  0.01125348808737052,\n",
       "  0.011178709192642836,\n",
       "  0.011102234658851738,\n",
       "  0.011028218791294762,\n",
       "  0.010958064878876564,\n",
       "  0.010886007158233339,\n",
       "  0.01081740915401427,\n",
       "  0.010746628707759392,\n",
       "  0.010678440736796465,\n",
       "  0.010612007665652902,\n",
       "  0.010544133424962295,\n",
       "  0.010478942986256483,\n",
       "  0.010415399793566033,\n",
       "  0.010354488078336159,\n",
       "  0.010290478112000659,\n",
       "  0.010225506424346446,\n",
       "  0.010170670957606258,\n",
       "  0.010104107251668513,\n",
       "  0.010045431289979338,\n",
       "  0.0099843314236771567,\n",
       "  0.0099269908782548249,\n",
       "  0.0098738127374631661,\n",
       "  0.0098119932357512308,\n",
       "  0.0097558055844568781,\n",
       "  0.0096987169758611658,\n",
       "  0.0096458888613695885,\n",
       "  0.0095878381482682497,\n",
       "  0.0095365465019780944,\n",
       "  0.0094809262238249436,\n",
       "  0.0094298538657252816,\n",
       "  0.0093753703789979329,\n",
       "  0.0093246921362410695,\n",
       "  0.0092759935778157915,\n",
       "  0.0092229964779172776,\n",
       "  0.0091722744894501846,\n",
       "  0.009124669202437723,\n",
       "  0.0090765591114090726,\n",
       "  0.0090244849927873516,\n",
       "  0.0089789610606479768,\n",
       "  0.0089280426005537636,\n",
       "  0.0088813392885475212,\n",
       "  0.0088344158389942577,\n",
       "  0.0087874186599815433,\n",
       "  0.0087418517471654824,\n",
       "  0.008696451496227017,\n",
       "  0.0086505982998769815,\n",
       "  0.0086055264806754669,\n",
       "  0.0085611448992689299,\n",
       "  0.0085176896755638984,\n",
       "  0.0084750991686919509,\n",
       "  0.0084327075074835649,\n",
       "  0.0083904712615238408,\n",
       "  0.0083456910510381031,\n",
       "  0.0083046474219592679,\n",
       "  0.0082640291140800275,\n",
       "  0.008221622646842416,\n",
       "  0.0081812996752092457,\n",
       "  0.0081422630488584904,\n",
       "  0.0080999287782268208,\n",
       "  0.0080599037698653973,\n",
       "  0.0080205612293240535,\n",
       "  0.0079826434677726463,\n",
       "  0.0079429964709815757,\n",
       "  0.0079046555808934549,\n",
       "  0.0078663088190478476,\n",
       "  0.0078290308652183751,\n",
       "  0.0077915953951263276,\n",
       "  0.0077541666386070256,\n",
       "  0.0077172654491075915,\n",
       "  0.0076813898082182504,\n",
       "  0.0076455067306262356,\n",
       "  0.007609261741163228,\n",
       "  0.0075762258378220737,\n",
       "  0.007539297937738278,\n",
       "  0.0075054730609241029,\n",
       "  0.0074710618325655506,\n",
       "  0.00743735954045924,\n",
       "  0.0074037924073030543,\n",
       "  0.0073703744290301715,\n",
       "  0.0073373899740547151,\n",
       "  0.0073051426181290468,\n",
       "  0.0072730008514566118,\n",
       "  0.0072414053903205886,\n",
       "  0.0072090592211999267,\n",
       "  0.0071781809595256743,\n",
       "  0.0071464270502193603,\n",
       "  0.0071157519969933516,\n",
       "  0.007085939264929981,\n",
       "  0.0070547412352164459,\n",
       "  0.0070246595024089668,\n",
       "  0.0069956085523428801,\n",
       "  0.0069662392192546326,\n",
       "  0.0069373030356158129,\n",
       "  0.0069078998586838015,\n",
       "  0.0068789689729114893,\n",
       "  0.0068511347284072738,\n",
       "  0.0068245240074669561,\n",
       "  0.0067952758219324439,\n",
       "  0.0067673267688734872,\n",
       "  0.0067401010572824922,\n",
       "  0.0067128594053930358,\n",
       "  0.0066866069316011438,\n",
       "  0.0066585888826740649,\n",
       "  0.0066323591471676964,\n",
       "  0.0066060001129204663,\n",
       "  0.0065797297197642129,\n",
       "  0.0065537164538818941,\n",
       "  0.0065281954065247831,\n",
       "  0.0065031213998792753,\n",
       "  0.0064773851250467512,\n",
       "  0.0064526937328277089,\n",
       "  0.0064272709246147343,\n",
       "  0.0064024622501045432,\n",
       "  0.0063785386702854066,\n",
       "  0.0063538134294784101,\n",
       "  0.0063295086138492815,\n",
       "  0.0063062659577212583,\n",
       "  0.0062828758959984224,\n",
       "  0.0062596660719732508,\n",
       "  0.0062351599599945505,\n",
       "  0.0062117602191886024,\n",
       "  0.0061887600979016908,\n",
       "  0.0061657446623151748,\n",
       "  0.0061431099639703349,\n",
       "  0.0061208387916707185,\n",
       "  0.0060984701861076154,\n",
       "  0.0060760730591866872,\n",
       "  0.0060538477104989099,\n",
       "  0.0060323846392401244,\n",
       "  0.0060103087010560351,\n",
       "  0.0059891956598619586,\n",
       "  0.0059677329038785691,\n",
       "  0.0059459973508779801,\n",
       "  0.0059253347375250323,\n",
       "  0.0059042967762783167,\n",
       "  0.0058833117813286611,\n",
       "  0.0058623887686227649,\n",
       "  0.0058418838936771323,\n",
       "  0.0058218161332870347,\n",
       "  0.0058017275889091996,\n",
       "  0.0057814652539928436,\n",
       "  0.0057618795655702618,\n",
       "  0.0057422201140400241,\n",
       "  0.0057220570449854321,\n",
       "  0.0057024927679695742,\n",
       "  0.0056828379143315337,\n",
       "  0.005663757358860105,\n",
       "  0.0056451304187116261,\n",
       "  0.0056253762869217516,\n",
       "  0.0056063298213567874,\n",
       "  0.0055875469648899766,\n",
       "  0.0055691478047687855,\n",
       "  0.0055504367251093794,\n",
       "  0.0055321233027778244,\n",
       "  0.0055138677571329149,\n",
       "  0.0054957612012706495,\n",
       "  0.0054776296745452525,\n",
       "  0.005459663342369199,\n",
       "  0.0054418200153458669,\n",
       "  0.0054239654824652112,\n",
       "  0.0054065769490418024,\n",
       "  0.0053891692863244495,\n",
       "  0.0053718724249425583,\n",
       "  0.0053542650739814733,\n",
       "  0.0053371342989135349,\n",
       "  0.0053198777678295809,\n",
       "  0.0053028687674667617,\n",
       "  0.0052861119715106658,\n",
       "  0.0052695662130890835,\n",
       "  0.0052529270849882839,\n",
       "  0.0052361629980196469,\n",
       "  0.0052195362986897998,\n",
       "  0.0052032134981419998,\n",
       "  0.0051869896565198831,\n",
       "  0.0051706006564310077,\n",
       "  0.0051546361026511573,\n",
       "  0.005138503331682069,\n",
       "  0.005122636146680291,\n",
       "  0.005107179100350274,\n",
       "  0.0050911252486139629,\n",
       "  0.0050752932278846025,\n",
       "  0.0050599679364492492,\n",
       "  0.0050441906462594865,\n",
       "  0.0050287947747354178,\n",
       "  0.0050134909801262473,\n",
       "  0.0049983250945747565,\n",
       "  0.0049830333803003368,\n",
       "  0.0049679865232092465,\n",
       "  0.0049528252583258166,\n",
       "  0.0049379611507670024,\n",
       "  0.0049232286140285745,\n",
       "  0.0049083835876180164,\n",
       "  0.0048936675973559581,\n",
       "  0.0048788150975697525,\n",
       "  0.0048644680316490959,\n",
       "  0.0048497833016888786,\n",
       "  0.0048357069176622062,\n",
       "  0.0048210779872078944,\n",
       "  0.0048067210909959659,\n",
       "  0.0047925347074396288,\n",
       "  0.0047783749738258057,\n",
       "  0.004764287221635387,\n",
       "  0.0047504896110469822,\n",
       "  0.0047364518184589864,\n",
       "  0.0047226600764870008,\n",
       "  0.0047093260420158663,\n",
       "  0.0046952669383286839,\n",
       "  0.0046816082240440328,\n",
       "  0.0046679778170092574,\n",
       "  0.0046545654853798772,\n",
       "  0.0046411143760868231,\n",
       "  0.0046278714181218535,\n",
       "  0.0046146472302592487,\n",
       "  0.0046014648047814841,\n",
       "  0.0045885256347059193,\n",
       "  0.0045753720518665374,\n",
       "  0.0045624663920831501,\n",
       "  0.0045496527063504291],\n",
       " [])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './code')\n",
    "import mnist_loader, network2\n",
    "\n",
    "# train the model with only the first 1,000 training images\n",
    "\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "net = network2.Network([784, 30, 10], cost = network2.CrossEntropyCost)\n",
    "net.large_weight_initializer()\n",
    "net.SGD(\n",
    "    training_data[:1000], \n",
    "    400, \n",
    "    10, \n",
    "    0.5, \n",
    "    evaluation_data = test_data,\n",
    "    monitor_evaluation_accuracy = True,\n",
    "    monitor_training_cost = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Regularization is a technique used to reduce overfitting when increasing the `training_data` isn't an option and reducing the size of the network isn't an option either (which is another option to reduce overfitting).\n",
    "One regularization technique is called *weight decay* or *L2 regularization*.\n",
    "\n",
    "### *Weight Decay* (*L2 Regularization*)\n",
    "\n",
    "This technique involves adding an extra term to the cost function, which is called the *regularization term*.\n",
    "The regularized cross-entropy function is: $ C = -\\frac{1}{n} \\sum_{xj} \\left[y_j \\ln a^L_j + (1 - y_j) \\ln(1 - a^L_j) \\right] + \\frac{\\lambda}{2n} \\sum_w w^2 $.\n",
    "The *regularization term* is $ \\frac{\\lambda}{2n} \\sum_w w^2 $, where $ \\lambda \\gt 0 $ is the *regularization parameter* and $ n $ is the size of the training set.\n",
    "To generalize how to *regularize* a cost function, let $ C = C_0 + \\frac{\\lambda}{2n} \\sum_w w^2 $ where $ C $ is the *regularized* cost function, and $ C_0 $ is the original *unregularized* cost function.\n",
    "\n",
    "One can look at regularization as finding a compromise between finding small weights and minimizing the original cost function; the importance of these two factors depends on the value of $ \\lambda $.\n",
    "When $ \\lambda $ is small, minimizing the original cost function is preferred; and when $ \\lambda $ is large the small weights are preferred.\n",
    "\n",
    "#### *Regularized* Gradient Descent\n",
    "\n",
    "When using backpropagation, the only difference with a *regularized* cost function and an *unregularized* cost function in the partial derivatives is that the *regularlized* partial derivatives are: $ \\frac{\\partial C}{\\partial w} = \\frac{\\partial C_0}{\\partial w} + \\frac{\\lambda}{n} w $ and $ \\frac{\\partial C}{\\partial b} = \\frac{C_0}{\\partial b} $.\n",
    "The learning rule for the biases is the same for *regularized* and *unregularized* cost function, but it changes for the weights: $ w \\rightarrow w - \\eta \\frac{\\partial C_0}{\\partial w} - \\frac{\\eta \\lambda}{n} w = \\left(1 - \\frac{\\eta \\lambda}{n} \\right) w - \\eta \\frac{\\partial C_0}{\\partial w} $.\n",
    "The only difference is that the weight $ w $ is rescaled by a factor $ \\left( 1 - \\frac{\\eta \\lambda}{n} \\right) $.\n",
    "The name *weight decay* comes from this property, because the factor makes the weight smaller.\n",
    "\n",
    "#### *Regularized* Stochastic Gradient Descent\n",
    "\n",
    "The equation for the *regularized* stochastic gradient descent is: $ w \\rightarrow \\left(1 - \\frac{\\eta \\lambda}{n} \\right) w - \\frac{\\eta}{m} \\sum_x \\frac{\\partial C_x}{\\partial w} $, where the sum is over the training examples $ x $ in the mini-batch and $ C_x $ is the *unregularized* cost for each training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Cost on training data: 3.1093118559161024\n",
      "Accuracy on training data: 629 / 1000\n",
      "Cost on evaluation data: 2.4374085774393746\n",
      "Accuracy on evaluation data: 5290 / 10000\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 2.4756915124491066\n",
      "Accuracy on training data: 786 / 1000\n",
      "Cost on evaluation data: 1.8765149887329786\n",
      "Accuracy on evaluation data: 6690 / 10000\n",
      "\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 2.263067702333121\n",
      "Accuracy on training data: 833 / 1000\n",
      "Cost on evaluation data: 1.7794002969003386\n",
      "Accuracy on evaluation data: 6905 / 10000\n",
      "\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 2.0616368508614893\n",
      "Accuracy on training data: 872 / 1000\n",
      "Cost on evaluation data: 1.6342959983529344\n",
      "Accuracy on evaluation data: 7336 / 10000\n",
      "\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 1.8992574421272295\n",
      "Accuracy on training data: 901 / 1000\n",
      "Cost on evaluation data: 1.5198826263664664\n",
      "Accuracy on evaluation data: 7597 / 10000\n",
      "\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 1.7935570116374095\n",
      "Accuracy on training data: 925 / 1000\n",
      "Cost on evaluation data: 1.4600010151048566\n",
      "Accuracy on evaluation data: 7702 / 10000\n",
      "\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 1.7002312057894562\n",
      "Accuracy on training data: 934 / 1000\n",
      "Cost on evaluation data: 1.4231405794377416\n",
      "Accuracy on evaluation data: 7785 / 10000\n",
      "\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 1.6147208153100236\n",
      "Accuracy on training data: 947 / 1000\n",
      "Cost on evaluation data: 1.3957942251437045\n",
      "Accuracy on evaluation data: 7857 / 10000\n",
      "\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 1.5601501127159791\n",
      "Accuracy on training data: 953 / 1000\n",
      "Cost on evaluation data: 1.395070356089705\n",
      "Accuracy on evaluation data: 7835 / 10000\n",
      "\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 1.503488823839746\n",
      "Accuracy on training data: 962 / 1000\n",
      "Cost on evaluation data: 1.3536018442452293\n",
      "Accuracy on evaluation data: 7949 / 10000\n",
      "\n",
      "Epoch 10 training complete\n",
      "Cost on training data: 1.4672335137471868\n",
      "Accuracy on training data: 966 / 1000\n",
      "Cost on evaluation data: 1.353699453315265\n",
      "Accuracy on evaluation data: 7975 / 10000\n",
      "\n",
      "Epoch 11 training complete\n",
      "Cost on training data: 1.424497773060733\n",
      "Accuracy on training data: 972 / 1000\n",
      "Cost on evaluation data: 1.3602396250591193\n",
      "Accuracy on evaluation data: 7993 / 10000\n",
      "\n",
      "Epoch 12 training complete\n",
      "Cost on training data: 1.3910846959311722\n",
      "Accuracy on training data: 974 / 1000\n",
      "Cost on evaluation data: 1.3504967367686207\n",
      "Accuracy on evaluation data: 7989 / 10000\n",
      "\n",
      "Epoch 13 training complete\n",
      "Cost on training data: 1.3582977844821154\n",
      "Accuracy on training data: 980 / 1000\n",
      "Cost on evaluation data: 1.341945692237333\n",
      "Accuracy on evaluation data: 8043 / 10000\n",
      "\n",
      "Epoch 14 training complete\n",
      "Cost on training data: 1.3149284026486128\n",
      "Accuracy on training data: 980 / 1000\n",
      "Cost on evaluation data: 1.310829694956184\n",
      "Accuracy on evaluation data: 8100 / 10000\n",
      "\n",
      "Epoch 15 training complete\n",
      "Cost on training data: 1.294650029649588\n",
      "Accuracy on training data: 978 / 1000\n",
      "Cost on evaluation data: 1.3211836742294636\n",
      "Accuracy on evaluation data: 8115 / 10000\n",
      "\n",
      "Epoch 16 training complete\n",
      "Cost on training data: 1.2668119044888697\n",
      "Accuracy on training data: 986 / 1000\n",
      "Cost on evaluation data: 1.3327887177519562\n",
      "Accuracy on evaluation data: 8097 / 10000\n",
      "\n",
      "Epoch 17 training complete\n",
      "Cost on training data: 1.243551974515794\n",
      "Accuracy on training data: 986 / 1000\n",
      "Cost on evaluation data: 1.302170611677719\n",
      "Accuracy on evaluation data: 8150 / 10000\n",
      "\n",
      "Epoch 18 training complete\n",
      "Cost on training data: 1.2141421646665358\n",
      "Accuracy on training data: 986 / 1000\n",
      "Cost on evaluation data: 1.3072866391718785\n",
      "Accuracy on evaluation data: 8156 / 10000\n",
      "\n",
      "Epoch 19 training complete\n",
      "Cost on training data: 1.1973450744054999\n",
      "Accuracy on training data: 986 / 1000\n",
      "Cost on evaluation data: 1.2999129180713995\n",
      "Accuracy on evaluation data: 8190 / 10000\n",
      "\n",
      "Epoch 20 training complete\n",
      "Cost on training data: 1.176034570897145\n",
      "Accuracy on training data: 986 / 1000\n",
      "Cost on evaluation data: 1.295385260379767\n",
      "Accuracy on evaluation data: 8184 / 10000\n",
      "\n",
      "Epoch 21 training complete\n",
      "Cost on training data: 1.1568177801623862\n",
      "Accuracy on training data: 989 / 1000\n",
      "Cost on evaluation data: 1.2964146300969608\n",
      "Accuracy on evaluation data: 8202 / 10000\n",
      "\n",
      "Epoch 22 training complete\n",
      "Cost on training data: 1.1469729295257598\n",
      "Accuracy on training data: 989 / 1000\n",
      "Cost on evaluation data: 1.3251401573192871\n",
      "Accuracy on evaluation data: 8138 / 10000\n",
      "\n",
      "Epoch 23 training complete\n",
      "Cost on training data: 1.1271407397545303\n",
      "Accuracy on training data: 990 / 1000\n",
      "Cost on evaluation data: 1.2972418492112934\n",
      "Accuracy on evaluation data: 8204 / 10000\n",
      "\n",
      "Epoch 24 training complete\n",
      "Cost on training data: 1.1084155982871633\n",
      "Accuracy on training data: 991 / 1000\n",
      "Cost on evaluation data: 1.305965444272756\n",
      "Accuracy on evaluation data: 8188 / 10000\n",
      "\n",
      "Epoch 25 training complete\n",
      "Cost on training data: 1.0934603812593506\n",
      "Accuracy on training data: 991 / 1000\n",
      "Cost on evaluation data: 1.3062330982641408\n",
      "Accuracy on evaluation data: 8203 / 10000\n",
      "\n",
      "Epoch 26 training complete\n",
      "Cost on training data: 1.0877525125148257\n",
      "Accuracy on training data: 991 / 1000\n",
      "Cost on evaluation data: 1.326618422774592\n",
      "Accuracy on evaluation data: 8165 / 10000\n",
      "\n",
      "Epoch 27 training complete\n",
      "Cost on training data: 1.0648572248712316\n",
      "Accuracy on training data: 993 / 1000\n",
      "Cost on evaluation data: 1.3119714562962712\n",
      "Accuracy on evaluation data: 8216 / 10000\n",
      "\n",
      "Epoch 28 training complete\n",
      "Cost on training data: 1.0528511914800478\n",
      "Accuracy on training data: 993 / 1000\n",
      "Cost on evaluation data: 1.3114572966685203\n",
      "Accuracy on evaluation data: 8207 / 10000\n",
      "\n",
      "Epoch 29 training complete\n",
      "Cost on training data: 1.0394769546682165\n",
      "Accuracy on training data: 993 / 1000\n",
      "Cost on evaluation data: 1.3108039840214631\n",
      "Accuracy on evaluation data: 8222 / 10000\n",
      "\n",
      "Epoch 30 training complete\n",
      "Cost on training data: 1.0279302729664084\n",
      "Accuracy on training data: 993 / 1000\n",
      "Cost on evaluation data: 1.3110856345223105\n",
      "Accuracy on evaluation data: 8241 / 10000\n",
      "\n",
      "Epoch 31 training complete\n",
      "Cost on training data: 1.0150567962287862\n",
      "Accuracy on training data: 993 / 1000\n",
      "Cost on evaluation data: 1.2986315739941245\n",
      "Accuracy on evaluation data: 8238 / 10000\n",
      "\n",
      "Epoch 32 training complete\n",
      "Cost on training data: 1.0052562302654704\n",
      "Accuracy on training data: 994 / 1000\n",
      "Cost on evaluation data: 1.3015682774135116\n",
      "Accuracy on evaluation data: 8265 / 10000\n",
      "\n",
      "Epoch 33 training complete\n",
      "Cost on training data: 0.9936837139732213\n",
      "Accuracy on training data: 993 / 1000\n",
      "Cost on evaluation data: 1.3118830810975821\n",
      "Accuracy on evaluation data: 8259 / 10000\n",
      "\n",
      "Epoch 34 training complete\n",
      "Cost on training data: 0.9801628929341769\n",
      "Accuracy on training data: 996 / 1000\n",
      "Cost on evaluation data: 1.316708337421517\n",
      "Accuracy on evaluation data: 8252 / 10000\n",
      "\n",
      "Epoch 35 training complete\n",
      "Cost on training data: 0.9711303285531491\n",
      "Accuracy on training data: 995 / 1000\n",
      "Cost on evaluation data: 1.3084092524000437\n",
      "Accuracy on evaluation data: 8215 / 10000\n",
      "\n",
      "Epoch 36 training complete\n",
      "Cost on training data: 0.9577572318843851\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 1.3042997865592694\n",
      "Accuracy on evaluation data: 8259 / 10000\n",
      "\n",
      "Epoch 37 training complete\n",
      "Cost on training data: 0.9488409404957953\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 1.2970537130522122\n",
      "Accuracy on evaluation data: 8272 / 10000\n",
      "\n",
      "Epoch 38 training complete\n",
      "Cost on training data: 0.940342767147605\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 1.3078298636695138\n",
      "Accuracy on evaluation data: 8234 / 10000\n",
      "\n",
      "Epoch 39 training complete\n",
      "Cost on training data: 0.9292157239085134\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 1.2999754871564573\n",
      "Accuracy on evaluation data: 8282 / 10000\n",
      "\n",
      "Epoch 40 training complete\n",
      "Cost on training data: 0.9183504891462297\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 1.2942152145694825\n",
      "Accuracy on evaluation data: 8280 / 10000\n",
      "\n",
      "Epoch 41 training complete\n",
      "Cost on training data: 0.9089488374719721\n",
      "Accuracy on training data: 998 / 1000\n",
      "Cost on evaluation data: 1.309535797380428\n",
      "Accuracy on evaluation data: 8240 / 10000\n",
      "\n",
      "Epoch 42 training complete\n",
      "Cost on training data: 0.9002072660047677\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 1.3005896950798275\n",
      "Accuracy on evaluation data: 8272 / 10000\n",
      "\n",
      "Epoch 43 training complete\n",
      "Cost on training data: 0.8911964619025647\n",
      "Accuracy on training data: 998 / 1000\n",
      "Cost on evaluation data: 1.3107243311992205\n",
      "Accuracy on evaluation data: 8254 / 10000\n",
      "\n",
      "Epoch 44 training complete\n",
      "Cost on training data: 0.8827971330617111\n",
      "Accuracy on training data: 997 / 1000\n",
      "Cost on evaluation data: 1.299577304821208\n",
      "Accuracy on evaluation data: 8277 / 10000\n",
      "\n",
      "Epoch 45 training complete\n",
      "Cost on training data: 0.8722979485445315\n",
      "Accuracy on training data: 998 / 1000\n",
      "Cost on evaluation data: 1.2868819017220414\n",
      "Accuracy on evaluation data: 8290 / 10000\n",
      "\n",
      "Epoch 46 training complete\n",
      "Cost on training data: 0.8640553181701112\n",
      "Accuracy on training data: 998 / 1000\n",
      "Cost on evaluation data: 1.304649726759117\n",
      "Accuracy on evaluation data: 8267 / 10000\n",
      "\n",
      "Epoch 47 training complete\n",
      "Cost on training data: 0.8559750510380599\n",
      "Accuracy on training data: 998 / 1000\n",
      "Cost on evaluation data: 1.2872144775660779\n",
      "Accuracy on evaluation data: 8302 / 10000\n",
      "\n",
      "Epoch 48 training complete\n",
      "Cost on training data: 0.8468704075774498\n",
      "Accuracy on training data: 998 / 1000\n",
      "Cost on evaluation data: 1.3003004175586466\n",
      "Accuracy on evaluation data: 8277 / 10000\n",
      "\n",
      "Epoch 49 training complete\n",
      "Cost on training data: 0.8397481754064721\n",
      "Accuracy on training data: 998 / 1000\n",
      "Cost on evaluation data: 1.2968503255862929\n",
      "Accuracy on evaluation data: 8298 / 10000\n",
      "\n",
      "Epoch 50 training complete\n",
      "Cost on training data: 0.8306797640991321\n",
      "Accuracy on training data: 998 / 1000\n",
      "Cost on evaluation data: 1.2944657397306618\n",
      "Accuracy on evaluation data: 8311 / 10000\n",
      "\n",
      "Epoch 51 training complete\n",
      "Cost on training data: 0.8238107208606211\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2910694356167203\n",
      "Accuracy on evaluation data: 8303 / 10000\n",
      "\n",
      "Epoch 52 training complete\n",
      "Cost on training data: 0.8149984310383958\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2864195674736612\n",
      "Accuracy on evaluation data: 8312 / 10000\n",
      "\n",
      "Epoch 53 training complete\n",
      "Cost on training data: 0.8071699688448256\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2814170014964308\n",
      "Accuracy on evaluation data: 8321 / 10000\n",
      "\n",
      "Epoch 54 training complete\n",
      "Cost on training data: 0.8004446880726358\n",
      "Accuracy on training data: 998 / 1000\n",
      "Cost on evaluation data: 1.3012104564951528\n",
      "Accuracy on evaluation data: 8290 / 10000\n",
      "\n",
      "Epoch 55 training complete\n",
      "Cost on training data: 0.7922361596243022\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2878439994036242\n",
      "Accuracy on evaluation data: 8313 / 10000\n",
      "\n",
      "Epoch 56 training complete\n",
      "Cost on training data: 0.7848783062760628\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2775770974176182\n",
      "Accuracy on evaluation data: 8323 / 10000\n",
      "\n",
      "Epoch 57 training complete\n",
      "Cost on training data: 0.7779387830402796\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2815348878884414\n",
      "Accuracy on evaluation data: 8334 / 10000\n",
      "\n",
      "Epoch 58 training complete\n",
      "Cost on training data: 0.770889401004173\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2826521482382953\n",
      "Accuracy on evaluation data: 8321 / 10000\n",
      "\n",
      "Epoch 59 training complete\n",
      "Cost on training data: 0.7641211097464988\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2721596621884628\n",
      "Accuracy on evaluation data: 8320 / 10000\n",
      "\n",
      "Epoch 60 training complete\n",
      "Cost on training data: 0.7565538552714203\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2827993274832854\n",
      "Accuracy on evaluation data: 8326 / 10000\n",
      "\n",
      "Epoch 61 training complete\n",
      "Cost on training data: 0.7502505935892104\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2703567152750601\n",
      "Accuracy on evaluation data: 8328 / 10000\n",
      "\n",
      "Epoch 62 training complete\n",
      "Cost on training data: 0.7431063376450946\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2710825036780977\n",
      "Accuracy on evaluation data: 8325 / 10000\n",
      "\n",
      "Epoch 63 training complete\n",
      "Cost on training data: 0.7364022283121484\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2759548070733344\n",
      "Accuracy on evaluation data: 8339 / 10000\n",
      "\n",
      "Epoch 64 training complete\n",
      "Cost on training data: 0.7297787250595161\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2698593010859933\n",
      "Accuracy on evaluation data: 8335 / 10000\n",
      "\n",
      "Epoch 65 training complete\n",
      "Cost on training data: 0.7234030490699941\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.260746818594979\n",
      "Accuracy on evaluation data: 8336 / 10000\n",
      "\n",
      "Epoch 66 training complete\n",
      "Cost on training data: 0.7168668359877332\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2625884335301645\n",
      "Accuracy on evaluation data: 8346 / 10000\n",
      "\n",
      "Epoch 67 training complete\n",
      "Cost on training data: 0.71046303565085\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2656994611831034\n",
      "Accuracy on evaluation data: 8338 / 10000\n",
      "\n",
      "Epoch 68 training complete\n",
      "Cost on training data: 0.7042855872966022\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.25872869501002\n",
      "Accuracy on evaluation data: 8360 / 10000\n",
      "\n",
      "Epoch 69 training complete\n",
      "Cost on training data: 0.6980194174793825\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2580989368157394\n",
      "Accuracy on evaluation data: 8356 / 10000\n",
      "\n",
      "Epoch 70 training complete\n",
      "Cost on training data: 0.691985590615226\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2540974096270463\n",
      "Accuracy on evaluation data: 8361 / 10000\n",
      "\n",
      "Epoch 71 training complete\n",
      "Cost on training data: 0.6857316350683207\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2630306716823183\n",
      "Accuracy on evaluation data: 8349 / 10000\n",
      "\n",
      "Epoch 72 training complete\n",
      "Cost on training data: 0.6798084883332005\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2485941458289047\n",
      "Accuracy on evaluation data: 8361 / 10000\n",
      "\n",
      "Epoch 73 training complete\n",
      "Cost on training data: 0.6738442281856496\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2513498416570286\n",
      "Accuracy on evaluation data: 8357 / 10000\n",
      "\n",
      "Epoch 74 training complete\n",
      "Cost on training data: 0.6677109266837405\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2538663836172848\n",
      "Accuracy on evaluation data: 8354 / 10000\n",
      "\n",
      "Epoch 75 training complete\n",
      "Cost on training data: 0.6624735194326941\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.248713837687759\n",
      "Accuracy on evaluation data: 8349 / 10000\n",
      "\n",
      "Epoch 76 training complete\n",
      "Cost on training data: 0.6563013532016432\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.246917448575613\n",
      "Accuracy on evaluation data: 8368 / 10000\n",
      "\n",
      "Epoch 77 training complete\n",
      "Cost on training data: 0.6505554050069164\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2502502407247416\n",
      "Accuracy on evaluation data: 8356 / 10000\n",
      "\n",
      "Epoch 78 training complete\n",
      "Cost on training data: 0.6449963599052199\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2453935713873536\n",
      "Accuracy on evaluation data: 8356 / 10000\n",
      "\n",
      "Epoch 79 training complete\n",
      "Cost on training data: 0.6400604386352039\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2283951306693623\n",
      "Accuracy on evaluation data: 8381 / 10000\n",
      "\n",
      "Epoch 80 training complete\n",
      "Cost on training data: 0.6340334023045651\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2368389018977715\n",
      "Accuracy on evaluation data: 8370 / 10000\n",
      "\n",
      "Epoch 81 training complete\n",
      "Cost on training data: 0.628435939456693\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2370481942253582\n",
      "Accuracy on evaluation data: 8378 / 10000\n",
      "\n",
      "Epoch 82 training complete\n",
      "Cost on training data: 0.6234456304569618\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2441937973753814\n",
      "Accuracy on evaluation data: 8349 / 10000\n",
      "\n",
      "Epoch 83 training complete\n",
      "Cost on training data: 0.6178096913707546\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2343283813421548\n",
      "Accuracy on evaluation data: 8365 / 10000\n",
      "\n",
      "Epoch 84 training complete\n",
      "Cost on training data: 0.6128747864851791\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.2274376713717698\n",
      "Accuracy on evaluation data: 8374 / 10000\n",
      "\n",
      "Epoch 85 training complete\n",
      "Cost on training data: 0.6074933433143326\n",
      "Accuracy on training data: 999 / 1000\n",
      "Cost on evaluation data: 1.22880885893289\n",
      "Accuracy on evaluation data: 8371 / 10000\n",
      "\n",
      "Epoch 86 training complete\n",
      "Cost on training data: 0.6027450239913035\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2242288948297\n",
      "Accuracy on evaluation data: 8384 / 10000\n",
      "\n",
      "Epoch 87 training complete\n",
      "Cost on training data: 0.5971435936703152\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2251209413762745\n",
      "Accuracy on evaluation data: 8372 / 10000\n",
      "\n",
      "Epoch 88 training complete\n",
      "Cost on training data: 0.5920568390423314\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2237079857619326\n",
      "Accuracy on evaluation data: 8385 / 10000\n",
      "\n",
      "Epoch 89 training complete\n",
      "Cost on training data: 0.5870901258109696\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2177628724279395\n",
      "Accuracy on evaluation data: 8388 / 10000\n",
      "\n",
      "Epoch 90 training complete\n",
      "Cost on training data: 0.5824598216868137\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2108452503011384\n",
      "Accuracy on evaluation data: 8377 / 10000\n",
      "\n",
      "Epoch 91 training complete\n",
      "Cost on training data: 0.5771636871037518\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2163195289992266\n",
      "Accuracy on evaluation data: 8383 / 10000\n",
      "\n",
      "Epoch 92 training complete\n",
      "Cost on training data: 0.5725814047119644\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2125076658021314\n",
      "Accuracy on evaluation data: 8385 / 10000\n",
      "\n",
      "Epoch 93 training complete\n",
      "Cost on training data: 0.5675648107545558\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2082899243136045\n",
      "Accuracy on evaluation data: 8387 / 10000\n",
      "\n",
      "Epoch 94 training complete\n",
      "Cost on training data: 0.5635240772532871\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2015376477919952\n",
      "Accuracy on evaluation data: 8397 / 10000\n",
      "\n",
      "Epoch 95 training complete\n",
      "Cost on training data: 0.5582049884193097\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2032218255050273\n",
      "Accuracy on evaluation data: 8391 / 10000\n",
      "\n",
      "Epoch 96 training complete\n",
      "Cost on training data: 0.5539462609146658\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.2135456598891252\n",
      "Accuracy on evaluation data: 8378 / 10000\n",
      "\n",
      "Epoch 97 training complete\n",
      "Cost on training data: 0.5493060398973859\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.209148117284928\n",
      "Accuracy on evaluation data: 8375 / 10000\n",
      "\n",
      "Epoch 98 training complete\n",
      "Cost on training data: 0.544658222662592\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1992759401198783\n",
      "Accuracy on evaluation data: 8398 / 10000\n",
      "\n",
      "Epoch 99 training complete\n",
      "Cost on training data: 0.5401824376211344\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.200557893958948\n",
      "Accuracy on evaluation data: 8385 / 10000\n",
      "\n",
      "Epoch 100 training complete\n",
      "Cost on training data: 0.5357386888454957\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1900695666616092\n",
      "Accuracy on evaluation data: 8412 / 10000\n",
      "\n",
      "Epoch 101 training complete\n",
      "Cost on training data: 0.5313762905926216\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.193581503031673\n",
      "Accuracy on evaluation data: 8388 / 10000\n",
      "\n",
      "Epoch 102 training complete\n",
      "Cost on training data: 0.5268996175490356\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1875378657854423\n",
      "Accuracy on evaluation data: 8392 / 10000\n",
      "\n",
      "Epoch 103 training complete\n",
      "Cost on training data: 0.5226797762724231\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1887233665708863\n",
      "Accuracy on evaluation data: 8391 / 10000\n",
      "\n",
      "Epoch 104 training complete\n",
      "Cost on training data: 0.5182947032773533\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1847759279965535\n",
      "Accuracy on evaluation data: 8403 / 10000\n",
      "\n",
      "Epoch 105 training complete\n",
      "Cost on training data: 0.5142453199555005\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1762440994793033\n",
      "Accuracy on evaluation data: 8404 / 10000\n",
      "\n",
      "Epoch 106 training complete\n",
      "Cost on training data: 0.5099925044024498\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1798712893107504\n",
      "Accuracy on evaluation data: 8388 / 10000\n",
      "\n",
      "Epoch 107 training complete\n",
      "Cost on training data: 0.5057556382643773\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.172003849939622\n",
      "Accuracy on evaluation data: 8403 / 10000\n",
      "\n",
      "Epoch 108 training complete\n",
      "Cost on training data: 0.5018861815805996\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.172039532179961\n",
      "Accuracy on evaluation data: 8410 / 10000\n",
      "\n",
      "Epoch 109 training complete\n",
      "Cost on training data: 0.497670599795254\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1752658867567105\n",
      "Accuracy on evaluation data: 8407 / 10000\n",
      "\n",
      "Epoch 110 training complete\n",
      "Cost on training data: 0.4939530417901579\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1653376653277423\n",
      "Accuracy on evaluation data: 8434 / 10000\n",
      "\n",
      "Epoch 111 training complete\n",
      "Cost on training data: 0.48965804049506617\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1638420499254356\n",
      "Accuracy on evaluation data: 8411 / 10000\n",
      "\n",
      "Epoch 112 training complete\n",
      "Cost on training data: 0.4858016885829848\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1635770874294584\n",
      "Accuracy on evaluation data: 8414 / 10000\n",
      "\n",
      "Epoch 113 training complete\n",
      "Cost on training data: 0.48182235136386203\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1616044339438707\n",
      "Accuracy on evaluation data: 8414 / 10000\n",
      "\n",
      "Epoch 114 training complete\n",
      "Cost on training data: 0.47804493560185995\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1583297398533103\n",
      "Accuracy on evaluation data: 8427 / 10000\n",
      "\n",
      "Epoch 115 training complete\n",
      "Cost on training data: 0.4742161291625162\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1624040454788274\n",
      "Accuracy on evaluation data: 8421 / 10000\n",
      "\n",
      "Epoch 116 training complete\n",
      "Cost on training data: 0.4703828920004245\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.158328559090246\n",
      "Accuracy on evaluation data: 8418 / 10000\n",
      "\n",
      "Epoch 117 training complete\n",
      "Cost on training data: 0.466663684881204\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1507817915846827\n",
      "Accuracy on evaluation data: 8431 / 10000\n",
      "\n",
      "Epoch 118 training complete\n",
      "Cost on training data: 0.4632242163468707\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1422410011240396\n",
      "Accuracy on evaluation data: 8447 / 10000\n",
      "\n",
      "Epoch 119 training complete\n",
      "Cost on training data: 0.4592379606092416\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1492561408852227\n",
      "Accuracy on evaluation data: 8423 / 10000\n",
      "\n",
      "Epoch 120 training complete\n",
      "Cost on training data: 0.45567472101049933\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1427966053261234\n",
      "Accuracy on evaluation data: 8443 / 10000\n",
      "\n",
      "Epoch 121 training complete\n",
      "Cost on training data: 0.45202032849796503\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1455973738772693\n",
      "Accuracy on evaluation data: 8437 / 10000\n",
      "\n",
      "Epoch 122 training complete\n",
      "Cost on training data: 0.44845709711428866\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.146215414089935\n",
      "Accuracy on evaluation data: 8430 / 10000\n",
      "\n",
      "Epoch 123 training complete\n",
      "Cost on training data: 0.4449126866594918\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1429536081353069\n",
      "Accuracy on evaluation data: 8442 / 10000\n",
      "\n",
      "Epoch 124 training complete\n",
      "Cost on training data: 0.4415166596595466\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.133528749635365\n",
      "Accuracy on evaluation data: 8452 / 10000\n",
      "\n",
      "Epoch 125 training complete\n",
      "Cost on training data: 0.4381074579697815\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1353366247792434\n",
      "Accuracy on evaluation data: 8460 / 10000\n",
      "\n",
      "Epoch 126 training complete\n",
      "Cost on training data: 0.4346794532050623\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1318254847543285\n",
      "Accuracy on evaluation data: 8455 / 10000\n",
      "\n",
      "Epoch 127 training complete\n",
      "Cost on training data: 0.431280920783854\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1275368675214057\n",
      "Accuracy on evaluation data: 8468 / 10000\n",
      "\n",
      "Epoch 128 training complete\n",
      "Cost on training data: 0.42799160811175535\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.126207263036298\n",
      "Accuracy on evaluation data: 8461 / 10000\n",
      "\n",
      "Epoch 129 training complete\n",
      "Cost on training data: 0.42458930619165003\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1271290902048152\n",
      "Accuracy on evaluation data: 8461 / 10000\n",
      "\n",
      "Epoch 130 training complete\n",
      "Cost on training data: 0.42144243695179506\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.129956173095776\n",
      "Accuracy on evaluation data: 8455 / 10000\n",
      "\n",
      "Epoch 131 training complete\n",
      "Cost on training data: 0.4180586024679339\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1214895714211859\n",
      "Accuracy on evaluation data: 8473 / 10000\n",
      "\n",
      "Epoch 132 training complete\n",
      "Cost on training data: 0.4150791880242284\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1177337154311524\n",
      "Accuracy on evaluation data: 8476 / 10000\n",
      "\n",
      "Epoch 133 training complete\n",
      "Cost on training data: 0.4116845422050177\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1189626196429452\n",
      "Accuracy on evaluation data: 8481 / 10000\n",
      "\n",
      "Epoch 134 training complete\n",
      "Cost on training data: 0.40853607874817244\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1213104766557165\n",
      "Accuracy on evaluation data: 8473 / 10000\n",
      "\n",
      "Epoch 135 training complete\n",
      "Cost on training data: 0.4054551899328198\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1205309075310388\n",
      "Accuracy on evaluation data: 8480 / 10000\n",
      "\n",
      "Epoch 136 training complete\n",
      "Cost on training data: 0.4023538013537401\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1122199490745934\n",
      "Accuracy on evaluation data: 8487 / 10000\n",
      "\n",
      "Epoch 137 training complete\n",
      "Cost on training data: 0.39929626909963845\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1116839211554552\n",
      "Accuracy on evaluation data: 8474 / 10000\n",
      "\n",
      "Epoch 138 training complete\n",
      "Cost on training data: 0.396285103873218\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1104476253914648\n",
      "Accuracy on evaluation data: 8494 / 10000\n",
      "\n",
      "Epoch 139 training complete\n",
      "Cost on training data: 0.3933131063788569\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1072931488727578\n",
      "Accuracy on evaluation data: 8497 / 10000\n",
      "\n",
      "Epoch 140 training complete\n",
      "Cost on training data: 0.3903867855578727\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1076485110654593\n",
      "Accuracy on evaluation data: 8490 / 10000\n",
      "\n",
      "Epoch 141 training complete\n",
      "Cost on training data: 0.3876537399677879\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1046531721591948\n",
      "Accuracy on evaluation data: 8487 / 10000\n",
      "\n",
      "Epoch 142 training complete\n",
      "Cost on training data: 0.3845029937251228\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.1037201362917402\n",
      "Accuracy on evaluation data: 8505 / 10000\n",
      "\n",
      "Epoch 143 training complete\n",
      "Cost on training data: 0.3816492260836915\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0957605281514289\n",
      "Accuracy on evaluation data: 8514 / 10000\n",
      "\n",
      "Epoch 144 training complete\n",
      "Cost on training data: 0.37877275755748785\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0996125278229099\n",
      "Accuracy on evaluation data: 8509 / 10000\n",
      "\n",
      "Epoch 145 training complete\n",
      "Cost on training data: 0.3761056910343285\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.089668950811577\n",
      "Accuracy on evaluation data: 8512 / 10000\n",
      "\n",
      "Epoch 146 training complete\n",
      "Cost on training data: 0.3733070949836261\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.089257974352046\n",
      "Accuracy on evaluation data: 8523 / 10000\n",
      "\n",
      "Epoch 147 training complete\n",
      "Cost on training data: 0.37039563441137413\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0912185316414633\n",
      "Accuracy on evaluation data: 8512 / 10000\n",
      "\n",
      "Epoch 148 training complete\n",
      "Cost on training data: 0.36775819021001604\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.088443390883996\n",
      "Accuracy on evaluation data: 8513 / 10000\n",
      "\n",
      "Epoch 149 training complete\n",
      "Cost on training data: 0.36510367762405216\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.096352930573201\n",
      "Accuracy on evaluation data: 8501 / 10000\n",
      "\n",
      "Epoch 150 training complete\n",
      "Cost on training data: 0.36234836679373916\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.084244648558889\n",
      "Accuracy on evaluation data: 8515 / 10000\n",
      "\n",
      "Epoch 151 training complete\n",
      "Cost on training data: 0.35964685245525657\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.086780626914333\n",
      "Accuracy on evaluation data: 8514 / 10000\n",
      "\n",
      "Epoch 152 training complete\n",
      "Cost on training data: 0.3570916710161808\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0798677486969905\n",
      "Accuracy on evaluation data: 8528 / 10000\n",
      "\n",
      "Epoch 153 training complete\n",
      "Cost on training data: 0.3544895347075775\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.077846288477015\n",
      "Accuracy on evaluation data: 8529 / 10000\n",
      "\n",
      "Epoch 154 training complete\n",
      "Cost on training data: 0.3519343620708596\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.075456107270529\n",
      "Accuracy on evaluation data: 8531 / 10000\n",
      "\n",
      "Epoch 155 training complete\n",
      "Cost on training data: 0.34956494745647526\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0729816475234053\n",
      "Accuracy on evaluation data: 8528 / 10000\n",
      "\n",
      "Epoch 156 training complete\n",
      "Cost on training data: 0.34697776704884997\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0808413981405371\n",
      "Accuracy on evaluation data: 8527 / 10000\n",
      "\n",
      "Epoch 157 training complete\n",
      "Cost on training data: 0.34445352061301493\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.068925585286793\n",
      "Accuracy on evaluation data: 8536 / 10000\n",
      "\n",
      "Epoch 158 training complete\n",
      "Cost on training data: 0.3419595347242885\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.071126227329423\n",
      "Accuracy on evaluation data: 8531 / 10000\n",
      "\n",
      "Epoch 159 training complete\n",
      "Cost on training data: 0.3394942016675959\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0703314275863374\n",
      "Accuracy on evaluation data: 8536 / 10000\n",
      "\n",
      "Epoch 160 training complete\n",
      "Cost on training data: 0.33696716173026425\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0670068886548334\n",
      "Accuracy on evaluation data: 8536 / 10000\n",
      "\n",
      "Epoch 161 training complete\n",
      "Cost on training data: 0.3347263535845993\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0660598744725658\n",
      "Accuracy on evaluation data: 8537 / 10000\n",
      "\n",
      "Epoch 162 training complete\n",
      "Cost on training data: 0.3322980781914346\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0636492699266793\n",
      "Accuracy on evaluation data: 8551 / 10000\n",
      "\n",
      "Epoch 163 training complete\n",
      "Cost on training data: 0.3299481040620262\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0631796557275905\n",
      "Accuracy on evaluation data: 8535 / 10000\n",
      "\n",
      "Epoch 164 training complete\n",
      "Cost on training data: 0.32769409027294344\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0658051548424778\n",
      "Accuracy on evaluation data: 8541 / 10000\n",
      "\n",
      "Epoch 165 training complete\n",
      "Cost on training data: 0.32528157731144614\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0599773348170636\n",
      "Accuracy on evaluation data: 8538 / 10000\n",
      "\n",
      "Epoch 166 training complete\n",
      "Cost on training data: 0.3229028310198588\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0539174975348549\n",
      "Accuracy on evaluation data: 8550 / 10000\n",
      "\n",
      "Epoch 167 training complete\n",
      "Cost on training data: 0.32079530002255985\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0505767829986419\n",
      "Accuracy on evaluation data: 8553 / 10000\n",
      "\n",
      "Epoch 168 training complete\n",
      "Cost on training data: 0.31849802451787557\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.052837932752481\n",
      "Accuracy on evaluation data: 8551 / 10000\n",
      "\n",
      "Epoch 169 training complete\n",
      "Cost on training data: 0.3163574587419601\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0565324032875745\n",
      "Accuracy on evaluation data: 8539 / 10000\n",
      "\n",
      "Epoch 170 training complete\n",
      "Cost on training data: 0.31401628396466635\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0428388733734617\n",
      "Accuracy on evaluation data: 8553 / 10000\n",
      "\n",
      "Epoch 171 training complete\n",
      "Cost on training data: 0.31190306305763393\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.045853015520592\n",
      "Accuracy on evaluation data: 8550 / 10000\n",
      "\n",
      "Epoch 172 training complete\n",
      "Cost on training data: 0.30973063248703003\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0490424619497547\n",
      "Accuracy on evaluation data: 8555 / 10000\n",
      "\n",
      "Epoch 173 training complete\n",
      "Cost on training data: 0.3076078138061438\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0487899824868536\n",
      "Accuracy on evaluation data: 8548 / 10000\n",
      "\n",
      "Epoch 174 training complete\n",
      "Cost on training data: 0.3054179198599345\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0419689773959602\n",
      "Accuracy on evaluation data: 8566 / 10000\n",
      "\n",
      "Epoch 175 training complete\n",
      "Cost on training data: 0.3035029671417175\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0507687457071917\n",
      "Accuracy on evaluation data: 8542 / 10000\n",
      "\n",
      "Epoch 176 training complete\n",
      "Cost on training data: 0.3013384221564846\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.030363746761755\n",
      "Accuracy on evaluation data: 8565 / 10000\n",
      "\n",
      "Epoch 177 training complete\n",
      "Cost on training data: 0.29922322393893236\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0437791015681457\n",
      "Accuracy on evaluation data: 8558 / 10000\n",
      "\n",
      "Epoch 178 training complete\n",
      "Cost on training data: 0.29713130585628494\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0329366661328032\n",
      "Accuracy on evaluation data: 8572 / 10000\n",
      "\n",
      "Epoch 179 training complete\n",
      "Cost on training data: 0.2951184959147163\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0378950639331619\n",
      "Accuracy on evaluation data: 8572 / 10000\n",
      "\n",
      "Epoch 180 training complete\n",
      "Cost on training data: 0.29314942076254313\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0313381681025233\n",
      "Accuracy on evaluation data: 8575 / 10000\n",
      "\n",
      "Epoch 181 training complete\n",
      "Cost on training data: 0.29127148503076455\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0244076071695947\n",
      "Accuracy on evaluation data: 8574 / 10000\n",
      "\n",
      "Epoch 182 training complete\n",
      "Cost on training data: 0.28924219954008795\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0324621309382078\n",
      "Accuracy on evaluation data: 8563 / 10000\n",
      "\n",
      "Epoch 183 training complete\n",
      "Cost on training data: 0.2873700812083683\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0273034563297871\n",
      "Accuracy on evaluation data: 8570 / 10000\n",
      "\n",
      "Epoch 184 training complete\n",
      "Cost on training data: 0.2853831055901662\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0330526583073822\n",
      "Accuracy on evaluation data: 8559 / 10000\n",
      "\n",
      "Epoch 185 training complete\n",
      "Cost on training data: 0.2835262610855316\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.03245103766701\n",
      "Accuracy on evaluation data: 8569 / 10000\n",
      "\n",
      "Epoch 186 training complete\n",
      "Cost on training data: 0.2816109884475326\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0220958932475859\n",
      "Accuracy on evaluation data: 8580 / 10000\n",
      "\n",
      "Epoch 187 training complete\n",
      "Cost on training data: 0.2797077554399773\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0187007656644178\n",
      "Accuracy on evaluation data: 8584 / 10000\n",
      "\n",
      "Epoch 188 training complete\n",
      "Cost on training data: 0.2778702007474667\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0225232171622511\n",
      "Accuracy on evaluation data: 8577 / 10000\n",
      "\n",
      "Epoch 189 training complete\n",
      "Cost on training data: 0.2759886439256298\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0126046467038665\n",
      "Accuracy on evaluation data: 8589 / 10000\n",
      "\n",
      "Epoch 190 training complete\n",
      "Cost on training data: 0.2743137857502068\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.01752272453477\n",
      "Accuracy on evaluation data: 8592 / 10000\n",
      "\n",
      "Epoch 191 training complete\n",
      "Cost on training data: 0.27246016026263103\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0206486373685166\n",
      "Accuracy on evaluation data: 8580 / 10000\n",
      "\n",
      "Epoch 192 training complete\n",
      "Cost on training data: 0.27083509930213234\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0186751497940392\n",
      "Accuracy on evaluation data: 8597 / 10000\n",
      "\n",
      "Epoch 193 training complete\n",
      "Cost on training data: 0.26876225573981083\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0075140543848307\n",
      "Accuracy on evaluation data: 8590 / 10000\n",
      "\n",
      "Epoch 194 training complete\n",
      "Cost on training data: 0.26723485214278847\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.013811651112653\n",
      "Accuracy on evaluation data: 8586 / 10000\n",
      "\n",
      "Epoch 195 training complete\n",
      "Cost on training data: 0.2653359661432554\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0106676687529643\n",
      "Accuracy on evaluation data: 8598 / 10000\n",
      "\n",
      "Epoch 196 training complete\n",
      "Cost on training data: 0.2635490189716985\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.003071268995026\n",
      "Accuracy on evaluation data: 8602 / 10000\n",
      "\n",
      "Epoch 197 training complete\n",
      "Cost on training data: 0.2618907619749282\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0037714181923918\n",
      "Accuracy on evaluation data: 8610 / 10000\n",
      "\n",
      "Epoch 198 training complete\n",
      "Cost on training data: 0.26036658719744554\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9952855440141859\n",
      "Accuracy on evaluation data: 8623 / 10000\n",
      "\n",
      "Epoch 199 training complete\n",
      "Cost on training data: 0.25863408898485246\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0079693295650205\n",
      "Accuracy on evaluation data: 8588 / 10000\n",
      "\n",
      "Epoch 200 training complete\n",
      "Cost on training data: 0.2568722943523502\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0014553072562185\n",
      "Accuracy on evaluation data: 8601 / 10000\n",
      "\n",
      "Epoch 201 training complete\n",
      "Cost on training data: 0.25529552127111743\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 1.0024996679712623\n",
      "Accuracy on evaluation data: 8597 / 10000\n",
      "\n",
      "Epoch 202 training complete\n",
      "Cost on training data: 0.25373052293092374\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9997796891248643\n",
      "Accuracy on evaluation data: 8595 / 10000\n",
      "\n",
      "Epoch 203 training complete\n",
      "Cost on training data: 0.25214971566007316\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9971565791614444\n",
      "Accuracy on evaluation data: 8603 / 10000\n",
      "\n",
      "Epoch 204 training complete\n",
      "Cost on training data: 0.25046994960719704\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9928884570778371\n",
      "Accuracy on evaluation data: 8610 / 10000\n",
      "\n",
      "Epoch 205 training complete\n",
      "Cost on training data: 0.24894185181847236\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9948842413463759\n",
      "Accuracy on evaluation data: 8608 / 10000\n",
      "\n",
      "Epoch 206 training complete\n",
      "Cost on training data: 0.24735766360524933\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9984244016792526\n",
      "Accuracy on evaluation data: 8598 / 10000\n",
      "\n",
      "Epoch 207 training complete\n",
      "Cost on training data: 0.24593769494623965\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9838925910378364\n",
      "Accuracy on evaluation data: 8637 / 10000\n",
      "\n",
      "Epoch 208 training complete\n",
      "Cost on training data: 0.2444788945976714\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9878599675229212\n",
      "Accuracy on evaluation data: 8614 / 10000\n",
      "\n",
      "Epoch 209 training complete\n",
      "Cost on training data: 0.24285847190964813\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9887852715980066\n",
      "Accuracy on evaluation data: 8627 / 10000\n",
      "\n",
      "Epoch 210 training complete\n",
      "Cost on training data: 0.24143466622360713\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9879160181574167\n",
      "Accuracy on evaluation data: 8628 / 10000\n",
      "\n",
      "Epoch 211 training complete\n",
      "Cost on training data: 0.23992707474278163\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9878923207213005\n",
      "Accuracy on evaluation data: 8629 / 10000\n",
      "\n",
      "Epoch 212 training complete\n",
      "Cost on training data: 0.2384381352959392\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9883798299506429\n",
      "Accuracy on evaluation data: 8606 / 10000\n",
      "\n",
      "Epoch 213 training complete\n",
      "Cost on training data: 0.23710718560228367\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9891752161702981\n",
      "Accuracy on evaluation data: 8613 / 10000\n",
      "\n",
      "Epoch 214 training complete\n",
      "Cost on training data: 0.2354890892634137\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9878080554644533\n",
      "Accuracy on evaluation data: 8626 / 10000\n",
      "\n",
      "Epoch 215 training complete\n",
      "Cost on training data: 0.2345210587867409\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9904956629861633\n",
      "Accuracy on evaluation data: 8608 / 10000\n",
      "\n",
      "Epoch 216 training complete\n",
      "Cost on training data: 0.23297548107652308\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9874351934569836\n",
      "Accuracy on evaluation data: 8611 / 10000\n",
      "\n",
      "Epoch 217 training complete\n",
      "Cost on training data: 0.231342308351856\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9883705029005355\n",
      "Accuracy on evaluation data: 8626 / 10000\n",
      "\n",
      "Epoch 218 training complete\n",
      "Cost on training data: 0.23023188512368867\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9771534262436038\n",
      "Accuracy on evaluation data: 8634 / 10000\n",
      "\n",
      "Epoch 219 training complete\n",
      "Cost on training data: 0.22864078753693637\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9782456811660808\n",
      "Accuracy on evaluation data: 8630 / 10000\n",
      "\n",
      "Epoch 220 training complete\n",
      "Cost on training data: 0.227323508363912\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.973873240909452\n",
      "Accuracy on evaluation data: 8629 / 10000\n",
      "\n",
      "Epoch 221 training complete\n",
      "Cost on training data: 0.22609335592709975\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9675543758264702\n",
      "Accuracy on evaluation data: 8635 / 10000\n",
      "\n",
      "Epoch 222 training complete\n",
      "Cost on training data: 0.22472158562069955\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.975465961918868\n",
      "Accuracy on evaluation data: 8637 / 10000\n",
      "\n",
      "Epoch 223 training complete\n",
      "Cost on training data: 0.22343433366658794\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.974180679067908\n",
      "Accuracy on evaluation data: 8622 / 10000\n",
      "\n",
      "Epoch 224 training complete\n",
      "Cost on training data: 0.22222122111591522\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9660922690928958\n",
      "Accuracy on evaluation data: 8645 / 10000\n",
      "\n",
      "Epoch 225 training complete\n",
      "Cost on training data: 0.2208418195405344\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9687006985730781\n",
      "Accuracy on evaluation data: 8638 / 10000\n",
      "\n",
      "Epoch 226 training complete\n",
      "Cost on training data: 0.219871996181031\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9673444949875205\n",
      "Accuracy on evaluation data: 8646 / 10000\n",
      "\n",
      "Epoch 227 training complete\n",
      "Cost on training data: 0.21830360286938003\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9685631538689877\n",
      "Accuracy on evaluation data: 8646 / 10000\n",
      "\n",
      "Epoch 228 training complete\n",
      "Cost on training data: 0.2172417975010616\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.970574622663936\n",
      "Accuracy on evaluation data: 8632 / 10000\n",
      "\n",
      "Epoch 229 training complete\n",
      "Cost on training data: 0.21592548831805913\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.970751958992527\n",
      "Accuracy on evaluation data: 8629 / 10000\n",
      "\n",
      "Epoch 230 training complete\n",
      "Cost on training data: 0.21469549495491855\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9654111090060359\n",
      "Accuracy on evaluation data: 8633 / 10000\n",
      "\n",
      "Epoch 231 training complete\n",
      "Cost on training data: 0.21341049479856716\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9636988239137209\n",
      "Accuracy on evaluation data: 8639 / 10000\n",
      "\n",
      "Epoch 232 training complete\n",
      "Cost on training data: 0.21239566137110946\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9652050515289053\n",
      "Accuracy on evaluation data: 8633 / 10000\n",
      "\n",
      "Epoch 233 training complete\n",
      "Cost on training data: 0.21105228186386418\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9647066219438741\n",
      "Accuracy on evaluation data: 8631 / 10000\n",
      "\n",
      "Epoch 234 training complete\n",
      "Cost on training data: 0.21000554507882463\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9576506895829274\n",
      "Accuracy on evaluation data: 8636 / 10000\n",
      "\n",
      "Epoch 235 training complete\n",
      "Cost on training data: 0.20903115895300328\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9596612371586327\n",
      "Accuracy on evaluation data: 8645 / 10000\n",
      "\n",
      "Epoch 236 training complete\n",
      "Cost on training data: 0.20782159984049714\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9640241707771896\n",
      "Accuracy on evaluation data: 8643 / 10000\n",
      "\n",
      "Epoch 237 training complete\n",
      "Cost on training data: 0.20645329145616617\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9610522845107163\n",
      "Accuracy on evaluation data: 8642 / 10000\n",
      "\n",
      "Epoch 238 training complete\n",
      "Cost on training data: 0.20545871468448298\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.953053378331193\n",
      "Accuracy on evaluation data: 8656 / 10000\n",
      "\n",
      "Epoch 239 training complete\n",
      "Cost on training data: 0.20434075907792756\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9630995072537839\n",
      "Accuracy on evaluation data: 8620 / 10000\n",
      "\n",
      "Epoch 240 training complete\n",
      "Cost on training data: 0.20327499972730667\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9491575906640001\n",
      "Accuracy on evaluation data: 8650 / 10000\n",
      "\n",
      "Epoch 241 training complete\n",
      "Cost on training data: 0.2022172721610046\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9577763378607261\n",
      "Accuracy on evaluation data: 8638 / 10000\n",
      "\n",
      "Epoch 242 training complete\n",
      "Cost on training data: 0.20101521118230337\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9531778584765277\n",
      "Accuracy on evaluation data: 8653 / 10000\n",
      "\n",
      "Epoch 243 training complete\n",
      "Cost on training data: 0.20012018322884118\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9484288321709573\n",
      "Accuracy on evaluation data: 8653 / 10000\n",
      "\n",
      "Epoch 244 training complete\n",
      "Cost on training data: 0.19910710245743124\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9573397829027174\n",
      "Accuracy on evaluation data: 8639 / 10000\n",
      "\n",
      "Epoch 245 training complete\n",
      "Cost on training data: 0.19791708677746833\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9491304589823414\n",
      "Accuracy on evaluation data: 8648 / 10000\n",
      "\n",
      "Epoch 246 training complete\n",
      "Cost on training data: 0.19696842883934174\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9527988721623171\n",
      "Accuracy on evaluation data: 8652 / 10000\n",
      "\n",
      "Epoch 247 training complete\n",
      "Cost on training data: 0.19599479919873086\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9456222928810862\n",
      "Accuracy on evaluation data: 8672 / 10000\n",
      "\n",
      "Epoch 248 training complete\n",
      "Cost on training data: 0.19507198467316417\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.958721905398631\n",
      "Accuracy on evaluation data: 8652 / 10000\n",
      "\n",
      "Epoch 249 training complete\n",
      "Cost on training data: 0.19397188029868465\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9478472364377204\n",
      "Accuracy on evaluation data: 8660 / 10000\n",
      "\n",
      "Epoch 250 training complete\n",
      "Cost on training data: 0.1928503851232975\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9447716887129817\n",
      "Accuracy on evaluation data: 8665 / 10000\n",
      "\n",
      "Epoch 251 training complete\n",
      "Cost on training data: 0.19190992756426387\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9425031942776583\n",
      "Accuracy on evaluation data: 8665 / 10000\n",
      "\n",
      "Epoch 252 training complete\n",
      "Cost on training data: 0.19110437020615303\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9559330729937402\n",
      "Accuracy on evaluation data: 8648 / 10000\n",
      "\n",
      "Epoch 253 training complete\n",
      "Cost on training data: 0.1899020341361906\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9449504245942326\n",
      "Accuracy on evaluation data: 8658 / 10000\n",
      "\n",
      "Epoch 254 training complete\n",
      "Cost on training data: 0.18910786357232123\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9466332646643663\n",
      "Accuracy on evaluation data: 8666 / 10000\n",
      "\n",
      "Epoch 255 training complete\n",
      "Cost on training data: 0.18811013363243423\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9448403845593449\n",
      "Accuracy on evaluation data: 8655 / 10000\n",
      "\n",
      "Epoch 256 training complete\n",
      "Cost on training data: 0.18706985013936334\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9419747568111457\n",
      "Accuracy on evaluation data: 8656 / 10000\n",
      "\n",
      "Epoch 257 training complete\n",
      "Cost on training data: 0.18620967676686173\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9352832831398324\n",
      "Accuracy on evaluation data: 8671 / 10000\n",
      "\n",
      "Epoch 258 training complete\n",
      "Cost on training data: 0.18524757852322102\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9357216900073444\n",
      "Accuracy on evaluation data: 8676 / 10000\n",
      "\n",
      "Epoch 259 training complete\n",
      "Cost on training data: 0.18455432115959824\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9350880604800964\n",
      "Accuracy on evaluation data: 8658 / 10000\n",
      "\n",
      "Epoch 260 training complete\n",
      "Cost on training data: 0.18350696952290754\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9358684493754084\n",
      "Accuracy on evaluation data: 8670 / 10000\n",
      "\n",
      "Epoch 261 training complete\n",
      "Cost on training data: 0.18277643942157892\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9318811140460928\n",
      "Accuracy on evaluation data: 8675 / 10000\n",
      "\n",
      "Epoch 262 training complete\n",
      "Cost on training data: 0.18163047181941233\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9356878149949203\n",
      "Accuracy on evaluation data: 8661 / 10000\n",
      "\n",
      "Epoch 263 training complete\n",
      "Cost on training data: 0.18098488876558458\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9369956012937724\n",
      "Accuracy on evaluation data: 8663 / 10000\n",
      "\n",
      "Epoch 264 training complete\n",
      "Cost on training data: 0.1801991361775246\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9351511855674\n",
      "Accuracy on evaluation data: 8672 / 10000\n",
      "\n",
      "Epoch 265 training complete\n",
      "Cost on training data: 0.17923635159903417\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.925895038793785\n",
      "Accuracy on evaluation data: 8666 / 10000\n",
      "\n",
      "Epoch 266 training complete\n",
      "Cost on training data: 0.17829121090141895\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9405541582147597\n",
      "Accuracy on evaluation data: 8661 / 10000\n",
      "\n",
      "Epoch 267 training complete\n",
      "Cost on training data: 0.17743381495457627\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9298919749706543\n",
      "Accuracy on evaluation data: 8665 / 10000\n",
      "\n",
      "Epoch 268 training complete\n",
      "Cost on training data: 0.17655189744059666\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.930794899819664\n",
      "Accuracy on evaluation data: 8674 / 10000\n",
      "\n",
      "Epoch 269 training complete\n",
      "Cost on training data: 0.1757768824367166\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9333165259564813\n",
      "Accuracy on evaluation data: 8669 / 10000\n",
      "\n",
      "Epoch 270 training complete\n",
      "Cost on training data: 0.17515599387131187\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.923967763970937\n",
      "Accuracy on evaluation data: 8687 / 10000\n",
      "\n",
      "Epoch 271 training complete\n",
      "Cost on training data: 0.17428218432640286\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9316818164129962\n",
      "Accuracy on evaluation data: 8662 / 10000\n",
      "\n",
      "Epoch 272 training complete\n",
      "Cost on training data: 0.17343174255533747\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9262344193892019\n",
      "Accuracy on evaluation data: 8668 / 10000\n",
      "\n",
      "Epoch 273 training complete\n",
      "Cost on training data: 0.17259585383630904\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9319635885244847\n",
      "Accuracy on evaluation data: 8673 / 10000\n",
      "\n",
      "Epoch 274 training complete\n",
      "Cost on training data: 0.17196512619908713\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9283138898874556\n",
      "Accuracy on evaluation data: 8678 / 10000\n",
      "\n",
      "Epoch 275 training complete\n",
      "Cost on training data: 0.17114438498709952\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9241715161483195\n",
      "Accuracy on evaluation data: 8672 / 10000\n",
      "\n",
      "Epoch 276 training complete\n",
      "Cost on training data: 0.1705903431151126\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9231908544293437\n",
      "Accuracy on evaluation data: 8676 / 10000\n",
      "\n",
      "Epoch 277 training complete\n",
      "Cost on training data: 0.16946444477795145\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9293683004018858\n",
      "Accuracy on evaluation data: 8676 / 10000\n",
      "\n",
      "Epoch 278 training complete\n",
      "Cost on training data: 0.1688893513279813\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9254664636956448\n",
      "Accuracy on evaluation data: 8678 / 10000\n",
      "\n",
      "Epoch 279 training complete\n",
      "Cost on training data: 0.1680763217000439\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9185846312546324\n",
      "Accuracy on evaluation data: 8686 / 10000\n",
      "\n",
      "Epoch 280 training complete\n",
      "Cost on training data: 0.16719422597940164\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9201434627694255\n",
      "Accuracy on evaluation data: 8684 / 10000\n",
      "\n",
      "Epoch 281 training complete\n",
      "Cost on training data: 0.1665387538776428\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9240127122705325\n",
      "Accuracy on evaluation data: 8679 / 10000\n",
      "\n",
      "Epoch 282 training complete\n",
      "Cost on training data: 0.1659809769236756\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9143468274140467\n",
      "Accuracy on evaluation data: 8693 / 10000\n",
      "\n",
      "Epoch 283 training complete\n",
      "Cost on training data: 0.16509767694324654\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.929193073184214\n",
      "Accuracy on evaluation data: 8677 / 10000\n",
      "\n",
      "Epoch 284 training complete\n",
      "Cost on training data: 0.16437468279352582\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9234477601936643\n",
      "Accuracy on evaluation data: 8677 / 10000\n",
      "\n",
      "Epoch 285 training complete\n",
      "Cost on training data: 0.16375509949087247\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9229715174399458\n",
      "Accuracy on evaluation data: 8690 / 10000\n",
      "\n",
      "Epoch 286 training complete\n",
      "Cost on training data: 0.16312438123972411\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9219872997475576\n",
      "Accuracy on evaluation data: 8685 / 10000\n",
      "\n",
      "Epoch 287 training complete\n",
      "Cost on training data: 0.16238773594503353\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9278863941206141\n",
      "Accuracy on evaluation data: 8666 / 10000\n",
      "\n",
      "Epoch 288 training complete\n",
      "Cost on training data: 0.16171737745222378\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9162170082621952\n",
      "Accuracy on evaluation data: 8693 / 10000\n",
      "\n",
      "Epoch 289 training complete\n",
      "Cost on training data: 0.16090668793452625\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9208713243196645\n",
      "Accuracy on evaluation data: 8686 / 10000\n",
      "\n",
      "Epoch 290 training complete\n",
      "Cost on training data: 0.1602558870311065\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9188114701058946\n",
      "Accuracy on evaluation data: 8683 / 10000\n",
      "\n",
      "Epoch 291 training complete\n",
      "Cost on training data: 0.15961448429884695\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9143963232959892\n",
      "Accuracy on evaluation data: 8698 / 10000\n",
      "\n",
      "Epoch 292 training complete\n",
      "Cost on training data: 0.1589749487673211\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9174418624759592\n",
      "Accuracy on evaluation data: 8685 / 10000\n",
      "\n",
      "Epoch 293 training complete\n",
      "Cost on training data: 0.15834784452997885\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9173218247919941\n",
      "Accuracy on evaluation data: 8698 / 10000\n",
      "\n",
      "Epoch 294 training complete\n",
      "Cost on training data: 0.1576186070585951\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9119292853465099\n",
      "Accuracy on evaluation data: 8695 / 10000\n",
      "\n",
      "Epoch 295 training complete\n",
      "Cost on training data: 0.1570761666369136\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9186811552148649\n",
      "Accuracy on evaluation data: 8687 / 10000\n",
      "\n",
      "Epoch 296 training complete\n",
      "Cost on training data: 0.15648837850283748\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9160085440327893\n",
      "Accuracy on evaluation data: 8687 / 10000\n",
      "\n",
      "Epoch 297 training complete\n",
      "Cost on training data: 0.15576480506475202\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9133655126329147\n",
      "Accuracy on evaluation data: 8686 / 10000\n",
      "\n",
      "Epoch 298 training complete\n",
      "Cost on training data: 0.15523894243080136\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9087815535870053\n",
      "Accuracy on evaluation data: 8693 / 10000\n",
      "\n",
      "Epoch 299 training complete\n",
      "Cost on training data: 0.15474076986756774\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9161816679389662\n",
      "Accuracy on evaluation data: 8691 / 10000\n",
      "\n",
      "Epoch 300 training complete\n",
      "Cost on training data: 0.15397885155860988\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9182861163018529\n",
      "Accuracy on evaluation data: 8700 / 10000\n",
      "\n",
      "Epoch 301 training complete\n",
      "Cost on training data: 0.1533877164300387\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9029855589340913\n",
      "Accuracy on evaluation data: 8703 / 10000\n",
      "\n",
      "Epoch 302 training complete\n",
      "Cost on training data: 0.1526398901511361\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9111749814076591\n",
      "Accuracy on evaluation data: 8709 / 10000\n",
      "\n",
      "Epoch 303 training complete\n",
      "Cost on training data: 0.15198280219247623\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9080301970058986\n",
      "Accuracy on evaluation data: 8701 / 10000\n",
      "\n",
      "Epoch 304 training complete\n",
      "Cost on training data: 0.15136713153224185\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9127576726542325\n",
      "Accuracy on evaluation data: 8703 / 10000\n",
      "\n",
      "Epoch 305 training complete\n",
      "Cost on training data: 0.15087689393804346\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9115275021164442\n",
      "Accuracy on evaluation data: 8700 / 10000\n",
      "\n",
      "Epoch 306 training complete\n",
      "Cost on training data: 0.15061481695517195\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9133896533080395\n",
      "Accuracy on evaluation data: 8691 / 10000\n",
      "\n",
      "Epoch 307 training complete\n",
      "Cost on training data: 0.14979558768264625\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9075752340660524\n",
      "Accuracy on evaluation data: 8705 / 10000\n",
      "\n",
      "Epoch 308 training complete\n",
      "Cost on training data: 0.14927987039105556\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9045404654069266\n",
      "Accuracy on evaluation data: 8698 / 10000\n",
      "\n",
      "Epoch 309 training complete\n",
      "Cost on training data: 0.14867351671706927\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9076844448082639\n",
      "Accuracy on evaluation data: 8698 / 10000\n",
      "\n",
      "Epoch 310 training complete\n",
      "Cost on training data: 0.14824738331256163\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9152689540832921\n",
      "Accuracy on evaluation data: 8689 / 10000\n",
      "\n",
      "Epoch 311 training complete\n",
      "Cost on training data: 0.14751559615503584\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9114495681915712\n",
      "Accuracy on evaluation data: 8693 / 10000\n",
      "\n",
      "Epoch 312 training complete\n",
      "Cost on training data: 0.14714506689064788\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9066799139652518\n",
      "Accuracy on evaluation data: 8706 / 10000\n",
      "\n",
      "Epoch 313 training complete\n",
      "Cost on training data: 0.14654459799463124\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9046701180098665\n",
      "Accuracy on evaluation data: 8705 / 10000\n",
      "\n",
      "Epoch 314 training complete\n",
      "Cost on training data: 0.14610408878581255\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8994199031360061\n",
      "Accuracy on evaluation data: 8711 / 10000\n",
      "\n",
      "Epoch 315 training complete\n",
      "Cost on training data: 0.14541002450954113\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9096569242190339\n",
      "Accuracy on evaluation data: 8703 / 10000\n",
      "\n",
      "Epoch 316 training complete\n",
      "Cost on training data: 0.14494541257912696\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9045282182423501\n",
      "Accuracy on evaluation data: 8709 / 10000\n",
      "\n",
      "Epoch 317 training complete\n",
      "Cost on training data: 0.14430489547863912\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.905291891564321\n",
      "Accuracy on evaluation data: 8710 / 10000\n",
      "\n",
      "Epoch 318 training complete\n",
      "Cost on training data: 0.14389255948024976\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9110975799364553\n",
      "Accuracy on evaluation data: 8694 / 10000\n",
      "\n",
      "Epoch 319 training complete\n",
      "Cost on training data: 0.14328954410250852\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8986962005857224\n",
      "Accuracy on evaluation data: 8713 / 10000\n",
      "\n",
      "Epoch 320 training complete\n",
      "Cost on training data: 0.14274463063571757\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.898507476103654\n",
      "Accuracy on evaluation data: 8712 / 10000\n",
      "\n",
      "Epoch 321 training complete\n",
      "Cost on training data: 0.1424276100069769\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8981241748525064\n",
      "Accuracy on evaluation data: 8722 / 10000\n",
      "\n",
      "Epoch 322 training complete\n",
      "Cost on training data: 0.14200550336467796\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9054654543784574\n",
      "Accuracy on evaluation data: 8713 / 10000\n",
      "\n",
      "Epoch 323 training complete\n",
      "Cost on training data: 0.1416521564534741\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9113947706066297\n",
      "Accuracy on evaluation data: 8695 / 10000\n",
      "\n",
      "Epoch 324 training complete\n",
      "Cost on training data: 0.14086812838597168\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8999994643955275\n",
      "Accuracy on evaluation data: 8716 / 10000\n",
      "\n",
      "Epoch 325 training complete\n",
      "Cost on training data: 0.14041965531528952\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8966331004039775\n",
      "Accuracy on evaluation data: 8725 / 10000\n",
      "\n",
      "Epoch 326 training complete\n",
      "Cost on training data: 0.13988531243923624\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9057935070385501\n",
      "Accuracy on evaluation data: 8714 / 10000\n",
      "\n",
      "Epoch 327 training complete\n",
      "Cost on training data: 0.13940316385263907\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8977192623884048\n",
      "Accuracy on evaluation data: 8709 / 10000\n",
      "\n",
      "Epoch 328 training complete\n",
      "Cost on training data: 0.1390653547618996\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9010813292139707\n",
      "Accuracy on evaluation data: 8720 / 10000\n",
      "\n",
      "Epoch 329 training complete\n",
      "Cost on training data: 0.13840641762630665\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8973515335373226\n",
      "Accuracy on evaluation data: 8718 / 10000\n",
      "\n",
      "Epoch 330 training complete\n",
      "Cost on training data: 0.13809647786134235\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9066591311976745\n",
      "Accuracy on evaluation data: 8703 / 10000\n",
      "\n",
      "Epoch 331 training complete\n",
      "Cost on training data: 0.13763672931172397\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9024310271589527\n",
      "Accuracy on evaluation data: 8714 / 10000\n",
      "\n",
      "Epoch 332 training complete\n",
      "Cost on training data: 0.13722322341318105\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9025203978987246\n",
      "Accuracy on evaluation data: 8712 / 10000\n",
      "\n",
      "Epoch 333 training complete\n",
      "Cost on training data: 0.13666075636561736\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9016414163878432\n",
      "Accuracy on evaluation data: 8723 / 10000\n",
      "\n",
      "Epoch 334 training complete\n",
      "Cost on training data: 0.13659463567558616\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9039009093908671\n",
      "Accuracy on evaluation data: 8723 / 10000\n",
      "\n",
      "Epoch 335 training complete\n",
      "Cost on training data: 0.1359804256692187\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8974435723889556\n",
      "Accuracy on evaluation data: 8723 / 10000\n",
      "\n",
      "Epoch 336 training complete\n",
      "Cost on training data: 0.13549187475524538\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8998189108944624\n",
      "Accuracy on evaluation data: 8720 / 10000\n",
      "\n",
      "Epoch 337 training complete\n",
      "Cost on training data: 0.13514032994652425\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8968045188052997\n",
      "Accuracy on evaluation data: 8708 / 10000\n",
      "\n",
      "Epoch 338 training complete\n",
      "Cost on training data: 0.13455695875456714\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8977925450729703\n",
      "Accuracy on evaluation data: 8719 / 10000\n",
      "\n",
      "Epoch 339 training complete\n",
      "Cost on training data: 0.13412776711441238\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8938847330531301\n",
      "Accuracy on evaluation data: 8723 / 10000\n",
      "\n",
      "Epoch 340 training complete\n",
      "Cost on training data: 0.13382142120787888\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8964139335535857\n",
      "Accuracy on evaluation data: 8715 / 10000\n",
      "\n",
      "Epoch 341 training complete\n",
      "Cost on training data: 0.1334833758712266\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8937719578181665\n",
      "Accuracy on evaluation data: 8728 / 10000\n",
      "\n",
      "Epoch 342 training complete\n",
      "Cost on training data: 0.13307927456685978\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8959650808636153\n",
      "Accuracy on evaluation data: 8726 / 10000\n",
      "\n",
      "Epoch 343 training complete\n",
      "Cost on training data: 0.13253665147958232\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8928465639955515\n",
      "Accuracy on evaluation data: 8732 / 10000\n",
      "\n",
      "Epoch 344 training complete\n",
      "Cost on training data: 0.13220457971134855\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.9012065759731173\n",
      "Accuracy on evaluation data: 8715 / 10000\n",
      "\n",
      "Epoch 345 training complete\n",
      "Cost on training data: 0.13186082090170267\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8948018382553846\n",
      "Accuracy on evaluation data: 8715 / 10000\n",
      "\n",
      "Epoch 346 training complete\n",
      "Cost on training data: 0.13134932751771472\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.895988370151585\n",
      "Accuracy on evaluation data: 8733 / 10000\n",
      "\n",
      "Epoch 347 training complete\n",
      "Cost on training data: 0.1309511617192614\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8967561251069361\n",
      "Accuracy on evaluation data: 8713 / 10000\n",
      "\n",
      "Epoch 348 training complete\n",
      "Cost on training data: 0.13063705327677239\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8889887680910349\n",
      "Accuracy on evaluation data: 8739 / 10000\n",
      "\n",
      "Epoch 349 training complete\n",
      "Cost on training data: 0.13020793394631225\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8954866350575044\n",
      "Accuracy on evaluation data: 8723 / 10000\n",
      "\n",
      "Epoch 350 training complete\n",
      "Cost on training data: 0.12991289447468798\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8893035525245923\n",
      "Accuracy on evaluation data: 8722 / 10000\n",
      "\n",
      "Epoch 351 training complete\n",
      "Cost on training data: 0.1294274642087198\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8915072511626887\n",
      "Accuracy on evaluation data: 8720 / 10000\n",
      "\n",
      "Epoch 352 training complete\n",
      "Cost on training data: 0.1291551663448325\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.895362787427229\n",
      "Accuracy on evaluation data: 8727 / 10000\n",
      "\n",
      "Epoch 353 training complete\n",
      "Cost on training data: 0.12872749062865904\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8894780112760973\n",
      "Accuracy on evaluation data: 8723 / 10000\n",
      "\n",
      "Epoch 354 training complete\n",
      "Cost on training data: 0.12830604788834998\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.887906436010965\n",
      "Accuracy on evaluation data: 8740 / 10000\n",
      "\n",
      "Epoch 355 training complete\n",
      "Cost on training data: 0.12798326213367883\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8879469576978669\n",
      "Accuracy on evaluation data: 8738 / 10000\n",
      "\n",
      "Epoch 356 training complete\n",
      "Cost on training data: 0.12768015092104415\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8909771501055641\n",
      "Accuracy on evaluation data: 8735 / 10000\n",
      "\n",
      "Epoch 357 training complete\n",
      "Cost on training data: 0.12731643120411096\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8966096126636529\n",
      "Accuracy on evaluation data: 8725 / 10000\n",
      "\n",
      "Epoch 358 training complete\n",
      "Cost on training data: 0.127015768628487\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8928595880760601\n",
      "Accuracy on evaluation data: 8716 / 10000\n",
      "\n",
      "Epoch 359 training complete\n",
      "Cost on training data: 0.12665661911279769\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8896738367790016\n",
      "Accuracy on evaluation data: 8731 / 10000\n",
      "\n",
      "Epoch 360 training complete\n",
      "Cost on training data: 0.12630418545985422\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8890339056919567\n",
      "Accuracy on evaluation data: 8733 / 10000\n",
      "\n",
      "Epoch 361 training complete\n",
      "Cost on training data: 0.12591754109259137\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8899547417426124\n",
      "Accuracy on evaluation data: 8742 / 10000\n",
      "\n",
      "Epoch 362 training complete\n",
      "Cost on training data: 0.12576279093863005\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8875654555403062\n",
      "Accuracy on evaluation data: 8737 / 10000\n",
      "\n",
      "Epoch 363 training complete\n",
      "Cost on training data: 0.1252841535302036\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8951702671073046\n",
      "Accuracy on evaluation data: 8729 / 10000\n",
      "\n",
      "Epoch 364 training complete\n",
      "Cost on training data: 0.12496214109610776\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8922329577410045\n",
      "Accuracy on evaluation data: 8732 / 10000\n",
      "\n",
      "Epoch 365 training complete\n",
      "Cost on training data: 0.12477634181400482\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8890993273515203\n",
      "Accuracy on evaluation data: 8737 / 10000\n",
      "\n",
      "Epoch 366 training complete\n",
      "Cost on training data: 0.12418190706360846\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8854097123369176\n",
      "Accuracy on evaluation data: 8737 / 10000\n",
      "\n",
      "Epoch 367 training complete\n",
      "Cost on training data: 0.12404011301798691\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8842808318160865\n",
      "Accuracy on evaluation data: 8756 / 10000\n",
      "\n",
      "Epoch 368 training complete\n",
      "Cost on training data: 0.12372892795586488\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.885156643459547\n",
      "Accuracy on evaluation data: 8739 / 10000\n",
      "\n",
      "Epoch 369 training complete\n",
      "Cost on training data: 0.12334559775054427\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8891368292987036\n",
      "Accuracy on evaluation data: 8727 / 10000\n",
      "\n",
      "Epoch 370 training complete\n",
      "Cost on training data: 0.12313068799113454\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8885179436227116\n",
      "Accuracy on evaluation data: 8728 / 10000\n",
      "\n",
      "Epoch 371 training complete\n",
      "Cost on training data: 0.12268723592112933\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8829853052804392\n",
      "Accuracy on evaluation data: 8746 / 10000\n",
      "\n",
      "Epoch 372 training complete\n",
      "Cost on training data: 0.12245066933362532\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8907185612671755\n",
      "Accuracy on evaluation data: 8720 / 10000\n",
      "\n",
      "Epoch 373 training complete\n",
      "Cost on training data: 0.12205321962996793\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8878006409103569\n",
      "Accuracy on evaluation data: 8738 / 10000\n",
      "\n",
      "Epoch 374 training complete\n",
      "Cost on training data: 0.1217796506247912\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8867877562058122\n",
      "Accuracy on evaluation data: 8746 / 10000\n",
      "\n",
      "Epoch 375 training complete\n",
      "Cost on training data: 0.1217154116433743\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8812575373082276\n",
      "Accuracy on evaluation data: 8741 / 10000\n",
      "\n",
      "Epoch 376 training complete\n",
      "Cost on training data: 0.1211146928979416\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8853882138329274\n",
      "Accuracy on evaluation data: 8745 / 10000\n",
      "\n",
      "Epoch 377 training complete\n",
      "Cost on training data: 0.12094016257824963\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8884144167563619\n",
      "Accuracy on evaluation data: 8731 / 10000\n",
      "\n",
      "Epoch 378 training complete\n",
      "Cost on training data: 0.1207055783657987\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8879709464763519\n",
      "Accuracy on evaluation data: 8735 / 10000\n",
      "\n",
      "Epoch 379 training complete\n",
      "Cost on training data: 0.12030480907241134\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8816812442515348\n",
      "Accuracy on evaluation data: 8750 / 10000\n",
      "\n",
      "Epoch 380 training complete\n",
      "Cost on training data: 0.12002667710113231\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8882334696024144\n",
      "Accuracy on evaluation data: 8738 / 10000\n",
      "\n",
      "Epoch 381 training complete\n",
      "Cost on training data: 0.1197550512097989\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8842120244284016\n",
      "Accuracy on evaluation data: 8742 / 10000\n",
      "\n",
      "Epoch 382 training complete\n",
      "Cost on training data: 0.11952015550686504\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8784756010143383\n",
      "Accuracy on evaluation data: 8751 / 10000\n",
      "\n",
      "Epoch 383 training complete\n",
      "Cost on training data: 0.11924465868486546\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8880050666448069\n",
      "Accuracy on evaluation data: 8735 / 10000\n",
      "\n",
      "Epoch 384 training complete\n",
      "Cost on training data: 0.11897138214219677\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8897822486526168\n",
      "Accuracy on evaluation data: 8728 / 10000\n",
      "\n",
      "Epoch 385 training complete\n",
      "Cost on training data: 0.11869894327847384\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8809958104284773\n",
      "Accuracy on evaluation data: 8742 / 10000\n",
      "\n",
      "Epoch 386 training complete\n",
      "Cost on training data: 0.11846789595653887\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8832087297566961\n",
      "Accuracy on evaluation data: 8751 / 10000\n",
      "\n",
      "Epoch 387 training complete\n",
      "Cost on training data: 0.11814513138638674\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8810280444954374\n",
      "Accuracy on evaluation data: 8742 / 10000\n",
      "\n",
      "Epoch 388 training complete\n",
      "Cost on training data: 0.11781062936212913\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8772446111302281\n",
      "Accuracy on evaluation data: 8753 / 10000\n",
      "\n",
      "Epoch 389 training complete\n",
      "Cost on training data: 0.1176301933870251\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8797054177541402\n",
      "Accuracy on evaluation data: 8741 / 10000\n",
      "\n",
      "Epoch 390 training complete\n",
      "Cost on training data: 0.11763400874216907\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8762414386657276\n",
      "Accuracy on evaluation data: 8744 / 10000\n",
      "\n",
      "Epoch 391 training complete\n",
      "Cost on training data: 0.11711087562023884\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8806486978943542\n",
      "Accuracy on evaluation data: 8747 / 10000\n",
      "\n",
      "Epoch 392 training complete\n",
      "Cost on training data: 0.11683899322161895\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8823161322390223\n",
      "Accuracy on evaluation data: 8742 / 10000\n",
      "\n",
      "Epoch 393 training complete\n",
      "Cost on training data: 0.11656884289291108\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8795365942309408\n",
      "Accuracy on evaluation data: 8745 / 10000\n",
      "\n",
      "Epoch 394 training complete\n",
      "Cost on training data: 0.11654498429924139\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8812476922062561\n",
      "Accuracy on evaluation data: 8739 / 10000\n",
      "\n",
      "Epoch 395 training complete\n",
      "Cost on training data: 0.11659580863812458\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8874793241529771\n",
      "Accuracy on evaluation data: 8725 / 10000\n",
      "\n",
      "Epoch 396 training complete\n",
      "Cost on training data: 0.11596247839603235\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8840923072562196\n",
      "Accuracy on evaluation data: 8741 / 10000\n",
      "\n",
      "Epoch 397 training complete\n",
      "Cost on training data: 0.11568107545706538\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8835427750679575\n",
      "Accuracy on evaluation data: 8741 / 10000\n",
      "\n",
      "Epoch 398 training complete\n",
      "Cost on training data: 0.11535491369887901\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.8747393354750661\n",
      "Accuracy on evaluation data: 8762 / 10000\n",
      "\n",
      "Epoch 399 training complete\n",
      "Cost on training data: 0.11523512735856611\n",
      "Accuracy on training data: 1000 / 1000\n",
      "Cost on evaluation data: 0.876682472478315\n",
      "Accuracy on evaluation data: 8746 / 10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([2.4374085774393746,\n",
       "  1.8765149887329786,\n",
       "  1.7794002969003386,\n",
       "  1.6342959983529344,\n",
       "  1.5198826263664664,\n",
       "  1.4600010151048566,\n",
       "  1.4231405794377416,\n",
       "  1.3957942251437045,\n",
       "  1.3950703560897051,\n",
       "  1.3536018442452293,\n",
       "  1.3536994533152651,\n",
       "  1.3602396250591193,\n",
       "  1.3504967367686207,\n",
       "  1.341945692237333,\n",
       "  1.310829694956184,\n",
       "  1.3211836742294636,\n",
       "  1.3327887177519562,\n",
       "  1.302170611677719,\n",
       "  1.3072866391718785,\n",
       "  1.2999129180713995,\n",
       "  1.2953852603797671,\n",
       "  1.2964146300969608,\n",
       "  1.3251401573192871,\n",
       "  1.2972418492112934,\n",
       "  1.305965444272756,\n",
       "  1.3062330982641408,\n",
       "  1.3266184227745921,\n",
       "  1.3119714562962712,\n",
       "  1.3114572966685203,\n",
       "  1.3108039840214631,\n",
       "  1.3110856345223105,\n",
       "  1.2986315739941245,\n",
       "  1.3015682774135116,\n",
       "  1.3118830810975821,\n",
       "  1.3167083374215169,\n",
       "  1.3084092524000437,\n",
       "  1.3042997865592694,\n",
       "  1.2970537130522122,\n",
       "  1.3078298636695138,\n",
       "  1.2999754871564573,\n",
       "  1.2942152145694825,\n",
       "  1.309535797380428,\n",
       "  1.3005896950798275,\n",
       "  1.3107243311992205,\n",
       "  1.299577304821208,\n",
       "  1.2868819017220414,\n",
       "  1.3046497267591171,\n",
       "  1.2872144775660779,\n",
       "  1.3003004175586466,\n",
       "  1.2968503255862929,\n",
       "  1.2944657397306618,\n",
       "  1.2910694356167203,\n",
       "  1.2864195674736612,\n",
       "  1.2814170014964308,\n",
       "  1.3012104564951528,\n",
       "  1.2878439994036242,\n",
       "  1.2775770974176182,\n",
       "  1.2815348878884414,\n",
       "  1.2826521482382953,\n",
       "  1.2721596621884628,\n",
       "  1.2827993274832854,\n",
       "  1.2703567152750601,\n",
       "  1.2710825036780977,\n",
       "  1.2759548070733344,\n",
       "  1.2698593010859933,\n",
       "  1.2607468185949791,\n",
       "  1.2625884335301645,\n",
       "  1.2656994611831034,\n",
       "  1.25872869501002,\n",
       "  1.2580989368157394,\n",
       "  1.2540974096270463,\n",
       "  1.2630306716823183,\n",
       "  1.2485941458289047,\n",
       "  1.2513498416570286,\n",
       "  1.2538663836172848,\n",
       "  1.2487138376877589,\n",
       "  1.2469174485756129,\n",
       "  1.2502502407247416,\n",
       "  1.2453935713873536,\n",
       "  1.2283951306693623,\n",
       "  1.2368389018977715,\n",
       "  1.2370481942253582,\n",
       "  1.2441937973753814,\n",
       "  1.2343283813421548,\n",
       "  1.2274376713717698,\n",
       "  1.22880885893289,\n",
       "  1.2242288948297,\n",
       "  1.2251209413762745,\n",
       "  1.2237079857619326,\n",
       "  1.2177628724279395,\n",
       "  1.2108452503011384,\n",
       "  1.2163195289992266,\n",
       "  1.2125076658021314,\n",
       "  1.2082899243136045,\n",
       "  1.2015376477919952,\n",
       "  1.2032218255050273,\n",
       "  1.2135456598891252,\n",
       "  1.2091481172849281,\n",
       "  1.1992759401198783,\n",
       "  1.2005578939589481,\n",
       "  1.1900695666616092,\n",
       "  1.193581503031673,\n",
       "  1.1875378657854423,\n",
       "  1.1887233665708863,\n",
       "  1.1847759279965535,\n",
       "  1.1762440994793033,\n",
       "  1.1798712893107504,\n",
       "  1.1720038499396219,\n",
       "  1.172039532179961,\n",
       "  1.1752658867567105,\n",
       "  1.1653376653277423,\n",
       "  1.1638420499254356,\n",
       "  1.1635770874294584,\n",
       "  1.1616044339438707,\n",
       "  1.1583297398533103,\n",
       "  1.1624040454788274,\n",
       "  1.1583285590902459,\n",
       "  1.1507817915846827,\n",
       "  1.1422410011240396,\n",
       "  1.1492561408852227,\n",
       "  1.1427966053261234,\n",
       "  1.1455973738772693,\n",
       "  1.146215414089935,\n",
       "  1.1429536081353069,\n",
       "  1.133528749635365,\n",
       "  1.1353366247792434,\n",
       "  1.1318254847543285,\n",
       "  1.1275368675214057,\n",
       "  1.126207263036298,\n",
       "  1.1271290902048152,\n",
       "  1.1299561730957759,\n",
       "  1.1214895714211859,\n",
       "  1.1177337154311524,\n",
       "  1.1189626196429452,\n",
       "  1.1213104766557165,\n",
       "  1.1205309075310388,\n",
       "  1.1122199490745934,\n",
       "  1.1116839211554552,\n",
       "  1.1104476253914648,\n",
       "  1.1072931488727578,\n",
       "  1.1076485110654593,\n",
       "  1.1046531721591948,\n",
       "  1.1037201362917402,\n",
       "  1.0957605281514289,\n",
       "  1.0996125278229099,\n",
       "  1.0896689508115771,\n",
       "  1.0892579743520461,\n",
       "  1.0912185316414633,\n",
       "  1.0884433908839961,\n",
       "  1.0963529305732009,\n",
       "  1.084244648558889,\n",
       "  1.086780626914333,\n",
       "  1.0798677486969905,\n",
       "  1.077846288477015,\n",
       "  1.0754561072705291,\n",
       "  1.0729816475234053,\n",
       "  1.0808413981405371,\n",
       "  1.0689255852867929,\n",
       "  1.0711262273294231,\n",
       "  1.0703314275863374,\n",
       "  1.0670068886548334,\n",
       "  1.0660598744725658,\n",
       "  1.0636492699266793,\n",
       "  1.0631796557275905,\n",
       "  1.0658051548424778,\n",
       "  1.0599773348170636,\n",
       "  1.0539174975348549,\n",
       "  1.0505767829986419,\n",
       "  1.0528379327524811,\n",
       "  1.0565324032875745,\n",
       "  1.0428388733734617,\n",
       "  1.0458530155205921,\n",
       "  1.0490424619497547,\n",
       "  1.0487899824868536,\n",
       "  1.0419689773959602,\n",
       "  1.0507687457071917,\n",
       "  1.030363746761755,\n",
       "  1.0437791015681457,\n",
       "  1.0329366661328032,\n",
       "  1.0378950639331619,\n",
       "  1.0313381681025233,\n",
       "  1.0244076071695947,\n",
       "  1.0324621309382078,\n",
       "  1.0273034563297871,\n",
       "  1.0330526583073822,\n",
       "  1.03245103766701,\n",
       "  1.0220958932475859,\n",
       "  1.0187007656644178,\n",
       "  1.0225232171622511,\n",
       "  1.0126046467038665,\n",
       "  1.01752272453477,\n",
       "  1.0206486373685166,\n",
       "  1.0186751497940392,\n",
       "  1.0075140543848307,\n",
       "  1.0138116511126529,\n",
       "  1.0106676687529643,\n",
       "  1.0030712689950261,\n",
       "  1.0037714181923918,\n",
       "  0.9952855440141859,\n",
       "  1.0079693295650205,\n",
       "  1.0014553072562185,\n",
       "  1.0024996679712623,\n",
       "  0.99977968912486426,\n",
       "  0.9971565791614444,\n",
       "  0.99288845707783713,\n",
       "  0.99488424134637587,\n",
       "  0.99842440167925262,\n",
       "  0.98389259103783644,\n",
       "  0.98785996752292116,\n",
       "  0.98878527159800655,\n",
       "  0.9879160181574167,\n",
       "  0.98789232072130051,\n",
       "  0.98837982995064289,\n",
       "  0.98917521617029813,\n",
       "  0.98780805546445327,\n",
       "  0.99049566298616332,\n",
       "  0.98743519345698361,\n",
       "  0.98837050290053552,\n",
       "  0.9771534262436038,\n",
       "  0.97824568116608079,\n",
       "  0.97387324090945204,\n",
       "  0.96755437582647019,\n",
       "  0.97546596191886803,\n",
       "  0.97418067906790795,\n",
       "  0.96609226909289581,\n",
       "  0.96870069857307806,\n",
       "  0.96734449498752051,\n",
       "  0.96856315386898773,\n",
       "  0.97057462266393602,\n",
       "  0.97075195899252698,\n",
       "  0.96541110900603588,\n",
       "  0.96369882391372086,\n",
       "  0.96520505152890534,\n",
       "  0.96470662194387413,\n",
       "  0.95765068958292743,\n",
       "  0.95966123715863272,\n",
       "  0.9640241707771896,\n",
       "  0.96105228451071634,\n",
       "  0.95305337833119297,\n",
       "  0.9630995072537839,\n",
       "  0.94915759066400007,\n",
       "  0.95777633786072613,\n",
       "  0.9531778584765277,\n",
       "  0.9484288321709573,\n",
       "  0.95733978290271737,\n",
       "  0.94913045898234139,\n",
       "  0.95279887216231707,\n",
       "  0.94562229288108623,\n",
       "  0.958721905398631,\n",
       "  0.9478472364377204,\n",
       "  0.94477168871298167,\n",
       "  0.94250319427765827,\n",
       "  0.9559330729937402,\n",
       "  0.94495042459423262,\n",
       "  0.94663326466436626,\n",
       "  0.94484038455934494,\n",
       "  0.94197475681114573,\n",
       "  0.93528328313983244,\n",
       "  0.9357216900073444,\n",
       "  0.93508806048009641,\n",
       "  0.93586844937540836,\n",
       "  0.93188111404609275,\n",
       "  0.93568781499492026,\n",
       "  0.93699560129377235,\n",
       "  0.93515118556739996,\n",
       "  0.92589503879378499,\n",
       "  0.94055415821475974,\n",
       "  0.92989197497065434,\n",
       "  0.93079489981966401,\n",
       "  0.93331652595648129,\n",
       "  0.92396776397093705,\n",
       "  0.93168181641299619,\n",
       "  0.92623441938920192,\n",
       "  0.93196358852448469,\n",
       "  0.92831388988745556,\n",
       "  0.92417151614831949,\n",
       "  0.92319085442934368,\n",
       "  0.92936830040188578,\n",
       "  0.92546646369564478,\n",
       "  0.91858463125463241,\n",
       "  0.92014346276942549,\n",
       "  0.9240127122705325,\n",
       "  0.91434682741404671,\n",
       "  0.92919307318421396,\n",
       "  0.92344776019366426,\n",
       "  0.92297151743994577,\n",
       "  0.92198729974755755,\n",
       "  0.92788639412061413,\n",
       "  0.91621700826219521,\n",
       "  0.92087132431966445,\n",
       "  0.91881147010589459,\n",
       "  0.91439632329598919,\n",
       "  0.91744186247595916,\n",
       "  0.91732182479199409,\n",
       "  0.91192928534650985,\n",
       "  0.91868115521486493,\n",
       "  0.91600854403278931,\n",
       "  0.91336551263291466,\n",
       "  0.9087815535870053,\n",
       "  0.91618166793896616,\n",
       "  0.91828611630185286,\n",
       "  0.90298555893409127,\n",
       "  0.91117498140765907,\n",
       "  0.9080301970058986,\n",
       "  0.91275767265423247,\n",
       "  0.91152750211644418,\n",
       "  0.91338965330803945,\n",
       "  0.90757523406605245,\n",
       "  0.90454046540692656,\n",
       "  0.90768444480826393,\n",
       "  0.91526895408329212,\n",
       "  0.91144956819157119,\n",
       "  0.90667991396525183,\n",
       "  0.90467011800986652,\n",
       "  0.89941990313600606,\n",
       "  0.90965692421903388,\n",
       "  0.90452821824235008,\n",
       "  0.90529189156432099,\n",
       "  0.91109757993645535,\n",
       "  0.89869620058572242,\n",
       "  0.898507476103654,\n",
       "  0.8981241748525064,\n",
       "  0.90546545437845738,\n",
       "  0.91139477060662966,\n",
       "  0.89999946439552747,\n",
       "  0.89663310040397748,\n",
       "  0.90579350703855011,\n",
       "  0.89771926238840483,\n",
       "  0.90108132921397066,\n",
       "  0.89735153353732255,\n",
       "  0.90665913119767449,\n",
       "  0.90243102715895274,\n",
       "  0.90252039789872462,\n",
       "  0.90164141638784323,\n",
       "  0.90390090939086709,\n",
       "  0.89744357238895556,\n",
       "  0.89981891089446242,\n",
       "  0.89680451880529966,\n",
       "  0.89779254507297035,\n",
       "  0.8938847330531301,\n",
       "  0.89641393355358567,\n",
       "  0.89377195781816654,\n",
       "  0.89596508086361526,\n",
       "  0.89284656399555151,\n",
       "  0.90120657597311726,\n",
       "  0.89480183825538462,\n",
       "  0.89598837015158495,\n",
       "  0.89675612510693614,\n",
       "  0.88898876809103489,\n",
       "  0.89548663505750437,\n",
       "  0.88930355252459226,\n",
       "  0.8915072511626887,\n",
       "  0.89536278742722897,\n",
       "  0.88947801127609727,\n",
       "  0.88790643601096497,\n",
       "  0.88794695769786691,\n",
       "  0.89097715010556411,\n",
       "  0.89660961266365291,\n",
       "  0.89285958807606014,\n",
       "  0.88967383677900158,\n",
       "  0.88903390569195673,\n",
       "  0.88995474174261235,\n",
       "  0.88756545554030619,\n",
       "  0.89517026710730463,\n",
       "  0.89223295774100453,\n",
       "  0.88909932735152031,\n",
       "  0.88540971233691756,\n",
       "  0.88428083181608652,\n",
       "  0.88515664345954703,\n",
       "  0.88913682929870363,\n",
       "  0.88851794362271164,\n",
       "  0.88298530528043917,\n",
       "  0.89071856126717552,\n",
       "  0.88780064091035693,\n",
       "  0.88678775620581218,\n",
       "  0.88125753730822765,\n",
       "  0.88538821383292743,\n",
       "  0.8884144167563619,\n",
       "  0.88797094647635189,\n",
       "  0.88168124425153482,\n",
       "  0.88823346960241445,\n",
       "  0.88421202442840163,\n",
       "  0.8784756010143383,\n",
       "  0.88800506664480694,\n",
       "  0.88978224865261679,\n",
       "  0.8809958104284773,\n",
       "  0.88320872975669606,\n",
       "  0.88102804449543737,\n",
       "  0.87724461113022811,\n",
       "  0.87970541775414024,\n",
       "  0.87624143866572757,\n",
       "  0.88064869789435418,\n",
       "  0.88231613223902228,\n",
       "  0.87953659423094077,\n",
       "  0.88124769220625609,\n",
       "  0.88747932415297714,\n",
       "  0.88409230725621957,\n",
       "  0.88354277506795753,\n",
       "  0.8747393354750661,\n",
       "  0.87668247247831499],\n",
       " [5290,\n",
       "  6690,\n",
       "  6905,\n",
       "  7336,\n",
       "  7597,\n",
       "  7702,\n",
       "  7785,\n",
       "  7857,\n",
       "  7835,\n",
       "  7949,\n",
       "  7975,\n",
       "  7993,\n",
       "  7989,\n",
       "  8043,\n",
       "  8100,\n",
       "  8115,\n",
       "  8097,\n",
       "  8150,\n",
       "  8156,\n",
       "  8190,\n",
       "  8184,\n",
       "  8202,\n",
       "  8138,\n",
       "  8204,\n",
       "  8188,\n",
       "  8203,\n",
       "  8165,\n",
       "  8216,\n",
       "  8207,\n",
       "  8222,\n",
       "  8241,\n",
       "  8238,\n",
       "  8265,\n",
       "  8259,\n",
       "  8252,\n",
       "  8215,\n",
       "  8259,\n",
       "  8272,\n",
       "  8234,\n",
       "  8282,\n",
       "  8280,\n",
       "  8240,\n",
       "  8272,\n",
       "  8254,\n",
       "  8277,\n",
       "  8290,\n",
       "  8267,\n",
       "  8302,\n",
       "  8277,\n",
       "  8298,\n",
       "  8311,\n",
       "  8303,\n",
       "  8312,\n",
       "  8321,\n",
       "  8290,\n",
       "  8313,\n",
       "  8323,\n",
       "  8334,\n",
       "  8321,\n",
       "  8320,\n",
       "  8326,\n",
       "  8328,\n",
       "  8325,\n",
       "  8339,\n",
       "  8335,\n",
       "  8336,\n",
       "  8346,\n",
       "  8338,\n",
       "  8360,\n",
       "  8356,\n",
       "  8361,\n",
       "  8349,\n",
       "  8361,\n",
       "  8357,\n",
       "  8354,\n",
       "  8349,\n",
       "  8368,\n",
       "  8356,\n",
       "  8356,\n",
       "  8381,\n",
       "  8370,\n",
       "  8378,\n",
       "  8349,\n",
       "  8365,\n",
       "  8374,\n",
       "  8371,\n",
       "  8384,\n",
       "  8372,\n",
       "  8385,\n",
       "  8388,\n",
       "  8377,\n",
       "  8383,\n",
       "  8385,\n",
       "  8387,\n",
       "  8397,\n",
       "  8391,\n",
       "  8378,\n",
       "  8375,\n",
       "  8398,\n",
       "  8385,\n",
       "  8412,\n",
       "  8388,\n",
       "  8392,\n",
       "  8391,\n",
       "  8403,\n",
       "  8404,\n",
       "  8388,\n",
       "  8403,\n",
       "  8410,\n",
       "  8407,\n",
       "  8434,\n",
       "  8411,\n",
       "  8414,\n",
       "  8414,\n",
       "  8427,\n",
       "  8421,\n",
       "  8418,\n",
       "  8431,\n",
       "  8447,\n",
       "  8423,\n",
       "  8443,\n",
       "  8437,\n",
       "  8430,\n",
       "  8442,\n",
       "  8452,\n",
       "  8460,\n",
       "  8455,\n",
       "  8468,\n",
       "  8461,\n",
       "  8461,\n",
       "  8455,\n",
       "  8473,\n",
       "  8476,\n",
       "  8481,\n",
       "  8473,\n",
       "  8480,\n",
       "  8487,\n",
       "  8474,\n",
       "  8494,\n",
       "  8497,\n",
       "  8490,\n",
       "  8487,\n",
       "  8505,\n",
       "  8514,\n",
       "  8509,\n",
       "  8512,\n",
       "  8523,\n",
       "  8512,\n",
       "  8513,\n",
       "  8501,\n",
       "  8515,\n",
       "  8514,\n",
       "  8528,\n",
       "  8529,\n",
       "  8531,\n",
       "  8528,\n",
       "  8527,\n",
       "  8536,\n",
       "  8531,\n",
       "  8536,\n",
       "  8536,\n",
       "  8537,\n",
       "  8551,\n",
       "  8535,\n",
       "  8541,\n",
       "  8538,\n",
       "  8550,\n",
       "  8553,\n",
       "  8551,\n",
       "  8539,\n",
       "  8553,\n",
       "  8550,\n",
       "  8555,\n",
       "  8548,\n",
       "  8566,\n",
       "  8542,\n",
       "  8565,\n",
       "  8558,\n",
       "  8572,\n",
       "  8572,\n",
       "  8575,\n",
       "  8574,\n",
       "  8563,\n",
       "  8570,\n",
       "  8559,\n",
       "  8569,\n",
       "  8580,\n",
       "  8584,\n",
       "  8577,\n",
       "  8589,\n",
       "  8592,\n",
       "  8580,\n",
       "  8597,\n",
       "  8590,\n",
       "  8586,\n",
       "  8598,\n",
       "  8602,\n",
       "  8610,\n",
       "  8623,\n",
       "  8588,\n",
       "  8601,\n",
       "  8597,\n",
       "  8595,\n",
       "  8603,\n",
       "  8610,\n",
       "  8608,\n",
       "  8598,\n",
       "  8637,\n",
       "  8614,\n",
       "  8627,\n",
       "  8628,\n",
       "  8629,\n",
       "  8606,\n",
       "  8613,\n",
       "  8626,\n",
       "  8608,\n",
       "  8611,\n",
       "  8626,\n",
       "  8634,\n",
       "  8630,\n",
       "  8629,\n",
       "  8635,\n",
       "  8637,\n",
       "  8622,\n",
       "  8645,\n",
       "  8638,\n",
       "  8646,\n",
       "  8646,\n",
       "  8632,\n",
       "  8629,\n",
       "  8633,\n",
       "  8639,\n",
       "  8633,\n",
       "  8631,\n",
       "  8636,\n",
       "  8645,\n",
       "  8643,\n",
       "  8642,\n",
       "  8656,\n",
       "  8620,\n",
       "  8650,\n",
       "  8638,\n",
       "  8653,\n",
       "  8653,\n",
       "  8639,\n",
       "  8648,\n",
       "  8652,\n",
       "  8672,\n",
       "  8652,\n",
       "  8660,\n",
       "  8665,\n",
       "  8665,\n",
       "  8648,\n",
       "  8658,\n",
       "  8666,\n",
       "  8655,\n",
       "  8656,\n",
       "  8671,\n",
       "  8676,\n",
       "  8658,\n",
       "  8670,\n",
       "  8675,\n",
       "  8661,\n",
       "  8663,\n",
       "  8672,\n",
       "  8666,\n",
       "  8661,\n",
       "  8665,\n",
       "  8674,\n",
       "  8669,\n",
       "  8687,\n",
       "  8662,\n",
       "  8668,\n",
       "  8673,\n",
       "  8678,\n",
       "  8672,\n",
       "  8676,\n",
       "  8676,\n",
       "  8678,\n",
       "  8686,\n",
       "  8684,\n",
       "  8679,\n",
       "  8693,\n",
       "  8677,\n",
       "  8677,\n",
       "  8690,\n",
       "  8685,\n",
       "  8666,\n",
       "  8693,\n",
       "  8686,\n",
       "  8683,\n",
       "  8698,\n",
       "  8685,\n",
       "  8698,\n",
       "  8695,\n",
       "  8687,\n",
       "  8687,\n",
       "  8686,\n",
       "  8693,\n",
       "  8691,\n",
       "  8700,\n",
       "  8703,\n",
       "  8709,\n",
       "  8701,\n",
       "  8703,\n",
       "  8700,\n",
       "  8691,\n",
       "  8705,\n",
       "  8698,\n",
       "  8698,\n",
       "  8689,\n",
       "  8693,\n",
       "  8706,\n",
       "  8705,\n",
       "  8711,\n",
       "  8703,\n",
       "  8709,\n",
       "  8710,\n",
       "  8694,\n",
       "  8713,\n",
       "  8712,\n",
       "  8722,\n",
       "  8713,\n",
       "  8695,\n",
       "  8716,\n",
       "  8725,\n",
       "  8714,\n",
       "  8709,\n",
       "  8720,\n",
       "  8718,\n",
       "  8703,\n",
       "  8714,\n",
       "  8712,\n",
       "  8723,\n",
       "  8723,\n",
       "  8723,\n",
       "  8720,\n",
       "  8708,\n",
       "  8719,\n",
       "  8723,\n",
       "  8715,\n",
       "  8728,\n",
       "  8726,\n",
       "  8732,\n",
       "  8715,\n",
       "  8715,\n",
       "  8733,\n",
       "  8713,\n",
       "  8739,\n",
       "  8723,\n",
       "  8722,\n",
       "  8720,\n",
       "  8727,\n",
       "  8723,\n",
       "  8740,\n",
       "  8738,\n",
       "  8735,\n",
       "  8725,\n",
       "  8716,\n",
       "  8731,\n",
       "  8733,\n",
       "  8742,\n",
       "  8737,\n",
       "  8729,\n",
       "  8732,\n",
       "  8737,\n",
       "  8737,\n",
       "  8756,\n",
       "  8739,\n",
       "  8727,\n",
       "  8728,\n",
       "  8746,\n",
       "  8720,\n",
       "  8738,\n",
       "  8746,\n",
       "  8741,\n",
       "  8745,\n",
       "  8731,\n",
       "  8735,\n",
       "  8750,\n",
       "  8738,\n",
       "  8742,\n",
       "  8751,\n",
       "  8735,\n",
       "  8728,\n",
       "  8742,\n",
       "  8751,\n",
       "  8742,\n",
       "  8753,\n",
       "  8741,\n",
       "  8744,\n",
       "  8747,\n",
       "  8742,\n",
       "  8745,\n",
       "  8739,\n",
       "  8725,\n",
       "  8741,\n",
       "  8741,\n",
       "  8762,\n",
       "  8746],\n",
       " [3.1093118559161024,\n",
       "  2.4756915124491066,\n",
       "  2.2630677023331209,\n",
       "  2.0616368508614893,\n",
       "  1.8992574421272295,\n",
       "  1.7935570116374095,\n",
       "  1.7002312057894562,\n",
       "  1.6147208153100236,\n",
       "  1.5601501127159791,\n",
       "  1.5034888238397459,\n",
       "  1.4672335137471868,\n",
       "  1.4244977730607331,\n",
       "  1.3910846959311722,\n",
       "  1.3582977844821154,\n",
       "  1.3149284026486128,\n",
       "  1.294650029649588,\n",
       "  1.2668119044888697,\n",
       "  1.2435519745157939,\n",
       "  1.2141421646665358,\n",
       "  1.1973450744054999,\n",
       "  1.1760345708971449,\n",
       "  1.1568177801623862,\n",
       "  1.1469729295257598,\n",
       "  1.1271407397545303,\n",
       "  1.1084155982871633,\n",
       "  1.0934603812593506,\n",
       "  1.0877525125148257,\n",
       "  1.0648572248712316,\n",
       "  1.0528511914800478,\n",
       "  1.0394769546682165,\n",
       "  1.0279302729664084,\n",
       "  1.0150567962287862,\n",
       "  1.0052562302654704,\n",
       "  0.99368371397322131,\n",
       "  0.98016289293417691,\n",
       "  0.97113032855314907,\n",
       "  0.95775723188438511,\n",
       "  0.94884094049579526,\n",
       "  0.94034276714760501,\n",
       "  0.92921572390851337,\n",
       "  0.91835048914622974,\n",
       "  0.90894883747197208,\n",
       "  0.90020726600476775,\n",
       "  0.89119646190256474,\n",
       "  0.88279713306171115,\n",
       "  0.87229794854453147,\n",
       "  0.86405531817011116,\n",
       "  0.85597505103805993,\n",
       "  0.8468704075774498,\n",
       "  0.83974817540647206,\n",
       "  0.83067976409913213,\n",
       "  0.82381072086062113,\n",
       "  0.81499843103839575,\n",
       "  0.80716996884482561,\n",
       "  0.8004446880726358,\n",
       "  0.79223615962430216,\n",
       "  0.78487830627606281,\n",
       "  0.77793878304027964,\n",
       "  0.77088940100417302,\n",
       "  0.76412110974649883,\n",
       "  0.75655385527142027,\n",
       "  0.75025059358921042,\n",
       "  0.74310633764509459,\n",
       "  0.73640222831214841,\n",
       "  0.72977872505951613,\n",
       "  0.72340304906999409,\n",
       "  0.71686683598773315,\n",
       "  0.71046303565085001,\n",
       "  0.70428558729660218,\n",
       "  0.69801941747938245,\n",
       "  0.69198559061522602,\n",
       "  0.68573163506832069,\n",
       "  0.67980848833320051,\n",
       "  0.6738442281856496,\n",
       "  0.66771092668374055,\n",
       "  0.66247351943269406,\n",
       "  0.65630135320164318,\n",
       "  0.65055540500691644,\n",
       "  0.64499635990521986,\n",
       "  0.6400604386352039,\n",
       "  0.63403340230456506,\n",
       "  0.62843593945669296,\n",
       "  0.62344563045696177,\n",
       "  0.61780969137075459,\n",
       "  0.61287478648517912,\n",
       "  0.60749334331433258,\n",
       "  0.60274502399130347,\n",
       "  0.59714359367031522,\n",
       "  0.59205683904233142,\n",
       "  0.58709012581096964,\n",
       "  0.58245982168681365,\n",
       "  0.57716368710375177,\n",
       "  0.57258140471196439,\n",
       "  0.56756481075455578,\n",
       "  0.56352407725328713,\n",
       "  0.55820498841930966,\n",
       "  0.55394626091466581,\n",
       "  0.54930603989738591,\n",
       "  0.54465822266259201,\n",
       "  0.54018243762113438,\n",
       "  0.53573868884549569,\n",
       "  0.53137629059262159,\n",
       "  0.52689961754903558,\n",
       "  0.52267977627242312,\n",
       "  0.51829470327735327,\n",
       "  0.51424531995550049,\n",
       "  0.50999250440244981,\n",
       "  0.50575563826437731,\n",
       "  0.50188618158059961,\n",
       "  0.49767059979525402,\n",
       "  0.49395304179015792,\n",
       "  0.48965804049506617,\n",
       "  0.48580168858298478,\n",
       "  0.48182235136386203,\n",
       "  0.47804493560185995,\n",
       "  0.47421612916251621,\n",
       "  0.47038289200042449,\n",
       "  0.46666368488120402,\n",
       "  0.46322421634687072,\n",
       "  0.4592379606092416,\n",
       "  0.45567472101049933,\n",
       "  0.45202032849796503,\n",
       "  0.44845709711428866,\n",
       "  0.4449126866594918,\n",
       "  0.44151665965954662,\n",
       "  0.43810745796978151,\n",
       "  0.43467945320506229,\n",
       "  0.43128092078385399,\n",
       "  0.42799160811175535,\n",
       "  0.42458930619165003,\n",
       "  0.42144243695179506,\n",
       "  0.41805860246793392,\n",
       "  0.41507918802422838,\n",
       "  0.41168454220501771,\n",
       "  0.40853607874817244,\n",
       "  0.40545518993281982,\n",
       "  0.4023538013537401,\n",
       "  0.39929626909963845,\n",
       "  0.39628510387321803,\n",
       "  0.39331310637885691,\n",
       "  0.39038678555787271,\n",
       "  0.3876537399677879,\n",
       "  0.38450299372512281,\n",
       "  0.38164922608369151,\n",
       "  0.37877275755748785,\n",
       "  0.3761056910343285,\n",
       "  0.37330709498362608,\n",
       "  0.37039563441137413,\n",
       "  0.36775819021001604,\n",
       "  0.36510367762405216,\n",
       "  0.36234836679373916,\n",
       "  0.35964685245525657,\n",
       "  0.35709167101618078,\n",
       "  0.35448953470757749,\n",
       "  0.3519343620708596,\n",
       "  0.34956494745647526,\n",
       "  0.34697776704884997,\n",
       "  0.34445352061301493,\n",
       "  0.34195953472428853,\n",
       "  0.33949420166759592,\n",
       "  0.33696716173026425,\n",
       "  0.33472635358459929,\n",
       "  0.33229807819143459,\n",
       "  0.32994810406202618,\n",
       "  0.32769409027294344,\n",
       "  0.32528157731144614,\n",
       "  0.32290283101985878,\n",
       "  0.32079530002255985,\n",
       "  0.31849802451787557,\n",
       "  0.31635745874196008,\n",
       "  0.31401628396466635,\n",
       "  0.31190306305763393,\n",
       "  0.30973063248703003,\n",
       "  0.30760781380614383,\n",
       "  0.30541791985993449,\n",
       "  0.30350296714171748,\n",
       "  0.3013384221564846,\n",
       "  0.29922322393893236,\n",
       "  0.29713130585628494,\n",
       "  0.29511849591471628,\n",
       "  0.29314942076254313,\n",
       "  0.29127148503076455,\n",
       "  0.28924219954008795,\n",
       "  0.28737008120836832,\n",
       "  0.28538310559016622,\n",
       "  0.28352626108553158,\n",
       "  0.28161098844753257,\n",
       "  0.27970775543997728,\n",
       "  0.27787020074746671,\n",
       "  0.27598864392562977,\n",
       "  0.27431378575020682,\n",
       "  0.27246016026263103,\n",
       "  0.27083509930213234,\n",
       "  0.26876225573981083,\n",
       "  0.26723485214278847,\n",
       "  0.26533596614325539,\n",
       "  0.26354901897169852,\n",
       "  0.26189076197492822,\n",
       "  0.26036658719744554,\n",
       "  0.25863408898485246,\n",
       "  0.25687229435235021,\n",
       "  0.25529552127111743,\n",
       "  0.25373052293092374,\n",
       "  0.25214971566007316,\n",
       "  0.25046994960719704,\n",
       "  0.24894185181847236,\n",
       "  0.24735766360524933,\n",
       "  0.24593769494623965,\n",
       "  0.24447889459767139,\n",
       "  0.24285847190964813,\n",
       "  0.24143466622360713,\n",
       "  0.23992707474278163,\n",
       "  0.23843813529593921,\n",
       "  0.23710718560228367,\n",
       "  0.23548908926341369,\n",
       "  0.2345210587867409,\n",
       "  0.23297548107652308,\n",
       "  0.23134230835185601,\n",
       "  0.23023188512368867,\n",
       "  0.22864078753693637,\n",
       "  0.22732350836391199,\n",
       "  0.22609335592709975,\n",
       "  0.22472158562069955,\n",
       "  0.22343433366658794,\n",
       "  0.22222122111591522,\n",
       "  0.22084181954053439,\n",
       "  0.21987199618103101,\n",
       "  0.21830360286938003,\n",
       "  0.2172417975010616,\n",
       "  0.21592548831805913,\n",
       "  0.21469549495491855,\n",
       "  0.21341049479856716,\n",
       "  0.21239566137110946,\n",
       "  0.21105228186386418,\n",
       "  0.21000554507882463,\n",
       "  0.20903115895300328,\n",
       "  0.20782159984049714,\n",
       "  0.20645329145616617,\n",
       "  0.20545871468448298,\n",
       "  0.20434075907792756,\n",
       "  0.20327499972730667,\n",
       "  0.2022172721610046,\n",
       "  0.20101521118230337,\n",
       "  0.20012018322884118,\n",
       "  0.19910710245743124,\n",
       "  0.19791708677746833,\n",
       "  0.19696842883934174,\n",
       "  0.19599479919873086,\n",
       "  0.19507198467316417,\n",
       "  0.19397188029868465,\n",
       "  0.1928503851232975,\n",
       "  0.19190992756426387,\n",
       "  0.19110437020615303,\n",
       "  0.18990203413619061,\n",
       "  0.18910786357232123,\n",
       "  0.18811013363243423,\n",
       "  0.18706985013936334,\n",
       "  0.18620967676686173,\n",
       "  0.18524757852322102,\n",
       "  0.18455432115959824,\n",
       "  0.18350696952290754,\n",
       "  0.18277643942157892,\n",
       "  0.18163047181941233,\n",
       "  0.18098488876558458,\n",
       "  0.18019913617752459,\n",
       "  0.17923635159903417,\n",
       "  0.17829121090141895,\n",
       "  0.17743381495457627,\n",
       "  0.17655189744059666,\n",
       "  0.1757768824367166,\n",
       "  0.17515599387131187,\n",
       "  0.17428218432640286,\n",
       "  0.17343174255533747,\n",
       "  0.17259585383630904,\n",
       "  0.17196512619908713,\n",
       "  0.17114438498709952,\n",
       "  0.17059034311511259,\n",
       "  0.16946444477795145,\n",
       "  0.1688893513279813,\n",
       "  0.16807632170004391,\n",
       "  0.16719422597940164,\n",
       "  0.1665387538776428,\n",
       "  0.16598097692367561,\n",
       "  0.16509767694324654,\n",
       "  0.16437468279352582,\n",
       "  0.16375509949087247,\n",
       "  0.16312438123972411,\n",
       "  0.16238773594503353,\n",
       "  0.16171737745222378,\n",
       "  0.16090668793452625,\n",
       "  0.16025588703110649,\n",
       "  0.15961448429884695,\n",
       "  0.1589749487673211,\n",
       "  0.15834784452997885,\n",
       "  0.15761860705859509,\n",
       "  0.15707616663691359,\n",
       "  0.15648837850283748,\n",
       "  0.15576480506475202,\n",
       "  0.15523894243080136,\n",
       "  0.15474076986756774,\n",
       "  0.15397885155860988,\n",
       "  0.15338771643003871,\n",
       "  0.1526398901511361,\n",
       "  0.15198280219247623,\n",
       "  0.15136713153224185,\n",
       "  0.15087689393804346,\n",
       "  0.15061481695517195,\n",
       "  0.14979558768264625,\n",
       "  0.14927987039105556,\n",
       "  0.14867351671706927,\n",
       "  0.14824738331256163,\n",
       "  0.14751559615503584,\n",
       "  0.14714506689064788,\n",
       "  0.14654459799463124,\n",
       "  0.14610408878581255,\n",
       "  0.14541002450954113,\n",
       "  0.14494541257912696,\n",
       "  0.14430489547863912,\n",
       "  0.14389255948024976,\n",
       "  0.14328954410250852,\n",
       "  0.14274463063571757,\n",
       "  0.1424276100069769,\n",
       "  0.14200550336467796,\n",
       "  0.14165215645347409,\n",
       "  0.14086812838597168,\n",
       "  0.14041965531528952,\n",
       "  0.13988531243923624,\n",
       "  0.13940316385263907,\n",
       "  0.1390653547618996,\n",
       "  0.13840641762630665,\n",
       "  0.13809647786134235,\n",
       "  0.13763672931172397,\n",
       "  0.13722322341318105,\n",
       "  0.13666075636561736,\n",
       "  0.13659463567558616,\n",
       "  0.1359804256692187,\n",
       "  0.13549187475524538,\n",
       "  0.13514032994652425,\n",
       "  0.13455695875456714,\n",
       "  0.13412776711441238,\n",
       "  0.13382142120787888,\n",
       "  0.13348337587122661,\n",
       "  0.13307927456685978,\n",
       "  0.13253665147958232,\n",
       "  0.13220457971134855,\n",
       "  0.13186082090170267,\n",
       "  0.13134932751771472,\n",
       "  0.13095116171926141,\n",
       "  0.13063705327677239,\n",
       "  0.13020793394631225,\n",
       "  0.12991289447468798,\n",
       "  0.12942746420871981,\n",
       "  0.12915516634483251,\n",
       "  0.12872749062865904,\n",
       "  0.12830604788834998,\n",
       "  0.12798326213367883,\n",
       "  0.12768015092104415,\n",
       "  0.12731643120411096,\n",
       "  0.12701576862848701,\n",
       "  0.12665661911279769,\n",
       "  0.12630418545985422,\n",
       "  0.12591754109259137,\n",
       "  0.12576279093863005,\n",
       "  0.12528415353020361,\n",
       "  0.12496214109610776,\n",
       "  0.12477634181400482,\n",
       "  0.12418190706360846,\n",
       "  0.12404011301798691,\n",
       "  0.12372892795586488,\n",
       "  0.12334559775054427,\n",
       "  0.12313068799113454,\n",
       "  0.12268723592112933,\n",
       "  0.12245066933362532,\n",
       "  0.12205321962996793,\n",
       "  0.12177965062479119,\n",
       "  0.12171541164337429,\n",
       "  0.12111469289794161,\n",
       "  0.12094016257824963,\n",
       "  0.1207055783657987,\n",
       "  0.12030480907241134,\n",
       "  0.12002667710113231,\n",
       "  0.1197550512097989,\n",
       "  0.11952015550686504,\n",
       "  0.11924465868486546,\n",
       "  0.11897138214219677,\n",
       "  0.11869894327847384,\n",
       "  0.11846789595653887,\n",
       "  0.11814513138638674,\n",
       "  0.11781062936212913,\n",
       "  0.1176301933870251,\n",
       "  0.11763400874216907,\n",
       "  0.11711087562023884,\n",
       "  0.11683899322161895,\n",
       "  0.11656884289291108,\n",
       "  0.11654498429924139,\n",
       "  0.11659580863812458,\n",
       "  0.11596247839603235,\n",
       "  0.11568107545706538,\n",
       "  0.11535491369887901,\n",
       "  0.11523512735856611],\n",
       " [629,\n",
       "  786,\n",
       "  833,\n",
       "  872,\n",
       "  901,\n",
       "  925,\n",
       "  934,\n",
       "  947,\n",
       "  953,\n",
       "  962,\n",
       "  966,\n",
       "  972,\n",
       "  974,\n",
       "  980,\n",
       "  980,\n",
       "  978,\n",
       "  986,\n",
       "  986,\n",
       "  986,\n",
       "  986,\n",
       "  986,\n",
       "  989,\n",
       "  989,\n",
       "  990,\n",
       "  991,\n",
       "  991,\n",
       "  991,\n",
       "  993,\n",
       "  993,\n",
       "  993,\n",
       "  993,\n",
       "  993,\n",
       "  994,\n",
       "  993,\n",
       "  996,\n",
       "  995,\n",
       "  997,\n",
       "  997,\n",
       "  997,\n",
       "  997,\n",
       "  997,\n",
       "  998,\n",
       "  997,\n",
       "  998,\n",
       "  997,\n",
       "  998,\n",
       "  998,\n",
       "  998,\n",
       "  998,\n",
       "  998,\n",
       "  998,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  998,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  1000,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  999,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000,\n",
       "  1000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model with regularization and small traing_data\n",
    "import mnist_loader, network2\n",
    "\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "net = network2.Network(\n",
    "    [784, 30, 10],\n",
    "    cost = network2.CrossEntropyCost\n",
    ")\n",
    "net.large_weight_initializer()\n",
    "net.SGD(\n",
    "    training_data[:1000],\n",
    "    400,\n",
    "    10,\n",
    "    0.5,\n",
    "    evaluation_data = test_data,\n",
    "    lmbda = 0.1,\n",
    "    monitor_evaluation_cost = True,\n",
    "    monitor_evaluation_accuracy = True,\n",
    "    monitor_training_cost = True,\n",
    "    monitor_training_accuracy = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Accuracy on training data: 45571 / 50000\n",
      "Accuracy on evaluation data: 9173 / 10000\n",
      "\n",
      "Epoch 1 training complete\n",
      "Accuracy on training data: 46683 / 50000\n",
      "Accuracy on evaluation data: 9303 / 10000\n",
      "\n",
      "Epoch 2 training complete\n",
      "Accuracy on training data: 47243 / 50000\n",
      "Accuracy on evaluation data: 9409 / 10000\n",
      "\n",
      "Epoch 3 training complete\n",
      "Accuracy on training data: 47603 / 50000\n",
      "Accuracy on evaluation data: 9471 / 10000\n",
      "\n",
      "Epoch 4 training complete\n",
      "Accuracy on training data: 47706 / 50000\n",
      "Accuracy on evaluation data: 9495 / 10000\n",
      "\n",
      "Epoch 5 training complete\n",
      "Accuracy on training data: 47738 / 50000\n",
      "Accuracy on evaluation data: 9521 / 10000\n",
      "\n",
      "Epoch 6 training complete\n",
      "Accuracy on training data: 47975 / 50000\n",
      "Accuracy on evaluation data: 9586 / 10000\n",
      "\n",
      "Epoch 7 training complete\n",
      "Accuracy on training data: 47994 / 50000\n",
      "Accuracy on evaluation data: 9573 / 10000\n",
      "\n",
      "Epoch 8 training complete\n",
      "Accuracy on training data: 48129 / 50000\n",
      "Accuracy on evaluation data: 9589 / 10000\n",
      "\n",
      "Epoch 9 training complete\n",
      "Accuracy on training data: 47945 / 50000\n",
      "Accuracy on evaluation data: 9535 / 10000\n",
      "\n",
      "Epoch 10 training complete\n",
      "Accuracy on training data: 48063 / 50000\n",
      "Accuracy on evaluation data: 9555 / 10000\n",
      "\n",
      "Epoch 11 training complete\n",
      "Accuracy on training data: 48308 / 50000\n",
      "Accuracy on evaluation data: 9600 / 10000\n",
      "\n",
      "Epoch 12 training complete\n",
      "Accuracy on training data: 48294 / 50000\n",
      "Accuracy on evaluation data: 9610 / 10000\n",
      "\n",
      "Epoch 13 training complete\n",
      "Accuracy on training data: 48322 / 50000\n",
      "Accuracy on evaluation data: 9617 / 10000\n",
      "\n",
      "Epoch 14 training complete\n",
      "Accuracy on training data: 48139 / 50000\n",
      "Accuracy on evaluation data: 9565 / 10000\n",
      "\n",
      "Epoch 15 training complete\n",
      "Accuracy on training data: 48030 / 50000\n",
      "Accuracy on evaluation data: 9534 / 10000\n",
      "\n",
      "Epoch 16 training complete\n",
      "Accuracy on training data: 48019 / 50000\n",
      "Accuracy on evaluation data: 9567 / 10000\n",
      "\n",
      "Epoch 17 training complete\n",
      "Accuracy on training data: 48231 / 50000\n",
      "Accuracy on evaluation data: 9586 / 10000\n",
      "\n",
      "Epoch 18 training complete\n",
      "Accuracy on training data: 48224 / 50000\n",
      "Accuracy on evaluation data: 9584 / 10000\n",
      "\n",
      "Epoch 19 training complete\n",
      "Accuracy on training data: 48220 / 50000\n",
      "Accuracy on evaluation data: 9578 / 10000\n",
      "\n",
      "Epoch 20 training complete\n",
      "Accuracy on training data: 47795 / 50000\n",
      "Accuracy on evaluation data: 9511 / 10000\n",
      "\n",
      "Epoch 21 training complete\n",
      "Accuracy on training data: 48458 / 50000\n",
      "Accuracy on evaluation data: 9623 / 10000\n",
      "\n",
      "Epoch 22 training complete\n",
      "Accuracy on training data: 48413 / 50000\n",
      "Accuracy on evaluation data: 9589 / 10000\n",
      "\n",
      "Epoch 23 training complete\n",
      "Accuracy on training data: 48239 / 50000\n",
      "Accuracy on evaluation data: 9596 / 10000\n",
      "\n",
      "Epoch 24 training complete\n",
      "Accuracy on training data: 48335 / 50000\n",
      "Accuracy on evaluation data: 9593 / 10000\n",
      "\n",
      "Epoch 25 training complete\n",
      "Accuracy on training data: 48419 / 50000\n",
      "Accuracy on evaluation data: 9593 / 10000\n",
      "\n",
      "Epoch 26 training complete\n",
      "Accuracy on training data: 48322 / 50000\n",
      "Accuracy on evaluation data: 9581 / 10000\n",
      "\n",
      "Epoch 27 training complete\n",
      "Accuracy on training data: 48328 / 50000\n",
      "Accuracy on evaluation data: 9575 / 10000\n",
      "\n",
      "Epoch 28 training complete\n",
      "Accuracy on training data: 47905 / 50000\n",
      "Accuracy on evaluation data: 9503 / 10000\n",
      "\n",
      "Epoch 29 training complete\n",
      "Accuracy on training data: 48392 / 50000\n",
      "Accuracy on evaluation data: 9600 / 10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [9173,\n",
       "  9303,\n",
       "  9409,\n",
       "  9471,\n",
       "  9495,\n",
       "  9521,\n",
       "  9586,\n",
       "  9573,\n",
       "  9589,\n",
       "  9535,\n",
       "  9555,\n",
       "  9600,\n",
       "  9610,\n",
       "  9617,\n",
       "  9565,\n",
       "  9534,\n",
       "  9567,\n",
       "  9586,\n",
       "  9584,\n",
       "  9578,\n",
       "  9511,\n",
       "  9623,\n",
       "  9589,\n",
       "  9596,\n",
       "  9593,\n",
       "  9593,\n",
       "  9581,\n",
       "  9575,\n",
       "  9503,\n",
       "  9600],\n",
       " [],\n",
       " [45571,\n",
       "  46683,\n",
       "  47243,\n",
       "  47603,\n",
       "  47706,\n",
       "  47738,\n",
       "  47975,\n",
       "  47994,\n",
       "  48129,\n",
       "  47945,\n",
       "  48063,\n",
       "  48308,\n",
       "  48294,\n",
       "  48322,\n",
       "  48139,\n",
       "  48030,\n",
       "  48019,\n",
       "  48231,\n",
       "  48224,\n",
       "  48220,\n",
       "  47795,\n",
       "  48458,\n",
       "  48413,\n",
       "  48239,\n",
       "  48335,\n",
       "  48419,\n",
       "  48322,\n",
       "  48328,\n",
       "  47905,\n",
       "  48392])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model with regularization and full training_data\n",
    "import mnist_loader, network2\n",
    "\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "net = network2.Network(\n",
    "    [784, 30, 10],\n",
    "    cost = network2.CrossEntropyCost\n",
    ")\n",
    "net.large_weight_initializer()\n",
    "net.SGD(\n",
    "    training_data,\n",
    "    30,\n",
    "    10,\n",
    "    0.5,\n",
    "    evaluation_data = test_data,\n",
    "    lmbda = 5.0,\n",
    "    monitor_evaluation_accuracy = True,\n",
    "    monitor_training_accuracy = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Does Regularization Help Reduce Overfitting?\n",
    "\n",
    "A common story that explains why regularization works is:\n",
    "\n",
    "> The smaller weights are, the lower their complexity, which means they\n",
    "provide a simpler and more powerful explanation for the data, therefore\n",
    "these smaller weights should be preferred.\n",
    "\n",
    "There are two ways that one can design a model: \n",
    "\n",
    "  1. Make a model that fits your data points exactly, usually with a high-order polynomial.\n",
    "  2. Make a simple model that doesn't fit your data points exactly, but will hopefully scale better than your high-order polynomial model will.\n",
    "  \n",
    "Some say that in science we should go with the simplest explanation unless compelled not to.\n",
    "This is because it is probably not a coincidence that a simple model can explain complex data, therefore the simple model must \"be expressing some underlying turth about the phenomenon.\"\n",
    "When we use a simplified model, it is not prone to change with small variations here and there; it generalizes better.\n",
    "\"An unregularized network can use large weights to learn a complex model that carries a lot of information about the noise in the trianing data.\"\n",
    "\n",
    "People refer to the idea of preferring simipler explanations as \"Occam's Razor.\"\n",
    "This does not mean that the simpler model will always be true, sometimes the more complex model is better at proedicting than the simpler model.\n",
    "The ultimate test of a model is not whether a model is simple or not, but rather it is how well the model predicts new phenomena.\n",
    "\n",
    "It is a fact that regularized neural networks generalize better than unregularized networks.\n",
    "However, it is to be noted that no one completely knows why regularization works, it really is a magin wand to help the neural network generalize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Techniques for Regularization\n",
    "\n",
    "### L1 Regularization\n",
    "\n",
    "This technique will modify the unregularized cost function by adding the sum of the absolute values of the weights, which is represented by: $ C = C_0 + \\frac{\\lambda}{n} \\sum_w |w| $, where $ C_0 $ is the original cost function, $ w $ is the weights, $ n $ is the number of samples, and $ \\lambda $ is the *regularization parameter*.\n",
    "\n",
    "Just like L2 regularization, the goal of L1 regularization is to shrink the large weights.\n",
    "They differ in how the weights are shrunk.\n",
    "L1 regularization shrinks the weights by a constant amount toward 0, but in L2 regularization the weights shrink by an amount that is proportional to w.\n",
    "This means that when a weight has a large magnitude, $ |w| $, L1 regularization shrinks the weight less than L2 regularization does.\n",
    "\n",
    "To conclude, L1 regularization \"concentrates the weights of the network in a relatively small number of high-importance connections, while the other weights are driven toward zero.\"\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Dropout does not rely on modifying the cost function, it modifies the network itself.\n",
    "It will randomly (and temporarily) delete half of the hidden neurons in the network, while leaving the input and output neurons as they are.\n",
    "After the input is through the modified network, and the results are backpropagated, the neurons which were dropped out are restored and new neurons are subsequently dropped out.\n",
    "Once training is complete, the weights outgoing from the hidden neurons are halved because there are twice as many hidden neurons than there were during training.\n",
    "\n",
    "Imagine that dropout is training multiple different neural networks, and then once training is finished it averages all of the neural networks.\n",
    "\n",
    "### Artificially Increasing the Training Set Size\n",
    "\n",
    "Intuitively it makes sense that the neural network should perform better when there is more data available.\n",
    "In the MNIST training data, one can artificially augment the number of training examples by modifying the images slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization\n",
    "\n",
    "An *ad hoc* way of initializing weights and biases is to randomly assign them based on an independent Gaussian distribution with mean $ 0 $ and standard deviation $ 1 $.\n",
    "This can be a problem because with this distribution the output of the sigmoid function can be either $ z \\gg 1 $ or $ z \\ll -1 $ (mush greater than or much less than), which means that some neurons will be saturated and learning will be slowed down.\n",
    "\n",
    "A solution to this issue is to \"initialize the weights as Gaussian random variables with mean $ 0 $ and standard deviation $ 1 / \\sqrt{ n_{in} } $, which makes the neurons less likely to saturate.\"\n",
    "Even though this is used for the weights, the biases are initialized in the previous manner, as Gaussian random variables with mean $ 0 $ and standard deviations $ 1 $.\n",
    "\n",
    "In most cases changing the initial weights will only speed up learning, but in other cases it will also improve the final perform of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json, random, sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# the network with all of the improvements found in this chapter\n",
    "\n",
    "class CrossEntropyCost(object):\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        return np.sum(np.nan_to_num(-y * np.log(a) - (1 - y) * np.log(1 - a)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a - y)\n",
    "    \n",
    "class QuadraticCost(object):\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        return 0.5 * np.linalg.norm(a - y) ** 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a - y) * sigmoid_prime(z)\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, sizes, cost = CrossEntropyCost):\n",
    "        self.num_layer = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.default_weight_initializer()\n",
    "        self.cost = cost\n",
    "        \n",
    "    def default_weight_initializer(self):\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.rand(y, x) / np.sqrt(x)\n",
    "                       for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "    def large_weight_initializer(self):\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.rand(y, x)\n",
    "                       for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, lmbda = 0.0,\n",
    "           evaluation_data = None,\n",
    "           monitor_evaluation_cost = False,\n",
    "           monitor_evaluation_accuracy = False,\n",
    "           monitor_training_cost = False,\n",
    "           monitor_training_accuracy = False\n",
    "        ):\n",
    "        if evaluation_data: n_data = len(evaluation_data)\n",
    "        n = len(training_data)\n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        training_cost, training_accuracy = [], []\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k + mini_batch_size] \n",
    "                for k in range(0, n, mini_batch_size)\n",
    "            ]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(\n",
    "                    mini_batch, eta, lmbda, len(training_data)\n",
    "                )\n",
    "            print('Epoch {} training complete').format(j)\n",
    "            if monitor_training_cost:\n",
    "                cost = self.total_cost(training_data, lmbda)\n",
    "                training_cost.append(cost)\n",
    "                print('Cost on training data: {}').format(cost)\n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data, convert = True)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print('Accuracy on training data: {} / {}').format(accuracy, n)\n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(evaluation_data, lmbda, convert = True)\n",
    "                evaluation_cost.append(cost)\n",
    "                print('Cost on evaluation data: {}').format(cost)\n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print('Accuracy on evaluation data: {} / {}').format(\n",
    "                    self.accuracy(evaluation_data), n_data\n",
    "                )\n",
    "            print()\n",
    "        return evaluation_cost, evaluation_accuracy, \\\n",
    "            training_cost, training_accuracy\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def accuracy(self, data, convert=False):\n",
    "        if convert:\n",
    "            results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
    "                       for (x, y) in data]\n",
    "        else:\n",
    "            results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in data]\n",
    "        return sum(int(x == y) for (x, y) in results)\n",
    "\n",
    "    def total_cost(self, data, lmbda, convert=False):\n",
    "        cost = 0.0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            if convert: y = vectorized_result(y)\n",
    "            cost += self.cost.fn(a, y)/len(data)\n",
    "        cost += 0.5*(lmbda/len(data))*sum(\n",
    "            np.linalg.norm(w)**2 for w in self.weights)\n",
    "        return cost\n",
    "\n",
    "    def save(self, filename):\n",
    "        data = {\"sizes\": self.sizes,\n",
    "                \"weights\": [w.tolist() for w in self.weights],\n",
    "                \"biases\": [b.tolist() for b in self.biases],\n",
    "                \"cost\": str(self.cost.__name__)}\n",
    "        f = open(filename, \"w\")\n",
    "        json.dump(data, f)\n",
    "        f.close()\n",
    "\n",
    "def load(filename):\n",
    "    f = open(filename, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
    "    net = Network(data[\"sizes\"], cost=cost)\n",
    "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
    "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
    "    return net\n",
    "\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Choose a Network's Hyper-Parameters\n",
    "\n",
    "### Broad Strategy\n",
    "\n",
    "One way to help choose hyper-parameters is to scale down the volume of training data.\n",
    "This will not reduce the time that it takes the network to learn, but it will also help you realize which parameters need to be tuned, and what sort of effect they will have.\n",
    "Using the example of the MNIST dataset, you can also reduce the data to classify only `0`'s and `1`'s.\n",
    "One should also start with a small number (or none) of hidden layers, and then increase them over time.\n",
    "The more hidden layers a network has, the more complex it is and the longer it will take to learn.\n",
    "\n",
    "### Learning Rate\n",
    "\n",
    "When analyzing the learning rate, we are mainly concerned with the cost as the epochs progress.\n",
    "We can visulaize this by plotting the various learning rates with cost on the y-axis, and epochs on the x-axis.\n",
    "Recall that the goal of the network is to minimize the cost, thus the optimal learning rate will show the cost decreasing as the epochs progress.\n",
    "If the learning rate is too high, the cost will jump around, and never really decrease because it will overshoot the minimum.\n",
    "If the learning rate is too low, stochastic gradient descent will be slow.\n",
    "\n",
    "#### Process\n",
    "\n",
    "  1. Find a learning rate in which the cost immediatly begins decreasing (as opposed to oscillating or increasing).\n",
    "  2. Increase the learning rate by order of magnitude e.g. if you started with $ \\eta = 0.01 $ then you should try $ \\eta = 0.1, 1.0, ... $. \n",
    "  Increase the learning rate until the cost oscillates or increases within the first few epochs.\n",
    "  3. To optimize the learning rate, you can find the largest value of $ \\eta $ where the cost decreases during the first few epochs, this gives you a threshold value for $ \\eta $.\n",
    "  \n",
    "**Note:** Pick a $ \\eta $ that is no larger than the threshold value.\n",
    "The smaller the $ \\eta $ the more usable it will be over many epochs (there will not be much of a learning slowdown as the epochs progress).\n",
    "\n",
    "This is the only hyper-parameter that we will use the performance of the network on test data because $ \\eta $ is the only hyper-parameter that is primarily concerned with the test data, it has no implicit relation to the final classification accuracy (to some extent).\n",
    "The \"primary prupose [of the learning rate] is really to control the step size in gradient descent, and monitoring the training cost is the best way to detect if the step size is too big.\"\n",
    "\n",
    "### Number of Epochs\n",
    "\n",
    "We can use the process of early stopping to find the optimal number of epochs to train.\n",
    "Early stopping is taking the classification accuracy on the validation data after each epoch, and when the accuracy stops imrproving stop training.\n",
    "To be more precise, a network *stops improving* when it hasn't improved in the last ten epochs because we don't want to stop prematurely.\n",
    "\n",
    "This approach is good for preliminary exploration of a network, but it may be too aggressive for some networks.\n",
    "\n",
    "### Learning Rate Schedule\n",
    "\n",
    "This allows us to vary the learning rate, $ \\eta $ as the epochs progress.\n",
    "When a network begins to learn, the weights are usually badly wrong, but then as the network improves the weights start to get better.\n",
    "To optimize the learning rate for this kind of behavior, the learning rate should initially be large then it should decrease as the network improves.\n",
    "\n",
    "One approach is to model the early stopping method and when the validation accuracy starts to decrease then decrease $ \\eta $ by some amount (maybe a factor of two or ten).\n",
    "This schedule allows many different possibilities, which can be difficult to choose from.\n",
    "Initially, use a single constant value for the learning rate, then experiment with a learning schedule.\n",
    "\n",
    "### Regularization Parameter\n",
    "\n",
    "To begin, set the regularization parameter $ \\lambda = 0 $ so that you can find an appropriate $ \\eta $.\n",
    "Once an adequate $ \\eta $ is found, start with $ \\lambda = 1.0 $ and then increase or decrease by factors of $ 10 $.\n",
    "After this, re-optimze $ \\eta $ again.\n",
    "\n",
    "### Mini-batch Size\n",
    "\n",
    "*Online learning* is when you have a mini-batch size of $ 1 $.\n",
    "Naively it may seem that *online learning* is optimal, but depending on your linear algebra library and hardware, it actually could be faster to compute a mini-batch of size $ 100 $ than of size $ 1 $ because it is possible to compute the gradient update for all example in a mini-batch simultaneously.\n",
    "\n",
    "The issue with choosing a mini-batch size is that if it is too small you don't take advantage of the benefits of good matrix libraries, and if it is too large you are not updating your weights often enough.\n",
    "The good news is that the other hyper-parameters don't heavily influence the effect of the mini-batch size (the other parameters don't need to be optimized in order to find a good mini-batch size).\n",
    "\n",
    "As you scale $ \\eta $, plot the validation accuracy versus *time* (elapsed, not epoch!) for multiple mini-batch sizes. \n",
    "Choose the mini-batch size that gives you the most rapid improvement in performance.\n",
    "\n",
    "### Automated Techniques\n",
    "\n",
    "Hand tuning hyper-parameters is a good exercise in learning the idiosyncracies of neural networks and their behavior.\n",
    "However, this can be very time consuming.\n",
    "One approach to automated hyper-parameter tuning is a *grid search*, which will systematically search through a grid in hyper-parameter space.\n",
    "Another method is [hyperopt](https://github.com/jaberg/hyperopt), which is a Bayesian approach to automatically optimize hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
