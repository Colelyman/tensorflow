{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and Deep Learning\n",
    "\n",
    "These notes are taken from the [Neural Networks and Deep Learning online book](http://neuralnetworksanddeeplearning.com/) by [Michael Nielsen](http://michaelnielsen.org/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons\n",
    "One of the basic neurons is a perceptron, a perceptron is a function that takes inputs $ x_1, x_2, ..., x_n $ of $ \\{0, 1\\} $, with each input value having a corresponding weight value, $ w_1, w_2, ..., w_n $. \n",
    "The following function shows how the output is calculated: \n",
    "$$ \\begin{eqnarray}\n",
    "  \\mbox{output} & = & \\left\\{ \\begin{array}{ll}\n",
    "      0 & \\mbox{if } \\sum_j w_j x_j \\leq \\mbox{ threshold} \\\\\n",
    "      1 & \\mbox{if } \\sum_j w_j x_j > \\mbox{ threshold}\n",
    "      \\end{array} \\right.\n",
    "\\end{eqnarray} $$.\n",
    "\n",
    "Another way to write $ \\sum_j w_j x_j $ is $ w \\cdot x $. \n",
    "Another way to represent the `threshold` is with a `bias`, $ bias \\equiv -\\mbox{threshold} $.\n",
    "\n",
    "So the output function becomes: \n",
    "$$ \\begin{eqnarray}\n",
    "  \\mbox{output} = \\left\\{ \n",
    "    \\begin{array}{ll} \n",
    "      0 & \\mbox{if } w\\cdot x + b \\leq 0 \\\\\n",
    "      1 & \\mbox{if } w\\cdot x + b > 0\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "\\end{eqnarray} $$.\n",
    "\n",
    "One can think of the bias as how easy it is to have the perceptron output a `1`. \n",
    "One can think of the weight as how important that input value should be considered; the higher the weight, the more important the input value is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Neurons\n",
    "\n",
    "An advantage of sigmoid neurons over perceptrons is that a small change in the input value of a sigmoid neuron will result in a small change in the output value; unlike the behavior of a percpetron. \n",
    "This is so because the sigmoid neuron has a smoothing property baked in it.\n",
    "\n",
    "The first major difference between perceptrons and sigmoid functions is that sigmoid functions use floating point numbers as input and output, whereas perceptrons only use $ 0 $ and $ 1 $ for both input and output.\n",
    "\n",
    "Like the perceptron, the sigmoid neuron has inputs $ x_1, x_2, ..., x_n $, weights for each input $ w_1, w_2, ..., w_n $, and an overall bias $ b $.\n",
    "Unlike the perceptron, the function to calculate the output is $ \\sigma(w \\cdot x+b) $ such that:\n",
    "$$ \\begin{eqnarray} \n",
    "  \\sigma(z) \\equiv \\frac{1}{1+e^{-z}}.\n",
    "\\tag{3}\\end{eqnarray} $$\n",
    "\n",
    "$ \\sigma $ is called the sigmoid function or the logistic function.\n",
    "\n",
    "If $ z $ is a large positive number, then $ \\sigma(z) $ is going to be $ 1 $, or very close to it.\n",
    "If $ z $ is a very negative, then $ \\sigma(z) $ is going to be $ 0 $.\n",
    "\n",
    "In the beginning I said that sigmoid neurons allow a small change in $ \\Delta w_j $ would result in a small change in $ \\Delta output $. \n",
    "This is the function that proves that statement:\n",
    "$$ \\begin{eqnarray} \n",
    "  \\Delta \\mbox{output} \\approx \\sum_j \\frac{\\partial \\, \\mbox{output}}{\\partial w_j}\n",
    "  \\Delta w_j + \\frac{\\partial \\, \\mbox{output}}{\\partial b} \\Delta b,\n",
    "\\end{eqnarray} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost functions\n",
    "\n",
    "The cost function is sometimes referred to as a loss or objective function.\n",
    "The goal of the neural network is to minimize the value of the cost function.\n",
    "If the value of the cost function is high, then the neural network is not doing a good job at classifying the inputs.\n",
    "\n",
    "### Quadratic Cost Function\n",
    "For the MNIST dataset we will use a _quadratic_ cost function, also known as _mean squared error (MSE)_. \n",
    "The cost function is:\n",
    "$$ \\begin{eqnarray}  C(w,b) \\equiv\n",
    "  \\frac{1}{2n} \\sum_x \\| y(x) - a\\|^2.\n",
    "\\end{eqnarray} $$\n",
    "where $ w $ is the collection of all weights, $ b $ is the collection of all biases, $ n $ is the total number of training inputs, $ a $ is the vector of outputs from the network when $ x $ is the input, and the sum is over all training inputs, $ x $.\n",
    "$ \\| v \\| $ is the length function for a vector $ v $.\n",
    "\n",
    "It is noted that $ C(w,b) $ is non-negative because it is the sum of non-negative terms.\n",
    "The cost $ C(w,b) $ becomes small as $ y(x) $ becomes more equal to $ a $ for all training inputs, $ x $. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimization Techniques\n",
    "\n",
    "The goal of training the neural network is to minimize the cost function.\n",
    "An algorithm must be developed so that we can minimize this function.\n",
    "\n",
    "In the context of minimizing a function, we want to find the global minimum of some given function. \n",
    "A naieve approach to this problem would be to take the derivative of the function at different points and to find where the minimum is.\n",
    "This approach won't work when there are a large number of variables, like in a neural network.\n",
    "\n",
    "### _Gradient Descent_\n",
    "\n",
    "Imagine that a function represents a valley, and that we have an imaginary ball.\n",
    "Given that the ball starts at some random point, it is assumed that it will roll to the global minimum of the function.\n",
    "\n",
    "When the imaginary ball moves towards the minimum is represented mathematically by $ \\begin{eqnarray} \n",
    "  \\Delta C \\approx \\frac{\\partial C}{\\partial v_1} \\Delta v_1 +\n",
    "  \\frac{\\partial C}{\\partial v_2} \\Delta v_2.\n",
    "\\tag{7}\\end{eqnarray} $. \n",
    "We want $ \\Delta C $ to be negative.\n",
    "In order to minimize $ \\Delta C $ we must change $ \\Delta v_1 $ and $ \\Delta v_2 $.\n",
    "Let $ \\Delta v \\equiv (\\Delta v_1, \\Delta v_2)^T $ where $ T $ is the transpose operation (transpose turns row vectors into column vectors).\n",
    "Let $ \\begin{eqnarray} \n",
    "  \\nabla C \\equiv \\left( \\frac{\\partial C}{\\partial v_1}, \n",
    "  \\frac{\\partial C}{\\partial v_2} \\right)^T.\n",
    "\\tag{8}\\end{eqnarray} $, which leads to $ \\begin{eqnarray} \n",
    "  \\Delta C \\approx \\nabla C \\cdot \\Delta v.\n",
    "\\tag{9}\\end{eqnarray} $.\n",
    "\n",
    "Now we can modify the equation to be $ \\Delta v = -\\eta \\nabla C $ so that $ \\nabla $ is a small, positive parameter and is known as the _learning rate_.\n",
    "Therefore, $ \\Delta C \\approx -\\eta \\nabla C \\cdot \\nabla C = -\\eta \\|\\nabla C\\|^2 $.\n",
    "We know that $ \\| \\nabla C \\|^2 \\geq 0 $ which means that $ \\Delta C \\leq 0 $, if $ v $ is changed in the right way.\n",
    "Each position of the imaginary ball is moved in such a manner: $ v \\rightarrow v' = v -\\eta \\nabla C $.\n",
    "\n",
    "The gradient descent algorithm computes the gradient $ \\nabla C $ and moves in the _opposite_ direction.\n",
    "In order for this to work properly, $ \\eta $ must not be too large or the imaginary ball will go past the minimum; $ \\eta $ must also not be too small  or the algorithm will be too slow.\n",
    "\n",
    "In the above equation, there were only two variables, but there can be more variables $ v_1, ..., v_m $ such that $ \\Delta v = (\\Delta v_1, ..., \\Delta v_m)^T $ and $ \\nabla C \\equiv \\left(\\frac{\\partial C}{\\partial v_1}, \\ldots, \\frac{\\partial C}{\\partial v_m}\\right)^T $.\n",
    "\n",
    "When we apply the gradient descent problem directly to neural networks, the equations are: \n",
    "$ w_k \\rightarrow w_k' = w_k-\\eta \\frac{\\partial C}{\\partial w_k} $ and $ b_l \\rightarrow b_l' = b_l-\\eta \\frac{\\partial C}{\\partial b_l} $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
